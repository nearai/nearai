{"config":{"lang":["en"],"separator":"[\\s\\u200b\\-_,:!=\\[\\]()\"`/]+|\\.(?!\\d)|&[lg]t;|(?!\\b)(?=[A-Z][a-z])","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"NEAR AI: Building a Truly Open AI","text":"<p>Welcome! NEAR AI is a toolkit to help build, measure, and deploy AI systems focused on agents.</p> <p>Driven by one of the minds behinds TensorFlow and the Transformer Architecture, NEAR AI puts you back in control. Your data stays yours, and your AI works for you, with no compromises on privacy or ownership.</p> <ul> <li> <p> NEAR AI Agents</p> <p>Autonomous system that can interact with you and use tools to solve tasks</p> <p>  Quickstart  Registry  Tools </p> </li> <li> <p> AI Models</p> <p>Best in class AI models that you can use and fine-tune to solve your tasks</p> <p>  Benchmarks  Fine-Tuning </p> </li> <li> <p> Developer Hub </p> <p>NEAR AI developer hub where you can discover and deploy agents, datasets, and models with ease. </p> <p>  Agents  Models  Datasets </p> </li> <li> <p> Community </p> <p>Join our community! Get help and contribute to the future of AI</p> <p> Community</p> </li> </ul> <p>Alpha</p> <p>NEAR AI is currently in <code>alpha</code> - we're building something special and shipping new features every day! Want to help shape the future of AI? Join our community and contribute! \ud83d\ude80</p> <ul> <li>\ud83d\udc1b Report bugs and suggest features</li> <li>\ud83d\udcbb Submit pull requests</li> <li>\ud83d\udcd6 Improve documentation</li> <li>\ud83e\udd1d Help other users in the community</li> <li>\ud83c\udf1f Star our repository</li> </ul> <p>Check out our contributing guide to get started.</p>"},{"location":"api/","title":"API Reference","text":""},{"location":"api/#nearai","title":"nearai","text":""},{"location":"api/#nearai.EntryLocation","title":"EntryLocation","text":"<p>               Bases: <code>BaseModel</code></p> <p>EntryLocation</p> Source code in <code>nearai/openapi_client/models/entry_location.py</code> <pre><code>class EntryLocation(BaseModel):\n    \"\"\"\n    EntryLocation\n    \"\"\" # noqa: E501\n    namespace: StrictStr\n    name: StrictStr\n    version: StrictStr\n    __properties: ClassVar[List[str]] = [\"namespace\", \"name\", \"version\"]\n\n    model_config = ConfigDict(\n        populate_by_name=True,\n        validate_assignment=True,\n        protected_namespaces=(),\n    )\n\n\n    def to_str(self) -&gt; str:\n        \"\"\"Returns the string representation of the model using alias\"\"\"\n        return pprint.pformat(self.model_dump(by_alias=True))\n\n    def to_json(self) -&gt; str:\n        \"\"\"Returns the JSON representation of the model using alias\"\"\"\n        # TODO: pydantic v2: use .model_dump_json(by_alias=True, exclude_unset=True) instead\n        return json.dumps(self.to_dict())\n\n    @classmethod\n    def from_json(cls, json_str: str) -&gt; Optional[Self]:\n        \"\"\"Create an instance of EntryLocation from a JSON string\"\"\"\n        return cls.from_dict(json.loads(json_str))\n\n    def to_dict(self) -&gt; Dict[str, Any]:\n        \"\"\"Return the dictionary representation of the model using alias.\n\n        This has the following differences from calling pydantic's\n        `self.model_dump(by_alias=True)`:\n\n        * `None` is only added to the output dict for nullable fields that\n          were set at model initialization. Other fields with value `None`\n          are ignored.\n        \"\"\"\n        excluded_fields: Set[str] = set([\n        ])\n\n        _dict = self.model_dump(\n            by_alias=True,\n            exclude=excluded_fields,\n            exclude_none=True,\n        )\n        return _dict\n\n    @classmethod\n    def from_dict(cls, obj: Optional[Dict[str, Any]]) -&gt; Optional[Self]:\n        \"\"\"Create an instance of EntryLocation from a dict\"\"\"\n        if obj is None:\n            return None\n\n        if not isinstance(obj, dict):\n            return cls.model_validate(obj)\n\n        _obj = cls.model_validate({\n            \"namespace\": obj.get(\"namespace\"),\n            \"name\": obj.get(\"name\"),\n            \"version\": obj.get(\"version\")\n        })\n        return _obj\n</code></pre>"},{"location":"api/#nearai.EntryLocation.from_dict","title":"from_dict  <code>classmethod</code>","text":"<pre><code>from_dict(obj: Optional[Dict[str, Any]]) -&gt; Optional[Self]\n</code></pre> <p>Create an instance of EntryLocation from a dict</p> Source code in <code>nearai/openapi_client/models/entry_location.py</code> <pre><code>@classmethod\ndef from_dict(cls, obj: Optional[Dict[str, Any]]) -&gt; Optional[Self]:\n    \"\"\"Create an instance of EntryLocation from a dict\"\"\"\n    if obj is None:\n        return None\n\n    if not isinstance(obj, dict):\n        return cls.model_validate(obj)\n\n    _obj = cls.model_validate({\n        \"namespace\": obj.get(\"namespace\"),\n        \"name\": obj.get(\"name\"),\n        \"version\": obj.get(\"version\")\n    })\n    return _obj\n</code></pre>"},{"location":"api/#nearai.EntryLocation.from_json","title":"from_json  <code>classmethod</code>","text":"<pre><code>from_json(json_str: str) -&gt; Optional[Self]\n</code></pre> <p>Create an instance of EntryLocation from a JSON string</p> Source code in <code>nearai/openapi_client/models/entry_location.py</code> <pre><code>@classmethod\ndef from_json(cls, json_str: str) -&gt; Optional[Self]:\n    \"\"\"Create an instance of EntryLocation from a JSON string\"\"\"\n    return cls.from_dict(json.loads(json_str))\n</code></pre>"},{"location":"api/#nearai.EntryLocation.to_dict","title":"to_dict","text":"<pre><code>to_dict() -&gt; Dict[str, Any]\n</code></pre> <p>Return the dictionary representation of the model using alias.</p> <p>This has the following differences from calling pydantic's <code>self.model_dump(by_alias=True)</code>:</p> <ul> <li><code>None</code> is only added to the output dict for nullable fields that   were set at model initialization. Other fields with value <code>None</code>   are ignored.</li> </ul> Source code in <code>nearai/openapi_client/models/entry_location.py</code> <pre><code>def to_dict(self) -&gt; Dict[str, Any]:\n    \"\"\"Return the dictionary representation of the model using alias.\n\n    This has the following differences from calling pydantic's\n    `self.model_dump(by_alias=True)`:\n\n    * `None` is only added to the output dict for nullable fields that\n      were set at model initialization. Other fields with value `None`\n      are ignored.\n    \"\"\"\n    excluded_fields: Set[str] = set([\n    ])\n\n    _dict = self.model_dump(\n        by_alias=True,\n        exclude=excluded_fields,\n        exclude_none=True,\n    )\n    return _dict\n</code></pre>"},{"location":"api/#nearai.EntryLocation.to_json","title":"to_json","text":"<pre><code>to_json() -&gt; str\n</code></pre> <p>Returns the JSON representation of the model using alias</p> Source code in <code>nearai/openapi_client/models/entry_location.py</code> <pre><code>def to_json(self) -&gt; str:\n    \"\"\"Returns the JSON representation of the model using alias\"\"\"\n    # TODO: pydantic v2: use .model_dump_json(by_alias=True, exclude_unset=True) instead\n    return json.dumps(self.to_dict())\n</code></pre>"},{"location":"api/#nearai.EntryLocation.to_str","title":"to_str","text":"<pre><code>to_str() -&gt; str\n</code></pre> <p>Returns the string representation of the model using alias</p> Source code in <code>nearai/openapi_client/models/entry_location.py</code> <pre><code>def to_str(self) -&gt; str:\n    \"\"\"Returns the string representation of the model using alias\"\"\"\n    return pprint.pformat(self.model_dump(by_alias=True))\n</code></pre>"},{"location":"api/#nearai.parse_location","title":"parse_location","text":"<pre><code>parse_location(entry_location: str) -&gt; EntryLocation\n</code></pre> <p>Create a EntryLocation from a string in the format namespace/name/version.</p> Source code in <code>nearai/lib.py</code> <pre><code>def parse_location(entry_location: str) -&gt; EntryLocation:\n    \"\"\"Create a EntryLocation from a string in the format namespace/name/version.\"\"\"\n    match = entry_location_pattern.match(entry_location)\n\n    if match is None:\n        raise ValueError(f\"Invalid entry format: {entry_location}. Should have the format &lt;namespace&gt;/&lt;name&gt;/&lt;version&gt;\")\n\n    return EntryLocation(\n        namespace=match.group(\"namespace\"),\n        name=match.group(\"name\"),\n        version=match.group(\"version\"),\n    )\n</code></pre>"},{"location":"api/#nearai.agent_creator","title":"agent_creator","text":""},{"location":"api/#nearai.agent_creator.create_new_agent","title":"create_new_agent","text":"<pre><code>create_new_agent(namespace: str, name: Optional[str], description: Optional[str]) -&gt; None\n</code></pre> <p>Create a new agent from scratch with interactive options.</p> Source code in <code>nearai/agent_creator.py</code> <pre><code>def create_new_agent(namespace: str, name: Optional[str], description: Optional[str]) -&gt; None:\n    \"\"\"Create a new agent from scratch with interactive options.\"\"\"\n    # If no name/description provided, use interactive prompts\n    init_instructions = \"\"\n    if name is None and description is None:\n        _, name, description, init_instructions = prompt_agent_details()\n\n    # Set the agent path\n    registry_folder = get_registry_folder()\n    if registry_folder is None:\n        raise ValueError(\"Registry folder path cannot be None\")\n\n    # Narrow the type of namespace &amp; name from Optional[str] to str\n    namespace_str: str = namespace if namespace is not None else \"\"\n    if namespace_str == \"\":\n        raise ValueError(\"Namespace cannot be None or empty\")\n\n    name_str: str = name if name is not None else \"\"\n    if name_str == \"\":\n        raise ValueError(\"Name cannot be None or empty\")\n\n    agent_path = registry_folder / namespace_str / name_str / \"0.0.1\"\n    agent_path.mkdir(parents=True, exist_ok=True)\n\n    metadata: Dict[str, Any] = {\n        \"name\": name_str,\n        \"version\": \"0.0.1\",\n        \"description\": description or \"\",\n        \"category\": \"agent\",\n        \"tags\": [],\n        \"details\": {\n            \"agent\": {\n                \"defaults\": {\n                    \"model\": DEFAULT_MODEL,\n                    \"model_provider\": DEFAULT_PROVIDER,\n                    \"model_temperature\": DEFAULT_MODEL_TEMPERATURE,\n                    \"model_max_tokens\": DEFAULT_MODEL_MAX_TOKENS,\n                }\n            }\n        },\n        \"show_entry\": True,\n    }\n\n    metadata_path = agent_path / \"metadata.json\"\n    with open(metadata_path, \"w\") as f:\n        json.dump(metadata, f, indent=2)\n\n    # Create a default agent.py with the provided initial instructions\n    agent_py_content = f\"\"\"from nearai.agents.environment import Environment\n\n\ndef run(env: Environment):\n    # Your agent code here\n    prompt = {{\"role\": \"system\", \"content\": \"{init_instructions}\"}}\n    result = env.completion([prompt] + env.list_messages())\n    env.add_reply(result)\n\nrun(env)\n\n\"\"\"\n    agent_py_path = agent_path / \"agent.py\"\n    with open(agent_py_path, \"w\") as f:\n        f.write(agent_py_content)\n\n    # Display success message and options\n    display_success_and_options(agent_path)\n</code></pre>"},{"location":"api/#nearai.agent_creator.display_success_and_options","title":"display_success_and_options","text":"<pre><code>display_success_and_options(agent_path: Path) -&gt; None\n</code></pre> <p>Display success message and interactive options for next steps.</p> Source code in <code>nearai/agent_creator.py</code> <pre><code>def display_success_and_options(agent_path: Path) -&gt; None:\n    \"\"\"Display success message and interactive options for next steps.\"\"\"\n    console = Console()\n    success_title = Text(\" \ud83c\udf89 SUCCESS!\", style=\"bold green\")\n    path_text = Text.assemble((\"\\n  \u2022 New AI Agent created at: \", \"bold green\"), (f\"{agent_path}\", \"bold\"))\n\n    files_panel = Panel(\n        Text.assemble(\n            (\"Edit agent code here:\\n\\n\", \"yellow\"),\n            (f\"\ud83d\udcc4 - {agent_path}/agent.py\\n\", \"bold blue\"),\n            (f\"\ud83d\udcc4 - {agent_path}/metadata.json\", \"bold blue\"),\n        ),\n        title=\"Agent Files\",\n        border_style=\"yellow\",\n    )\n\n    commands_panel = Panel(\n        Text.assemble(\n            (\"Run this agent locally:\\n\", \"light_green\"),\n            (f\"  nearai agent interactive {agent_path} --local\\n\\n\", \"bold\"),\n            (\"Upload this agent to NEAR AI's public registry:\\n\", \"light_green\"),\n            (f\"  nearai registry upload {agent_path}\\n\\n\", \"bold\"),\n            (\"Run ANY agent from your local registry:\\n\", \"light_green\"),\n            (\"  nearai agent interactive --local\", \"bold\"),\n        ),\n        title=\"Useful Commands\",\n        border_style=\"green\",\n    )\n\n    console.print(\"\\n\")\n    console.print(success_title)\n    console.print(path_text)\n    console.print(\"\\n\")\n    console.print(files_panel)\n    console.print(\"\\n\")\n    console.print(commands_panel)\n    console.print(\"\\n\")\n\n    # Create next steps options with proper markup\n    options = [\n        \"Upload open source (public) agent to NEAR AI registry \ud83c\udf10\ud83d\ude80\",\n        \"Upload encrypted (private) agent to NEAR AI registry \ud83d\udd10\ud83d\ude80\",\n        \"Run agent \ud83d\udcac\",\n        \"Open agent code in editor \ud83e\uddd1\u200d\ud83d\udcbb\",\n        \"Exit \ud83d\udc4b\",\n    ]\n\n    # Create the panel with direct markup\n    next_steps_panel = Panel(\n        f\"\"\"[bold blue]What would you like to do next?[/bold blue]\n\n[bold blue]1.[/bold blue] {options[0]}\n[bold blue]2.[/bold blue] {options[1]}\n[bold blue]3.[/bold blue] {options[2]}\n[bold blue]4.[/bold blue] {options[3]}\n[bold blue]5.[/bold blue] {options[4]}\"\"\",\n        title=\"[bold blue]Next Steps[/bold blue]\",\n        border_style=\"blue\",\n    )\n    console.print(next_steps_panel)\n    console.print(\"\\n\")\n\n    # Main options loop\n    while True:\n        try:\n            choice = int(Prompt.ask(\"[bold]Choose an option\", default=\"5\")) - 1\n            if not (0 &lt;= choice &lt; len(options)):\n                console.print(\"[red]Invalid choice. Please try again.\")\n                continue\n        except ValueError:\n            console.print(\"[red]Please enter a valid number.\")\n            continue\n\n        # Exit option\n        if choice == 4:  # Exit\n            break\n\n        # Handle user choice\n        if choice == 0 or choice == 1:  # Upload agent\n            console.print(\"\\n[green]Uploading agent to registry...[/green]\")\n            try:\n                registry.upload(agent_path, show_progress=True, encrypt=(choice == 1))\n                console.print(\"[green bold]\u2713 Agent uploaded successfully![/green bold]\\n\")\n\n                # Extract namespace and name from agent_path\n                namespace = agent_path.parts[-3]\n                agent_name = agent_path.parts[-2]\n\n                # Generate and display link to the agent\n                agent_url = f\"https://app.near.ai/agents/{namespace}/{agent_name}/latest\"\n                console.print(\"[yellow]View your agent in the NEAR AI Developer Hub:[/yellow]\")\n                console.print(f\"[link={agent_url}]{agent_url}[/link]\\n\")\n\n                break  # Exit after successful upload\n            except Exception as e:\n                console.print(f\"[red bold]\u2717 Error uploading agent: {str(e)}[/red bold]\\n\")\n\n        elif choice == 2:  # Run agent\n            console.print(\"\\n[green]Running agent...[/green]\")\n            try:\n                from nearai.cli import AgentCli\n\n                agent_cli = AgentCli()\n                agent_cli.interactive(str(agent_path), local=True)\n                break  # Exit after running agent\n            except Exception as e:\n                console.print(f\"[red bold]\u2717 Error running agent: {str(e)}[/red bold]\\n\")\n\n        elif choice == 3:  # Code agent\n            console.print(\"\\n[green]Attempting to open agent in a code editor...[/green]\")\n            try:\n                # Check for common editors\n                editors = [\n                    (\"Visual Studio Code\", \"code\"),\n                    (\"PyCharm\", \"charm\"),\n                    (\"Atom\", \"atom\"),\n                    (\"IntelliJ IDEA\", \"idea\"),\n                ]\n\n                # Try each editor\n                editor_found = False\n                for editor_name, command in editors:\n                    cmd_path = shutil.which(command)\n                    if cmd_path:\n                        subprocess.run([command, str(agent_path)], check=False)\n                        console.print(f\"[green bold]\u2713 Agent opened in {editor_name}![/green bold]\\n\")\n                        editor_found = True\n                        break\n\n                # If no code editor found, try opening in file explorer\n                if not editor_found:\n                    console.print(\"[yellow]Could not find any common code editors. Trying file explorer...[/yellow]\")\n                    system = platform.system()\n                    explorer_opened = False\n\n                    if system == \"Windows\":\n                        subprocess.run([\"explorer\", str(agent_path)], check=False)\n                        explorer_opened = True\n                    elif system == \"Darwin\":  # macOS\n                        subprocess.run([\"open\", str(agent_path)], check=False)\n                        explorer_opened = True\n                    elif system == \"Linux\":\n                        subprocess.run([\"xdg-open\", str(agent_path)], check=False)\n                        explorer_opened = True\n\n                    if explorer_opened:\n                        console.print(\"[green bold]\u2713 Agent directory opened in file explorer![/green bold]\\n\")\n                    else:\n                        console.print(\"[yellow]Could not open directory automatically.[/yellow]\")\n                        console.print(\"[yellow]Your agent is located at:[/yellow]\")\n                        console.print(f\"[bold cyan]{agent_path}[/bold cyan]\\n\")\n\n                break  # Exit after attempt\n            except Exception as e:\n                console.print(f\"[red bold]\u2717 Error opening agent: {str(e)}[/red bold]\")\n                console.print(f\"[yellow]Your agent is located at: {agent_path}[/yellow]\\n\")\n</code></pre>"},{"location":"api/#nearai.agent_creator.fork_agent","title":"fork_agent","text":"<pre><code>fork_agent(fork: str, namespace: str, new_name: Optional[str]) -&gt; None\n</code></pre> <p>Fork an existing agent.</p> Source code in <code>nearai/agent_creator.py</code> <pre><code>def fork_agent(fork: str, namespace: str, new_name: Optional[str]) -&gt; None:\n    \"\"\"Fork an existing agent.\"\"\"\n    import shutil\n\n    # Parse the fork parameter\n    try:\n        entry_location = parse_location(fork)\n        fork_namespace = entry_location.namespace\n        fork_name = entry_location.name\n        fork_version = entry_location.version\n    except ValueError:\n        print(\"Invalid fork parameter format. Expected format: &lt;namespace&gt;/&lt;agent-name&gt;/&lt;version&gt;\")\n        return\n\n    # Download the agent from the registry\n    agent_location = f\"{fork_namespace}/{fork_name}/{fork_version}\"\n    print(f\"Downloading agent '{agent_location}'...\")\n    registry.download(agent_location, force=False, show_progress=True)\n    source_path = get_registry_folder() / fork_namespace / fork_name / fork_version\n\n    # Prompt for the new agent name if not provided\n    if not new_name:\n        new_name = input(\"Enter the new agent name: \").strip()\n        if not new_name:\n            print(\"Agent name cannot be empty.\")\n            return\n\n        # confirm pattern is ok\n        identifier_pattern = re.compile(r\"^[a-zA-Z0-9_\\-.]+$\")\n        if identifier_pattern.match(new_name) is None:\n            print(\"Invalid Name, please choose something different\")\n            return\n\n    # Set the destination path\n    dest_path = get_registry_folder() / namespace / new_name / \"0.0.1\"\n\n    # Copy the agent files\n    shutil.copytree(source_path, dest_path)\n\n    # Update metadata.json\n    metadata_path = dest_path / \"metadata.json\"\n    with open(metadata_path, \"r\") as file:\n        metadata = json.load(file)\n\n    metadata[\"name\"] = new_name\n    metadata[\"version\"] = \"0.0.1\"\n\n    with open(metadata_path, \"w\") as file:\n        json.dump(metadata, file, indent=2)\n\n    print(f\"\\nForked agent '{agent_location}' to '{dest_path}'\")\n    print(f\"Agent '{new_name}' created at '{dest_path}' with updated metadata.\")\n\n    # Display success and interactive options\n    display_success_and_options(dest_path)\n</code></pre>"},{"location":"api/#nearai.agent_creator.prompt_agent_details","title":"prompt_agent_details","text":"<pre><code>prompt_agent_details() -&gt; Tuple[str, str, str, str]\n</code></pre> <p>Prompt user for agent details and return them.</p> Source code in <code>nearai/agent_creator.py</code> <pre><code>def prompt_agent_details() -&gt; Tuple[str, str, str, str]:\n    \"\"\"Prompt user for agent details and return them.\"\"\"\n    console = Console()\n\n    # Get namespace from CONFIG, with null check\n    from nearai.config import CONFIG\n\n    if CONFIG.auth is None:\n        raise ValueError(\"Not logged in. Please run 'nearai login' first.\")\n    namespace = CONFIG.auth.namespace\n\n    # Welcome message\n    console.print(NEAR_AI_BANNER)\n    welcome_panel = Panel(\n        Text.assemble(\n            (\"Let's create a new agent! \ud83e\uddbe \\n\", \"bold green\"),\n            (\"We'll need some basic information to get started.\", \"dim\"),\n        ),\n        title=\"Agent Creator\",\n        border_style=\"green\",\n    )\n    console.print(welcome_panel)\n    console.print(\"\\n\")\n\n    # Name prompt with explanation\n    name_info = Panel(\n        Text.assemble(\n            (\"Choose a unique name for your agent using only:\\n\\n\", \"\"),\n            (\"\u2022 letters\\n\", \"dim\"),\n            (\"\u2022 numbers\\n\", \"dim\"),\n            (\"\u2022 dots (.)\\n\", \"dim\"),\n            (\"\u2022 hyphens (-)\\n\", \"dim\"),\n            (\"\u2022 underscores (_)\\n\\n\", \"dim\"),\n            (\"Examples: 'code-reviewer', 'data.analyzer', 'text_summarizer'\", \"green\"),\n        ),\n        title=\"Agent Name Rules\",\n        border_style=\"blue\",\n    )\n    console.print(name_info)\n\n    while True:\n        name = Prompt.ask(\"[bold blue]Enter agent name\").strip()\n        # Validate name format\n        if not re.match(r\"^[a-zA-Z0-9][a-zA-Z0-9._-]*$\", name):\n            console.print(\n                \"[red]\u274c Invalid name format. Please use only letters, numbers, dots, hyphens, or underscores.\"\n            )\n            continue\n        if \" \" in name:\n            console.print(\"[red]\u274c Spaces are not allowed. Use dots, hyphens, or underscores instead.\")\n            continue\n        break\n\n    console.print(\"\\n\")\n\n    # Description prompt\n    description_info = Panel(\n        \"Describe what your agent will do in a few words...\", title=\"Description Info\", border_style=\"blue\"\n    )\n    console.print(description_info)\n    description = Prompt.ask(\"[bold blue]Enter description\")\n\n    console.print(\"\\n\")\n\n    # Initial instructions prompt\n    init_instructions_info = Panel(\n        Text.assemble(\n            (\"Provide initial instructions for your AI agent...\\n\\n\", \"\"),\n            (\"This will be used as the system message to guide the agent's behavior.\\n\", \"dim\"),\n            (\"You can edit these instructions later in the `agent.py` file.\\n\\n\", \"dim\"),\n            (\n                \"Example: You are a helpful humorous assistant. Use puns or jokes to make the user smile.\",\n                \"green\",\n            ),\n        ),\n        title=\"Instructions\",\n        border_style=\"blue\",\n    )\n    console.print(init_instructions_info)\n    init_instructions = Prompt.ask(\"[bold blue]Enter instructions\")\n\n    # Confirmation\n    console.print(\"\\n\")\n    summary_panel = Panel(\n        Text.assemble(\n            (\"Summary of your new agent:\\n\\n\", \"bold\"),\n            (\"Namespace/Account:    \", \"dim\"),\n            (f\"{namespace}\\n\", \"green\"),\n            (\"Agent Name:           \", \"dim\"),\n            (f\"{name}\\n\", \"green\"),\n            (\"Description:          \", \"dim\"),\n            (f\"{description}\\n\", \"green\"),\n            (\"Instructions:         \", \"dim\"),\n            (f\"{init_instructions}\", \"green\"),\n        ),\n        title=\"\ud83d\udccb Review\",\n        border_style=\"green\",\n    )\n    console.print(summary_panel)\n    console.print(\"\\n\")\n\n    if not Confirm.ask(\"[bold]Would you like to proceed?\", default=True):\n        console.print(\"[red]\u274c Agent creation cancelled\")\n        raise SystemExit(0)\n\n    return namespace, name, description, init_instructions\n</code></pre>"},{"location":"api/#nearai.agents","title":"agents","text":""},{"location":"api/#nearai.agents.agent","title":"agent","text":""},{"location":"api/#nearai.agents.agent.Agent","title":"Agent","text":"<p>               Bases: <code>object</code></p> Source code in <code>nearai/agents/agent.py</code> <pre><code>class Agent(object):\n    def __init__(  # noqa: D107\n        self,\n        identifier: str,\n        agent_files: Union[List, Path],\n        metadata: Dict,\n        change_to_temp_dir: bool = True,\n        local_path: Optional[Path] = None,\n    ):  # noqa: D107\n        self.code: Optional[CodeType] = None\n        self.file_cache: dict[str, Union[str, bytes]] = {}\n        self.identifier = identifier\n        name_parts = identifier.split(os.sep)\n        self.namespace = name_parts[0]\n        self.name = name_parts[1]\n        self.version = name_parts[2]\n\n        self.metadata = metadata\n        self.env_vars: Dict[str, Any] = {}\n\n        self.model = \"\"\n        self.model_provider = \"\"\n        self.model_temperature: Optional[float] = None\n        self.model_max_tokens: Optional[int] = None\n        self.welcome_title: Optional[str] = None\n        self.welcome_description: Optional[str] = None\n\n        self.set_agent_metadata(metadata)\n        self.agent_files = agent_files\n        self.original_cwd = os.getcwd()\n        self.is_local = local_path is not None\n\n        self.temp_dir = self.write_agent_files_to_temp(agent_files, local_path)\n        self.ts_runner_dir = \"\"\n        self.change_to_temp_dir = change_to_temp_dir\n        self.agent_filename = \"\"\n        self.agent_language = \"\"\n\n    def get_full_name(self):\n        \"\"\"Returns full agent name.\"\"\"\n        return f\"{self.namespace}/{self.name}/{self.version}\"\n\n    @staticmethod\n    def write_agent_files_to_temp(agent_files, local_path: Optional[Path] = None):\n        \"\"\"Write agent files to a temporary directory.\"\"\"\n        unique_id = uuid.uuid4().hex\n        if local_path:\n            temp_dir = os.path.join(local_path, f\"{THREADS_DIR}/agent_{unique_id}\")\n            print(f\"Temp run folder created: {temp_dir}\")\n        else:\n            temp_dir = os.path.join(tempfile.gettempdir(), f\"agent_{unique_id}\")\n\n        if isinstance(agent_files, List):\n            os.makedirs(temp_dir, exist_ok=True)\n\n            for agent_file in agent_files:\n                if isinstance(agent_file, dict):\n                    filename = agent_file[\"filename\"]\n                    content = agent_file[\"content\"]\n                else:\n                    assert isinstance(agent_file, Path)\n                    assert local_path\n                    filename = os.path.relpath(agent_file, local_path)\n                    with open(agent_file, \"rb\") as f:\n                        content = f.read()\n\n                file_path = os.path.join(temp_dir, filename)\n\n                try:\n                    if not os.path.exists(os.path.dirname(file_path)):\n                        os.makedirs(os.path.dirname(file_path))\n\n                    if isinstance(content, dict) or isinstance(content, list):\n                        try:\n                            content = json.dumps(content)\n                        except Exception as e:\n                            print(f\"Error converting content to json: {e}\")\n                        content = str(content)\n\n                    if isinstance(content, str):\n                        content = content.encode(\"utf-8\")\n\n                    with open(file_path, \"wb\") as f:\n                        with io.BytesIO(content) as byte_stream:\n                            shutil.copyfileobj(byte_stream, f)\n                except Exception as e:\n                    print(f\"Error writing file {file_path}: {e}\")\n                    raise e\n\n        else:\n            # if agent files is a PosixPath, it is a path to the agent directory\n            # Copy all agent files including subfolders\n            shutil.copytree(agent_files, temp_dir, dirs_exist_ok=True)\n\n        return temp_dir\n\n    def set_agent_metadata(self, metadata) -&gt; None:\n        \"\"\"Set agent details from metadata.\"\"\"\n        try:\n            self.name = metadata[\"name\"]\n            self.version = metadata[\"version\"]\n        except KeyError as e:\n            raise ValueError(f\"Missing key in metadata: {e}\") from None\n\n        details = metadata.get(\"details\", {})\n        agent = details.get(\"agent\", {})\n        welcome = agent.get(\"welcome\", {})\n\n        self.env_vars = details.get(\"env_vars\", {})\n        self.welcome_title = welcome.get(\"title\")\n        self.welcome_description = welcome.get(\"description\")\n\n        if agent_metadata := details.get(\"agent\", None):\n            if defaults := agent_metadata.get(\"defaults\", None):\n                self.model = defaults.get(\"model\", self.model)\n                self.model_provider = defaults.get(\"model_provider\", self.model_provider)\n                self.model_temperature = defaults.get(\"model_temperature\", self.model_temperature)\n                self.model_max_tokens = defaults.get(\"model_max_tokens\", self.model_max_tokens)\n\n        if not self.version or not self.name:\n            raise ValueError(\"Both 'version' and 'name' must be non-empty in metadata.\")\n\n    def run_python_code(\n        self,\n        agent_namespace,\n        agent_runner_user,\n        agent_py_modules_import,\n        log_stdout_callback=None,\n        log_stderr_callback=None,\n    ) -&gt; Tuple[Optional[str], Optional[str]]:\n        \"\"\"Launch python agent.\"\"\"\n        try:\n            # Switch to user env.agent_runner_user (Unix-only, skip on Windows)\n            if agent_runner_user and platform.system() != \"Windows\":  # noqa: D203,D211,D212,D213\n                import pwd\n\n                user_info = pwd.getpwnam(agent_runner_user)\n                os.setgid(user_info.pw_gid)\n                os.setuid(user_info.pw_uid)\n\n            # Create a custom writer that logs and writes to buffer\n            class LoggingWriter:\n                # Static class variable to prevent recursion\n                _in_write_operation = False\n\n                def __init__(self, buffer, log_func, stream_name):\n                    self.buffer = buffer\n                    self.log_func = log_func\n                    self.stream_name = stream_name\n\n                def write(self, msg):\n                    # Write to buffer regardless of recursion state\n                    self.buffer.write(msg)\n\n                    # Only log if not already in a write operation and message is not empty\n                    if not LoggingWriter._in_write_operation and msg.strip():\n                        try:\n                            # Set recursion flag before logging\n                            LoggingWriter._in_write_operation = True\n                            self.log_func(f\"[AGENT {self.stream_name}] {msg.strip()}\")\n                        finally:\n                            # Always reset the flag, even if an exception occurs\n                            LoggingWriter._in_write_operation = False\n\n                def flush(self):\n                    self.buffer.flush()\n\n            if log_stdout_callback:\n                stdout_buffer = io.StringIO()\n                sys.stdout = LoggingWriter(stdout_buffer, log_stdout_callback, \"STDOUT\")\n            if log_stderr_callback:\n                stderr_buffer = io.StringIO()\n                sys.stderr = LoggingWriter(stderr_buffer, log_stderr_callback, \"STDERR\")\n\n            # Run the code\n            # NOTE: runpy.run_path does not work in a multithreaded environment when running benchmark.\n            #       The performance of runpy.run_path may also change depending on a system, e.g. it may\n            #       work on Linux but not work on Mac.\n            #       `compile` and `exec` have been tested to work properly in a multithreaded environment.\n            try:\n                if self.code:\n                    clear_module_cache(agent_py_modules_import, agent_namespace)\n                    exec(self.code, agent_namespace)\n            finally:\n                sys.stdout = sys.__stdout__\n                sys.stderr = sys.__stderr__\n\n            # If no errors occur, return None\n            return None, None\n\n        except Exception as e:\n            # Return error message and full traceback as strings\n            return str(e), traceback.format_exc()\n\n    def run_ts_agent(self, agent_filename, env_vars, json_params, log_stdout_callback=None, log_stderr_callback=None):\n        \"\"\"Launch typescript agent.\"\"\"\n        print(f\"Running typescript agent {agent_filename} from {self.ts_runner_dir}\")\n\n        # Configure npm to use tmp directories\n        env = os.environ.copy()\n        env.update(\n            {\n                \"NPM_CONFIG_CACHE\": \"/tmp/npm_cache\",\n                \"NPM_CONFIG_PREFIX\": \"/tmp/npm_prefix\",\n                \"HOME\": \"/tmp\",  # Redirect npm home\n                \"NPM_CONFIG_LOGLEVEL\": \"error\",  # Suppress warnings, show only errors\n            }\n        )\n\n        # Ensure directory structure exists\n        os.makedirs(\"/tmp/npm_cache\", exist_ok=True)\n        os.makedirs(\"/tmp/npm_prefix\", exist_ok=True)\n\n        # read file /tmp/build-info.txt if exists\n        if os.path.exists(\"/var/task/build-info.txt\"):\n            with open(\"/var/task/build-info.txt\", \"r\") as file:\n                print(\"BUILD ID: \", file.read())\n\n        if env_vars.get(\"DEBUG\"):\n            print(\"Directory structure:\", os.listdir(\"/tmp/ts_runner\"))\n            print(\"Check package.json:\", os.path.exists(os.path.join(self.ts_runner_dir, \"package.json\")))\n            print(\"Symlink exists:\", os.path.exists(\"/tmp/ts_runner/node_modules/.bin/tsc\"))\n            print(\"Build files exist:\", os.path.exists(\"/tmp/ts_runner/build/sdk/main.js\"))\n\n        # Launching a subprocess to run an npm script with specific configurations\n        ts_process = subprocess.Popen(\n            [\n                \"npm\",  # Command to run Node Package Manager\n                \"--loglevel=error\",  # Suppress npm warnings and info logs, only show errors\n                \"--prefix\",\n                self.ts_runner_dir,  # Specifies the directory where npm should look for package.json\n                \"run\",\n                \"start\",  # Runs the \"start\" script defined in package.json, this launches the agent\n                \"agents/agent.ts\",\n                json_params,  # Arguments passed to the \"start\" script to configure the agent\n            ],\n            stdout=subprocess.PIPE,  # Captures standard output from the process\n            stderr=subprocess.PIPE,  # Captures standard error\n            cwd=self.ts_runner_dir,  # Sets the current working directory for the process\n            env=env_vars,  # Provides custom environment variables to the subprocess\n        )\n\n        stdout, stderr = ts_process.communicate()\n\n        stdout = stdout.decode().strip()\n        if stdout and log_stdout_callback:\n            log_stdout_callback(f\"[AGENT STDOUT] {stdout}\")\n\n        stderr = stderr.decode().strip()\n        if stderr and log_stderr_callback:\n            log_stderr_callback(f\"[AGENT STDERR] {stderr}\")\n\n    def run(\n        self, env: Any, task: Optional[str] = None, log_stdout_callback=None, log_stderr_callback=None\n    ) -&gt; Tuple[Optional[str], Optional[str]]:\n        \"\"\"Run the agent code. Returns error message and traceback message.\"\"\"\n        agent_ts_files_to_transpile = []\n        agent_py_modules_import = []\n\n        if not self.agent_filename or True:\n            # if agent has \"agent.py\" file, we use python runner\n            if os.path.exists(os.path.join(self.temp_dir, AGENT_FILENAME_PY)):\n                self.agent_filename = os.path.join(self.temp_dir, AGENT_FILENAME_PY)\n                self.agent_language = \"py\"\n                with open(self.agent_filename, \"r\") as agent_file:\n                    self.code = compile(agent_file.read(), self.agent_filename, \"exec\")\n            # else, if agent has \"agent.ts\" file, we use typescript runner\n            elif os.path.exists(os.path.join(self.temp_dir, AGENT_FILENAME_TS)):\n                self.agent_filename = os.path.join(self.temp_dir, AGENT_FILENAME_TS)\n                self.agent_language = \"ts\"\n\n                # copy files from nearai/ts_runner_sdk to self.temp_dir\n                ts_runner_sdk_dir = \"/tmp/ts_runner\"\n                ts_runner_agent_dir = os.path.join(ts_runner_sdk_dir, \"agents\")\n\n                ts_runner_actual_path = \"/var/task/ts_runner\"\n\n                shutil.copytree(ts_runner_actual_path, ts_runner_sdk_dir, symlinks=True, dirs_exist_ok=True)\n\n                # make ts agents dir if not exists\n                if not os.path.exists(ts_runner_agent_dir):\n                    os.makedirs(ts_runner_agent_dir, exist_ok=True)\n\n                # copy agents files\n                shutil.copy(os.path.join(self.temp_dir, AGENT_FILENAME_TS), ts_runner_agent_dir)\n\n                self.ts_runner_dir = ts_runner_sdk_dir\n            else:\n                raise ValueError(f\"Agent run error: {AGENT_FILENAME_PY} or {AGENT_FILENAME_TS} does not exist\")\n\n            # cache all agent files in file_cache\n            for root, dirs, files in os.walk(self.temp_dir):\n                is_main_dir = root == self.temp_dir\n\n                if is_main_dir:\n                    # add all folders in the root directory as potential modules to import\n                    agent_py_modules_import.extend(dirs)\n\n                for file in files:\n                    file_path = os.path.join(root, file)\n\n                    # get file extension for agent_filename\n                    if file_path.endswith(\".ts\"):\n                        agent_ts_files_to_transpile.append(file_path)\n\n                    if is_main_dir and file != AGENT_FILENAME_PY and file_path.endswith(\".py\"):\n                        # save py file without extension as potential module to import\n                        agent_py_modules_import.append(os.path.splitext(os.path.basename(file_path))[0])\n\n                    relative_path = os.path.relpath(file_path, self.temp_dir)\n                    try:\n                        with open(file_path, \"rb\") as f:\n                            content = f.read()\n                            try:\n                                # Try to decode as text\n                                self.file_cache[relative_path] = content.decode(\"utf-8\")\n                            except UnicodeDecodeError:\n                                # If decoding fails, store as binary\n                                self.file_cache[relative_path] = content\n\n                    except Exception as e:\n                        print(f\"Error with cache creation {file_path}: {e}\")\n\n        else:\n            print(\"Using cached agent code\")\n\n        namespace = {\n            \"env\": env,\n            \"agent\": self,\n            \"task\": task,\n            \"__name__\": \"__main__\",\n            \"__file__\": self.agent_filename,\n        }\n\n        user_auth = env.user_auth\n\n        # clear user_auth we saved before\n        env.user_auth = None\n\n        error_message, traceback_message = None, None\n\n        try:\n            if self.change_to_temp_dir:\n                if not os.path.exists(self.temp_dir):\n                    os.makedirs(self.temp_dir, exist_ok=True)\n                os.chdir(self.temp_dir)\n            sys.path.insert(0, self.temp_dir)\n\n            if self.agent_language == \"ts\":\n                agent_json_params = json.dumps(\n                    {\n                        \"thread_id\": env._thread_id,\n                        \"user_auth\": user_auth,\n                        \"base_url\": env.base_url,\n                        \"env_vars\": env.env_vars,\n                        \"agent_ts_files_to_transpile\": agent_ts_files_to_transpile,\n                    }\n                )\n\n                process = multiprocessing.Process(\n                    target=self.run_ts_agent,\n                    args=[\n                        self.agent_filename,\n                        env.env_vars,\n                        agent_json_params,\n                        log_stdout_callback,\n                        log_stderr_callback,\n                    ],\n                )\n                process.start()\n                process.join()\n            else:\n                if env.agent_runner_user:\n                    process = multiprocessing.Process(\n                        target=self.run_python_code,\n                        args=[namespace, env.agent_runner_user, log_stdout_callback, log_stderr_callback],\n                    )\n                    process.start()\n                    process.join()\n                else:\n                    error_message, traceback_message = self.run_python_code(\n                        namespace,\n                        env.agent_runner_user,\n                        agent_py_modules_import,\n                        log_stdout_callback,\n                        log_stderr_callback,\n                    )\n        finally:\n            if os.path.exists(self.temp_dir):\n                sys.path.remove(self.temp_dir)\n            if self.change_to_temp_dir:\n                os.chdir(self.original_cwd)\n\n        return error_message, traceback_message\n\n    @staticmethod\n    def load_agents(agents: str, config: ClientConfig, local: bool = False):\n        \"\"\"Loads agents from the registry.\"\"\"\n        return [Agent.load_agent(agent, config, local) for agent in agents.split(\",\")]\n\n    @staticmethod\n    def load_agent(\n        name: str,\n        config: ClientConfig,\n        local: bool = False,\n    ):\n        \"\"\"Loads a single agent from the registry.\"\"\"\n        from nearai.registry import get_registry_folder, registry\n\n        identifier = None\n        if local:\n            agent_files_path = get_registry_folder() / name\n            if config.auth is None:\n                namespace = \"not-logged-in\"\n            else:\n                namespace = config.auth.account_id\n        else:\n            agent_files_path = registry.download(name)\n            identifier = name\n        assert agent_files_path is not None, f\"Agent {name} not found.\"\n\n        metadata_path = os.path.join(agent_files_path, \"metadata.json\")\n        if not os.path.exists(metadata_path):\n            raise FileNotFoundError(f\"Metadata file not found: {metadata_path}\")\n        with open(metadata_path) as f:\n            metadata: Dict[str, Any] = json.load(f)\n\n        if not identifier:\n            identifier = \"/\".join([namespace, metadata[\"name\"], metadata[\"version\"]])\n\n        return Agent(identifier, agent_files_path, metadata)\n</code></pre>"},{"location":"api/#nearai.agents.agent.Agent.get_full_name","title":"get_full_name","text":"<pre><code>get_full_name()\n</code></pre> <p>Returns full agent name.</p> Source code in <code>nearai/agents/agent.py</code> <pre><code>def get_full_name(self):\n    \"\"\"Returns full agent name.\"\"\"\n    return f\"{self.namespace}/{self.name}/{self.version}\"\n</code></pre>"},{"location":"api/#nearai.agents.agent.Agent.load_agent","title":"load_agent  <code>staticmethod</code>","text":"<pre><code>load_agent(name: str, config: ClientConfig, local: bool = False)\n</code></pre> <p>Loads a single agent from the registry.</p> Source code in <code>nearai/agents/agent.py</code> <pre><code>@staticmethod\ndef load_agent(\n    name: str,\n    config: ClientConfig,\n    local: bool = False,\n):\n    \"\"\"Loads a single agent from the registry.\"\"\"\n    from nearai.registry import get_registry_folder, registry\n\n    identifier = None\n    if local:\n        agent_files_path = get_registry_folder() / name\n        if config.auth is None:\n            namespace = \"not-logged-in\"\n        else:\n            namespace = config.auth.account_id\n    else:\n        agent_files_path = registry.download(name)\n        identifier = name\n    assert agent_files_path is not None, f\"Agent {name} not found.\"\n\n    metadata_path = os.path.join(agent_files_path, \"metadata.json\")\n    if not os.path.exists(metadata_path):\n        raise FileNotFoundError(f\"Metadata file not found: {metadata_path}\")\n    with open(metadata_path) as f:\n        metadata: Dict[str, Any] = json.load(f)\n\n    if not identifier:\n        identifier = \"/\".join([namespace, metadata[\"name\"], metadata[\"version\"]])\n\n    return Agent(identifier, agent_files_path, metadata)\n</code></pre>"},{"location":"api/#nearai.agents.agent.Agent.load_agents","title":"load_agents  <code>staticmethod</code>","text":"<pre><code>load_agents(agents: str, config: ClientConfig, local: bool = False)\n</code></pre> <p>Loads agents from the registry.</p> Source code in <code>nearai/agents/agent.py</code> <pre><code>@staticmethod\ndef load_agents(agents: str, config: ClientConfig, local: bool = False):\n    \"\"\"Loads agents from the registry.\"\"\"\n    return [Agent.load_agent(agent, config, local) for agent in agents.split(\",\")]\n</code></pre>"},{"location":"api/#nearai.agents.agent.Agent.run","title":"run","text":"<pre><code>run(env: Any, task: Optional[str] = None, log_stdout_callback=None, log_stderr_callback=None) -&gt; Tuple[Optional[str], Optional[str]]\n</code></pre> <p>Run the agent code. Returns error message and traceback message.</p> Source code in <code>nearai/agents/agent.py</code> <pre><code>def run(\n    self, env: Any, task: Optional[str] = None, log_stdout_callback=None, log_stderr_callback=None\n) -&gt; Tuple[Optional[str], Optional[str]]:\n    \"\"\"Run the agent code. Returns error message and traceback message.\"\"\"\n    agent_ts_files_to_transpile = []\n    agent_py_modules_import = []\n\n    if not self.agent_filename or True:\n        # if agent has \"agent.py\" file, we use python runner\n        if os.path.exists(os.path.join(self.temp_dir, AGENT_FILENAME_PY)):\n            self.agent_filename = os.path.join(self.temp_dir, AGENT_FILENAME_PY)\n            self.agent_language = \"py\"\n            with open(self.agent_filename, \"r\") as agent_file:\n                self.code = compile(agent_file.read(), self.agent_filename, \"exec\")\n        # else, if agent has \"agent.ts\" file, we use typescript runner\n        elif os.path.exists(os.path.join(self.temp_dir, AGENT_FILENAME_TS)):\n            self.agent_filename = os.path.join(self.temp_dir, AGENT_FILENAME_TS)\n            self.agent_language = \"ts\"\n\n            # copy files from nearai/ts_runner_sdk to self.temp_dir\n            ts_runner_sdk_dir = \"/tmp/ts_runner\"\n            ts_runner_agent_dir = os.path.join(ts_runner_sdk_dir, \"agents\")\n\n            ts_runner_actual_path = \"/var/task/ts_runner\"\n\n            shutil.copytree(ts_runner_actual_path, ts_runner_sdk_dir, symlinks=True, dirs_exist_ok=True)\n\n            # make ts agents dir if not exists\n            if not os.path.exists(ts_runner_agent_dir):\n                os.makedirs(ts_runner_agent_dir, exist_ok=True)\n\n            # copy agents files\n            shutil.copy(os.path.join(self.temp_dir, AGENT_FILENAME_TS), ts_runner_agent_dir)\n\n            self.ts_runner_dir = ts_runner_sdk_dir\n        else:\n            raise ValueError(f\"Agent run error: {AGENT_FILENAME_PY} or {AGENT_FILENAME_TS} does not exist\")\n\n        # cache all agent files in file_cache\n        for root, dirs, files in os.walk(self.temp_dir):\n            is_main_dir = root == self.temp_dir\n\n            if is_main_dir:\n                # add all folders in the root directory as potential modules to import\n                agent_py_modules_import.extend(dirs)\n\n            for file in files:\n                file_path = os.path.join(root, file)\n\n                # get file extension for agent_filename\n                if file_path.endswith(\".ts\"):\n                    agent_ts_files_to_transpile.append(file_path)\n\n                if is_main_dir and file != AGENT_FILENAME_PY and file_path.endswith(\".py\"):\n                    # save py file without extension as potential module to import\n                    agent_py_modules_import.append(os.path.splitext(os.path.basename(file_path))[0])\n\n                relative_path = os.path.relpath(file_path, self.temp_dir)\n                try:\n                    with open(file_path, \"rb\") as f:\n                        content = f.read()\n                        try:\n                            # Try to decode as text\n                            self.file_cache[relative_path] = content.decode(\"utf-8\")\n                        except UnicodeDecodeError:\n                            # If decoding fails, store as binary\n                            self.file_cache[relative_path] = content\n\n                except Exception as e:\n                    print(f\"Error with cache creation {file_path}: {e}\")\n\n    else:\n        print(\"Using cached agent code\")\n\n    namespace = {\n        \"env\": env,\n        \"agent\": self,\n        \"task\": task,\n        \"__name__\": \"__main__\",\n        \"__file__\": self.agent_filename,\n    }\n\n    user_auth = env.user_auth\n\n    # clear user_auth we saved before\n    env.user_auth = None\n\n    error_message, traceback_message = None, None\n\n    try:\n        if self.change_to_temp_dir:\n            if not os.path.exists(self.temp_dir):\n                os.makedirs(self.temp_dir, exist_ok=True)\n            os.chdir(self.temp_dir)\n        sys.path.insert(0, self.temp_dir)\n\n        if self.agent_language == \"ts\":\n            agent_json_params = json.dumps(\n                {\n                    \"thread_id\": env._thread_id,\n                    \"user_auth\": user_auth,\n                    \"base_url\": env.base_url,\n                    \"env_vars\": env.env_vars,\n                    \"agent_ts_files_to_transpile\": agent_ts_files_to_transpile,\n                }\n            )\n\n            process = multiprocessing.Process(\n                target=self.run_ts_agent,\n                args=[\n                    self.agent_filename,\n                    env.env_vars,\n                    agent_json_params,\n                    log_stdout_callback,\n                    log_stderr_callback,\n                ],\n            )\n            process.start()\n            process.join()\n        else:\n            if env.agent_runner_user:\n                process = multiprocessing.Process(\n                    target=self.run_python_code,\n                    args=[namespace, env.agent_runner_user, log_stdout_callback, log_stderr_callback],\n                )\n                process.start()\n                process.join()\n            else:\n                error_message, traceback_message = self.run_python_code(\n                    namespace,\n                    env.agent_runner_user,\n                    agent_py_modules_import,\n                    log_stdout_callback,\n                    log_stderr_callback,\n                )\n    finally:\n        if os.path.exists(self.temp_dir):\n            sys.path.remove(self.temp_dir)\n        if self.change_to_temp_dir:\n            os.chdir(self.original_cwd)\n\n    return error_message, traceback_message\n</code></pre>"},{"location":"api/#nearai.agents.agent.Agent.run_python_code","title":"run_python_code","text":"<pre><code>run_python_code(agent_namespace, agent_runner_user, agent_py_modules_import, log_stdout_callback=None, log_stderr_callback=None) -&gt; Tuple[Optional[str], Optional[str]]\n</code></pre> <p>Launch python agent.</p> Source code in <code>nearai/agents/agent.py</code> <pre><code>def run_python_code(\n    self,\n    agent_namespace,\n    agent_runner_user,\n    agent_py_modules_import,\n    log_stdout_callback=None,\n    log_stderr_callback=None,\n) -&gt; Tuple[Optional[str], Optional[str]]:\n    \"\"\"Launch python agent.\"\"\"\n    try:\n        # Switch to user env.agent_runner_user (Unix-only, skip on Windows)\n        if agent_runner_user and platform.system() != \"Windows\":  # noqa: D203,D211,D212,D213\n            import pwd\n\n            user_info = pwd.getpwnam(agent_runner_user)\n            os.setgid(user_info.pw_gid)\n            os.setuid(user_info.pw_uid)\n\n        # Create a custom writer that logs and writes to buffer\n        class LoggingWriter:\n            # Static class variable to prevent recursion\n            _in_write_operation = False\n\n            def __init__(self, buffer, log_func, stream_name):\n                self.buffer = buffer\n                self.log_func = log_func\n                self.stream_name = stream_name\n\n            def write(self, msg):\n                # Write to buffer regardless of recursion state\n                self.buffer.write(msg)\n\n                # Only log if not already in a write operation and message is not empty\n                if not LoggingWriter._in_write_operation and msg.strip():\n                    try:\n                        # Set recursion flag before logging\n                        LoggingWriter._in_write_operation = True\n                        self.log_func(f\"[AGENT {self.stream_name}] {msg.strip()}\")\n                    finally:\n                        # Always reset the flag, even if an exception occurs\n                        LoggingWriter._in_write_operation = False\n\n            def flush(self):\n                self.buffer.flush()\n\n        if log_stdout_callback:\n            stdout_buffer = io.StringIO()\n            sys.stdout = LoggingWriter(stdout_buffer, log_stdout_callback, \"STDOUT\")\n        if log_stderr_callback:\n            stderr_buffer = io.StringIO()\n            sys.stderr = LoggingWriter(stderr_buffer, log_stderr_callback, \"STDERR\")\n\n        # Run the code\n        # NOTE: runpy.run_path does not work in a multithreaded environment when running benchmark.\n        #       The performance of runpy.run_path may also change depending on a system, e.g. it may\n        #       work on Linux but not work on Mac.\n        #       `compile` and `exec` have been tested to work properly in a multithreaded environment.\n        try:\n            if self.code:\n                clear_module_cache(agent_py_modules_import, agent_namespace)\n                exec(self.code, agent_namespace)\n        finally:\n            sys.stdout = sys.__stdout__\n            sys.stderr = sys.__stderr__\n\n        # If no errors occur, return None\n        return None, None\n\n    except Exception as e:\n        # Return error message and full traceback as strings\n        return str(e), traceback.format_exc()\n</code></pre>"},{"location":"api/#nearai.agents.agent.Agent.run_ts_agent","title":"run_ts_agent","text":"<pre><code>run_ts_agent(agent_filename, env_vars, json_params, log_stdout_callback=None, log_stderr_callback=None)\n</code></pre> <p>Launch typescript agent.</p> Source code in <code>nearai/agents/agent.py</code> <pre><code>def run_ts_agent(self, agent_filename, env_vars, json_params, log_stdout_callback=None, log_stderr_callback=None):\n    \"\"\"Launch typescript agent.\"\"\"\n    print(f\"Running typescript agent {agent_filename} from {self.ts_runner_dir}\")\n\n    # Configure npm to use tmp directories\n    env = os.environ.copy()\n    env.update(\n        {\n            \"NPM_CONFIG_CACHE\": \"/tmp/npm_cache\",\n            \"NPM_CONFIG_PREFIX\": \"/tmp/npm_prefix\",\n            \"HOME\": \"/tmp\",  # Redirect npm home\n            \"NPM_CONFIG_LOGLEVEL\": \"error\",  # Suppress warnings, show only errors\n        }\n    )\n\n    # Ensure directory structure exists\n    os.makedirs(\"/tmp/npm_cache\", exist_ok=True)\n    os.makedirs(\"/tmp/npm_prefix\", exist_ok=True)\n\n    # read file /tmp/build-info.txt if exists\n    if os.path.exists(\"/var/task/build-info.txt\"):\n        with open(\"/var/task/build-info.txt\", \"r\") as file:\n            print(\"BUILD ID: \", file.read())\n\n    if env_vars.get(\"DEBUG\"):\n        print(\"Directory structure:\", os.listdir(\"/tmp/ts_runner\"))\n        print(\"Check package.json:\", os.path.exists(os.path.join(self.ts_runner_dir, \"package.json\")))\n        print(\"Symlink exists:\", os.path.exists(\"/tmp/ts_runner/node_modules/.bin/tsc\"))\n        print(\"Build files exist:\", os.path.exists(\"/tmp/ts_runner/build/sdk/main.js\"))\n\n    # Launching a subprocess to run an npm script with specific configurations\n    ts_process = subprocess.Popen(\n        [\n            \"npm\",  # Command to run Node Package Manager\n            \"--loglevel=error\",  # Suppress npm warnings and info logs, only show errors\n            \"--prefix\",\n            self.ts_runner_dir,  # Specifies the directory where npm should look for package.json\n            \"run\",\n            \"start\",  # Runs the \"start\" script defined in package.json, this launches the agent\n            \"agents/agent.ts\",\n            json_params,  # Arguments passed to the \"start\" script to configure the agent\n        ],\n        stdout=subprocess.PIPE,  # Captures standard output from the process\n        stderr=subprocess.PIPE,  # Captures standard error\n        cwd=self.ts_runner_dir,  # Sets the current working directory for the process\n        env=env_vars,  # Provides custom environment variables to the subprocess\n    )\n\n    stdout, stderr = ts_process.communicate()\n\n    stdout = stdout.decode().strip()\n    if stdout and log_stdout_callback:\n        log_stdout_callback(f\"[AGENT STDOUT] {stdout}\")\n\n    stderr = stderr.decode().strip()\n    if stderr and log_stderr_callback:\n        log_stderr_callback(f\"[AGENT STDERR] {stderr}\")\n</code></pre>"},{"location":"api/#nearai.agents.agent.Agent.set_agent_metadata","title":"set_agent_metadata","text":"<pre><code>set_agent_metadata(metadata) -&gt; None\n</code></pre> <p>Set agent details from metadata.</p> Source code in <code>nearai/agents/agent.py</code> <pre><code>def set_agent_metadata(self, metadata) -&gt; None:\n    \"\"\"Set agent details from metadata.\"\"\"\n    try:\n        self.name = metadata[\"name\"]\n        self.version = metadata[\"version\"]\n    except KeyError as e:\n        raise ValueError(f\"Missing key in metadata: {e}\") from None\n\n    details = metadata.get(\"details\", {})\n    agent = details.get(\"agent\", {})\n    welcome = agent.get(\"welcome\", {})\n\n    self.env_vars = details.get(\"env_vars\", {})\n    self.welcome_title = welcome.get(\"title\")\n    self.welcome_description = welcome.get(\"description\")\n\n    if agent_metadata := details.get(\"agent\", None):\n        if defaults := agent_metadata.get(\"defaults\", None):\n            self.model = defaults.get(\"model\", self.model)\n            self.model_provider = defaults.get(\"model_provider\", self.model_provider)\n            self.model_temperature = defaults.get(\"model_temperature\", self.model_temperature)\n            self.model_max_tokens = defaults.get(\"model_max_tokens\", self.model_max_tokens)\n\n    if not self.version or not self.name:\n        raise ValueError(\"Both 'version' and 'name' must be non-empty in metadata.\")\n</code></pre>"},{"location":"api/#nearai.agents.agent.Agent.write_agent_files_to_temp","title":"write_agent_files_to_temp  <code>staticmethod</code>","text":"<pre><code>write_agent_files_to_temp(agent_files, local_path: Optional[Path] = None)\n</code></pre> <p>Write agent files to a temporary directory.</p> Source code in <code>nearai/agents/agent.py</code> <pre><code>@staticmethod\ndef write_agent_files_to_temp(agent_files, local_path: Optional[Path] = None):\n    \"\"\"Write agent files to a temporary directory.\"\"\"\n    unique_id = uuid.uuid4().hex\n    if local_path:\n        temp_dir = os.path.join(local_path, f\"{THREADS_DIR}/agent_{unique_id}\")\n        print(f\"Temp run folder created: {temp_dir}\")\n    else:\n        temp_dir = os.path.join(tempfile.gettempdir(), f\"agent_{unique_id}\")\n\n    if isinstance(agent_files, List):\n        os.makedirs(temp_dir, exist_ok=True)\n\n        for agent_file in agent_files:\n            if isinstance(agent_file, dict):\n                filename = agent_file[\"filename\"]\n                content = agent_file[\"content\"]\n            else:\n                assert isinstance(agent_file, Path)\n                assert local_path\n                filename = os.path.relpath(agent_file, local_path)\n                with open(agent_file, \"rb\") as f:\n                    content = f.read()\n\n            file_path = os.path.join(temp_dir, filename)\n\n            try:\n                if not os.path.exists(os.path.dirname(file_path)):\n                    os.makedirs(os.path.dirname(file_path))\n\n                if isinstance(content, dict) or isinstance(content, list):\n                    try:\n                        content = json.dumps(content)\n                    except Exception as e:\n                        print(f\"Error converting content to json: {e}\")\n                    content = str(content)\n\n                if isinstance(content, str):\n                    content = content.encode(\"utf-8\")\n\n                with open(file_path, \"wb\") as f:\n                    with io.BytesIO(content) as byte_stream:\n                        shutil.copyfileobj(byte_stream, f)\n            except Exception as e:\n                print(f\"Error writing file {file_path}: {e}\")\n                raise e\n\n    else:\n        # if agent files is a PosixPath, it is a path to the agent directory\n        # Copy all agent files including subfolders\n        shutil.copytree(agent_files, temp_dir, dirs_exist_ok=True)\n\n    return temp_dir\n</code></pre>"},{"location":"api/#nearai.agents.agent.clear_module_cache","title":"clear_module_cache","text":"<pre><code>clear_module_cache(module_names, namespace)\n</code></pre> <p>Clears specified modules from the cache before executing the main code.</p> <p>When executing agent code that imports utility modules from different locations, Python's module caching can sometimes use cached versions from the wrong location instead of importing from the agent's directory.</p> <p>This function removes modules from sys.modules to ensure they're freshly imported when used in subsequent code executions, preventing issues with cached imports.</p> <pre><code>module_names: List of module names to clear from cache\nnamespace: Dictionary namespace for code execution\n</code></pre> Source code in <code>nearai/agents/agent.py</code> <pre><code>def clear_module_cache(module_names, namespace):\n    \"\"\"Clears specified modules from the cache before executing the main code.\n\n    When executing agent code that imports utility modules from different locations,\n    Python's module caching can sometimes use cached versions from the wrong location\n    instead of importing from the agent's directory.\n\n    This function removes modules from sys.modules to ensure they're freshly\n    imported when used in subsequent code executions, preventing issues with\n    cached imports.\n\n    Args:\n    ----\n        module_names: List of module names to clear from cache\n        namespace: Dictionary namespace for code execution\n\n    \"\"\"\n    cleanup_code = \"import sys\\n\"\n    for module_name in module_names:\n        cleanup_code += f\"if '{module_name}' in sys.modules:\\n\"\n        cleanup_code += f\"    del sys.modules['{module_name}']\\n\"\n\n    exec(cleanup_code, namespace)\n</code></pre>"},{"location":"api/#nearai.agents.agent.get_local_agent_files","title":"get_local_agent_files","text":"<pre><code>get_local_agent_files(path: Path) -&gt; List[Path]\n</code></pre> <p>List of local agent files.</p> <p>Files matching patterns in .gitignore (if present) are excluded.</p> Source code in <code>nearai/agents/agent.py</code> <pre><code>def get_local_agent_files(path: Path) -&gt; List[Path]:\n    \"\"\"List of local agent files.\n\n    Files matching patterns in .gitignore (if present) are excluded.\n    \"\"\"\n    # Initialize gitignore matcher\n    gitignore_spec = None\n    try:\n        import pathspec\n\n        gitignore_path = path / \".gitignore\"\n        if gitignore_path.exists() and gitignore_path.is_file():\n            with open(gitignore_path, \"r\") as f:\n                print(\".gitignore file detected. Will filter out git ignore files.\\n\")\n                # Start with Git's default ignore patterns\n                default_ignore_patterns = [\n                    # Git internal directories\n                    \".git/\",\n                    \".gitignore\",\n                    \".gitmodules\",\n                    \".gitattributes\",\n                    # Python specific\n                    \"__pycache__/\",\n                    \"*.py[cod]\",\n                    \"*$py.class\",\n                    \"*.so\",\n                    \".Python\",\n                    \"build/\",\n                    \"develop-eggs/\",\n                    \"dist/\",\n                    \"downloads/\",\n                    \"eggs/\",\n                    \".eggs/\",\n                    \"lib/\",\n                    \"lib64/\",\n                    \"parts/\",\n                    \"sdist/\",\n                    \"var/\",\n                    \"wheels/\",\n                    \"*.egg-info/\",\n                    \".installed.cfg\",\n                    \"*.egg\",\n                    # Common cache directories\n                    \".ruff_cache/\",\n                    \".pytest_cache/\",\n                    \".mypy_cache/\",\n                    \".hypothesis/\",\n                    \".coverage\",\n                    \"htmlcov/\",\n                    \".tox/\",\n                    \".nox/\",\n                    # Virtual environments\n                    \"venv/\",\n                    \"env/\",\n                    \".env/\",\n                    \".venv/\",\n                    \"ENV/\",\n                    # Jupyter Notebook\n                    \".ipynb_checkpoints\",\n                    # IDE specific\n                    \".idea/\",\n                    \".vscode/\",\n                    \"*.swp\",\n                    \"*.swo\",\n                    # macOS specific\n                    \".DS_Store\",\n                    \".AppleDouble\",\n                    \".LSOverride\",\n                    # Windows specific\n                    \"Thumbs.db\",\n                    \"ehthumbs.db\",\n                    \"Desktop.ini\",\n                ]\n                custom_patterns = f.readlines()\n                gitignore_spec = pathspec.PathSpec.from_lines(\"gitwildmatch\", default_ignore_patterns + custom_patterns)\n    except ImportError:\n        print(\"Error: pathspec library not found. .gitignore patterns will not be applied.\")\n        exit(1)\n    except Exception as e:\n        print(f\"Error: Failed to parse .gitignore file: {str(e)}\")\n        exit(1)\n\n    all_files = []\n\n    # Traverse all files in the directory `path`\n    for file in path.rglob(\"*\"):\n        if not file.is_file():\n            continue\n\n        relative = file.relative_to(path)\n        ignore_file = False\n\n        # Filter out backup files.\n        if not ignore_file and file.name.endswith(\"~\"):\n            ignore_file = True\n\n        # Filter out configuration files.\n        if not ignore_file and relative.parts[0] == \".nearai\":\n            ignore_file = True\n\n        # Filter out thread files.\n        if not ignore_file and relative.parts[0] == THREADS_DIR:\n            ignore_file = True\n\n        # Filter out __pycache__\n        if not ignore_file and \"__pycache__\" in relative.parts:\n            ignore_file = True\n\n        # Check if file matches gitignore patterns\n        if not ignore_file and gitignore_spec is not None:\n            rel_str = str(relative).replace(\"\\\\\", \"/\")\n            if gitignore_spec.match_file(rel_str):\n                ignore_file = True\n\n        if ignore_file:\n            continue\n\n        all_files.append(file)\n\n    print(\"Agent files:\")\n    for file in all_files:\n        relative = file.relative_to(path)\n        print(f\"-   {relative}\")\n    print(\"\")\n\n    return all_files\n</code></pre>"},{"location":"api/#nearai.agents.analytics","title":"analytics","text":""},{"location":"api/#nearai.agents.analytics.AnalyticsCollector","title":"AnalyticsCollector","text":"<p>Collects and manages analytics data for agent runs.</p> Source code in <code>nearai/agents/analytics.py</code> <pre><code>class AnalyticsCollector:\n    \"\"\"Collects and manages analytics data for agent runs.\"\"\"\n\n    def __init__(self, agent: Agent, debug_mode: bool, env_init_metrics: EnvInitMetrics, upload_entry_fn=None):  # noqa: D107\n        self._agent = agent\n        self._debug_mode = debug_mode\n        self._upload_entry_fn = upload_entry_fn\n        self.env_init_metrics = env_init_metrics\n\n        # Track API calls: {method_name: [call_data, ...]}\n        self.api_calls: Dict[str, List[Dict[str, Any]]] = {}\n        # Track other metrics\n        self.custom_metrics: Dict[str, Any] = {}\n\n    def init_env_run_metrics(self, runner_metrics: Optional[RunnerMetrics]):  # noqa: D102\n        self.start_time = time.time()\n        self.start_time_utc = datetime.now(timezone.utc)\n        self.start_time_local = datetime.now()\n        self.api_calls = {}\n        self.custom_metrics = {}\n        self.runner_metrics = runner_metrics\n        if self.runner_metrics:\n            self.runner_metrics.notify_of_next_step(self.start_time)\n        self.env_init_metrics.notify_of_next_step(self.start_time)\n\n    def record_api_call(self, method_name: str, latency_ms: float, success: bool, error: Optional[str] = None):\n        \"\"\"Record an API call with its metrics.\"\"\"\n        if method_name not in self.api_calls:\n            self.api_calls[method_name] = []\n\n        self.api_calls[method_name].append(\n            {\"timestamp\": time.time(), \"latency_ms\": latency_ms, \"success\": success, \"error\": error}\n        )\n        if self.env_init_metrics.ongoing():\n            self.env_init_metrics.num_api_calls += 1\n\n    def add_custom_metric(self, key: str, value: Any, description: str):\n        \"\"\"Add a custom metric.\"\"\"\n        self.custom_metrics[key] = {\"value\": value, \"description\": description}\n\n    def _get_agent_metadata(self) -&gt; Dict[str, Any]:\n        \"\"\"Extract metadata about the agent and run environment.\"\"\"\n        try:\n            agent_metadata = self._agent.metadata\n            return {\n                \"agent_full_name\": self._agent.get_full_name(),\n                \"agent_namespace\": self._agent.namespace,\n                \"agent_name\": self._agent.name,\n                \"agent_version\": self._agent.version,\n                \"model\": self._agent.model,\n                \"model_provider\": self._agent.model_provider,\n                \"model_temperature\": self._agent.model_temperature,\n                \"model_max_tokens\": self._agent.model_max_tokens,\n                \"framework\": agent_metadata.get(\"details\", {}).get(\"agent\", {}).get(\"framework\", \"minimal\"),\n                \"local\": self._agent.is_local,\n            }\n        except Exception as e:\n            return {\"agent_metadata_error\": str(e)}\n\n    def generate_metrics(self) -&gt; Dict[str, Any]:\n        \"\"\"Generate the final metrics structure.\"\"\"\n        end_time = time.time()\n        end_time_utc = datetime.now(timezone.utc)\n        end_time_local = datetime.now()\n\n        # Build metadata\n        metadata = {\n            # Time information\n            \"start_env_run_time_utc\": self.start_time_utc.isoformat(),\n            \"end_env_run_time_utc\": end_time_utc.isoformat(),\n            \"start_env_run_time_local\": self.start_time_local.isoformat(),\n            \"end_env_run_time_local\": end_time_local.isoformat(),\n            \"debug_mode\": self._debug_mode,\n        }\n        metadata.update(self._get_agent_metadata())\n\n        total_env_run_time_ms = (end_time - self.start_time) * 1000\n        # Build metrics\n        metrics = {\n            \"performance/latency/total_env_run_ms\": {\n                \"value\": total_env_run_time_ms,\n                \"description\": \"Total agent run time in milliseconds, not including any initialization\",\n            }\n        }\n\n        env_init_latency_ms = self.env_init_metrics.latency_ms\n        metrics[\"performance/latency/env_init_latency_ms\"] = {\n            \"value\": env_init_latency_ms,\n            \"description\": \"Environment initialization time\",\n        }\n        env_init_percentage = (env_init_latency_ms / (env_init_latency_ms + total_env_run_time_ms)) * 100\n        metrics[\"performance/env_init_time_percentage\"] = {\n            \"value\": round(env_init_percentage, 2),\n            \"description\": \"env_init_latency/(env_init_latency+env_run_time)\",\n        }\n\n        if self.runner_metrics:\n            runner_latency_ms = self.runner_metrics.latency_ms\n            metrics[\"performance/latency/runner_latency_ms\"] = {\n                \"value\": runner_latency_ms,\n                \"description\": \"Runner start time.\",\n            }\n            runner_percentage = (runner_latency_ms / (runner_latency_ms + total_env_run_time_ms)) * 100\n            metrics[\"performance/runner_time_percentage\"] = {\n                \"value\": round(runner_percentage, 2),\n                \"description\": \"runner_latency/(runner_latency+env_run_time)\",\n            }\n\n        total_init_and_env_run_time_ms = total_env_run_time_ms\n        if self.runner_metrics:\n            # Environment init time is included in runner time.\n            total_init_and_env_run_time_ms += runner_latency_ms\n        else:\n            total_init_and_env_run_time_ms += env_init_latency_ms\n        metrics[\"performance/latency/total_init_and_env_run_ms\"] = {\n            \"value\": total_init_and_env_run_time_ms,\n            \"description\": \"Total agent init and run time in milliseconds, including runner start time\",\n        }\n\n        # Add API call metrics\n        total_api_calls = 0\n        total_api_latency = 0\n        total_completion_api_latency = 0\n        total_successful_calls = 0\n        total_failed_calls = 0\n        all_errors = []\n        error_summary: dict[str, Any] = {}\n\n        if self.env_init_metrics.num_api_calls &gt; 0:\n            self.add_custom_metric(\n                \"api_calls/env_init/count\", self.env_init_metrics.num_api_calls, \"Num api calls during Environment init\"\n            )\n\n        for method_name, calls in self.api_calls.items():\n            call_count = len(calls)\n            successful_calls = len([c for c in calls if c[\"success\"]])\n            failed_calls = call_count - successful_calls\n            total_latency = sum(c[\"latency_ms\"] for c in calls)\n            avg_latency = total_latency / call_count if call_count &gt; 0 else 0\n            min_latency = min(c[\"latency_ms\"] for c in calls) if call_count &gt; 0 else 0\n            max_latency = max(c[\"latency_ms\"] for c in calls) if call_count &gt; 0 else 0\n\n            # Collect errors for this method\n            method_errors = [c[\"error\"] for c in calls if c[\"error\"] is not None]\n            all_errors.extend(method_errors)\n\n            # Count error types for this method\n            method_error_counts: dict[str, Any] = {}\n            for error in method_errors:\n                error_type = (\n                    type(error).__name__\n                    if hasattr(error, \"__class__\")\n                    else str(error).split(\":\")[0]\n                    if \":\" in str(error)\n                    else \"UnknownError\"\n                )\n                method_error_counts[error_type] = method_error_counts.get(error_type, 0) + 1\n                error_summary[error_type] = error_summary.get(error_type, 0) + 1\n\n            # Clean method name for metrics key (replace dots with underscores)\n            clean_method_name = method_name.replace(\".\", \"_\")\n            completion_call = \"_completion\" in clean_method_name\n            base_key = f\"api_calls/{clean_method_name}\"\n\n            # Update totals\n            total_api_calls += call_count\n            total_api_latency += total_latency\n            if completion_call:\n                total_completion_api_latency += total_latency\n            total_successful_calls += successful_calls\n            total_failed_calls += failed_calls\n\n            metrics[f\"{base_key}/count\"] = {\"value\": call_count, \"description\": f\"Number of calls to {method_name}\"}\n\n            metrics[f\"{base_key}/successful_count\"] = {\n                \"value\": successful_calls,\n                \"description\": f\"Number of successful calls to {method_name}\",\n            }\n\n            metrics[f\"{base_key}/failed_count\"] = {\n                \"value\": failed_calls,\n                \"description\": f\"Number of failed calls to {method_name}\",\n            }\n\n            metrics[f\"{base_key}/total_latency_ms\"] = {\n                \"value\": total_latency,\n                \"description\": f\"Total latency for all calls to {method_name} in milliseconds\",\n            }\n\n            metrics[f\"{base_key}/avg_latency_ms\"] = {\n                \"value\": round(avg_latency, 2),\n                \"description\": f\"Average latency for calls to {method_name} in milliseconds\",\n            }\n\n            metrics[f\"{base_key}/min_latency_ms\"] = {\n                \"value\": round(min_latency, 2),\n                \"description\": f\"Minimum latency for calls to {method_name} in milliseconds\",\n            }\n\n            metrics[f\"{base_key}/max_latency_ms\"] = {\n                \"value\": round(max_latency, 2),\n                \"description\": f\"Maximum latency for calls to {method_name} in milliseconds\",\n            }\n\n            # Add error metrics for this method if there are any errors\n            if method_errors:\n                metrics[f\"{base_key}/errors/total_count\"] = {\n                    \"value\": len(method_errors),\n                    \"description\": f\"Total number of errors for {method_name}\",\n                }\n\n                # Add error type breakdown for this method\n                for error_type, count in method_error_counts.items():\n                    safe_error_type = error_type.replace(\".\", \"_\").replace(\" \", \"_\")\n                    metrics[f\"{base_key}/errors/by_type/{safe_error_type}\"] = {\n                        \"value\": count,\n                        \"description\": f\"Number of {error_type} errors for {method_name}\",\n                    }\n\n                # Add sample error messages (first 3 unique errors)\n                unique_errors = list(set(method_errors))[:3]\n                for i, error in enumerate(unique_errors):\n                    error_msg = str(error)[:200] + (\"...\" if len(str(error)) &gt; 200 else \"\")  # Truncate long errors\n                    metrics[f\"{base_key}/errors/samples/error_{i + 1}\"] = {\n                        \"value\": error_msg,\n                        \"description\": f\"Sample error message {i + 1} for {method_name}\",\n                    }\n\n        # Add summary metrics\n        metrics[\"api_calls/summary/total_calls\"] = {\n            \"value\": total_api_calls,\n            \"description\": \"Total number of API calls made\",\n        }\n\n        metrics[\"api_calls/summary/total_successful_calls\"] = {\n            \"value\": total_successful_calls,\n            \"description\": \"Total number of successful API calls\",\n        }\n\n        metrics[\"api_calls/summary/total_failed_calls\"] = {\n            \"value\": total_failed_calls,\n            \"description\": \"Total number of failed API calls\",\n        }\n\n        metrics[\"api_calls/summary/total_api_latency_ms\"] = {\n            \"value\": round(total_api_latency, 2),\n            \"description\": \"Total latency for all API calls in milliseconds\",\n        }\n\n        if total_api_calls &gt; 0:\n            metrics[\"api_calls/summary/avg_api_latency_ms\"] = {\n                \"value\": round(total_api_latency / total_api_calls, 2),\n                \"description\": \"Average latency per API call in milliseconds\",\n            }\n\n            metrics[\"api_calls/summary/success_rate\"] = {\n                \"value\": round(total_successful_calls / total_api_calls, 4),\n                \"description\": \"Success rate of API calls (0.0 to 1.0)\",\n            }\n\n            metrics[\"api_calls/summary/failure_rate\"] = {\n                \"value\": round(total_failed_calls / total_api_calls, 4),\n                \"description\": \"Failure rate of API calls (0.0 to 1.0)\",\n            }\n\n        # Add error summary metrics\n        if all_errors:\n            metrics[\"errors/summary/total_error_count\"] = {\n                \"value\": len(all_errors),\n                \"description\": \"Total number of errors across all API calls\",\n            }\n\n            metrics[\"errors/summary/unique_error_count\"] = {\n                \"value\": len(set(all_errors)),\n                \"description\": \"Number of unique error messages\",\n            }\n\n            # Add error type summary\n            for error_type, count in error_summary.items():\n                safe_error_type = error_type.replace(\".\", \"_\").replace(\" \", \"_\")\n                metrics[f\"errors/summary/by_type/{safe_error_type}\"] = {\n                    \"value\": count,\n                    \"description\": f\"Total number of {error_type} errors across all methods\",\n                }\n\n            # Add most common errors (top 5)\n            most_common_errors = sorted(error_summary.items(), key=lambda x: x[1], reverse=True)[:5]\n            for i, (error_type, count) in enumerate(most_common_errors):\n                safe_error_type = error_type.replace(\".\", \"_\").replace(\" \", \"_\")\n                metrics[f\"errors/summary/most_common/rank_{i + 1}\"] = {\n                    \"value\": f\"{error_type} ({count} occurrences)\",\n                    \"description\": f\"#{i + 1} most common error type\",\n                }\n\n            # Add sample of unique error messages (first 5)\n            unique_error_samples = list(set(all_errors))[:5]\n            for i, error in enumerate(unique_error_samples):\n                error_msg = str(error)[:300] + (\"...\" if len(str(error)) &gt; 300 else \"\")  # Truncate very long errors\n                metrics[f\"errors/summary/samples/error_{i + 1}\"] = {\n                    \"value\": error_msg,\n                    \"description\": f\"Sample error message #{i + 1}\",\n                }\n        else:\n            metrics[\"errors/summary/total_error_count\"] = {\n                \"value\": 0,\n                \"description\": \"Total number of errors across all API calls\",\n            }\n\n        # Calculate API percentage\n        if total_env_run_time_ms &gt; 0:\n            api_percentage = (total_api_latency / total_env_run_time_ms) * 100\n            metrics[\"performance/api_time_percentage\"] = {\n                \"value\": round(api_percentage, 2),\n                \"description\": \"total_api_latency/total_env_run_time\",\n            }\n            completion_api_percentage = (total_completion_api_latency / total_env_run_time_ms) * 100\n            metrics[\"performance/completion_api_time_percentage\"] = {\n                \"value\": round(completion_api_percentage, 2),\n                \"description\": \"total_completion_api_latency/total_env_run_time\",\n            }\n\n        # Add custom metrics\n        metrics.update(self.custom_metrics)\n\n        return {\"metadata\": metadata, \"metrics\": metrics}\n\n    def flush_to_file(self, log_dir: Path):\n        \"\"\"Write metrics to the logs/metrics.json file.\"\"\"\n        try:\n            metrics_data = self.generate_metrics()\n\n            # Ensure logs directory exists\n            log_dir.mkdir(parents=True, exist_ok=True)\n\n            # Write to metrics.json\n            metrics_file = log_dir / \"metrics.json\"\n            with open(metrics_file, \"w\") as f:\n                json.dump(metrics_data, f, indent=2)\n\n        except Exception as e:\n            print(f\"Failed to write analytics metrics: {e}\")\n\n    def upload(self, thread_dir: Path):\n        \"\"\"Upload analytics to registry.\"\"\"\n        import shutil\n\n        log_dir = thread_dir / \"logs\"\n        try:\n            shutil.rmtree(log_dir)\n        except Exception:\n            pass\n        self.flush_to_file(log_dir)\n\n        if not self._upload_entry_fn:\n            print(\"Upload function not available for analytics entry upload\")\n            return\n\n        if self._debug_mode:\n            for log_file in thread_dir.glob(\"*_log.txt\"):\n                log_filename = log_file.name\n                source_path = thread_dir / log_filename\n                dest_path = log_dir / log_filename\n                try:\n                    shutil.copy2(source_path, dest_path)\n                except Exception as e:\n                    print(f\"Failed to copy {log_filename}: {e}\")\n\n        # Create logs name using agent name and timestamp\n        agent_name = self._agent.get_full_name()\n        timestamp = self.start_time_utc.strftime(\"%Y%m%d_%H%M%S\")\n        # Create a clean name for the logs entry\n        # Format: logs_{agent_name}_{timestamp}\n        logs_name = f\"logs_{agent_name}_{timestamp}\"\n        logs_name = re.sub(r\"[^a-zA-Z0-9_\\-.]\", \"_\", logs_name)\n        logs_description = f\"Analytics and logs for {agent_name} run at {self.start_time_utc.isoformat()}\"\n\n        metadata_path = log_dir / \"metadata.json\"\n        with open(metadata_path, \"w\") as f:\n            json.dump(\n                {\n                    \"name\": logs_name,\n                    \"version\": \"0.0.1\",\n                    \"description\": logs_description,\n                    \"category\": \"logs\",\n                    \"tags\": [],\n                    \"details\": {\"private_source\": True},\n                    \"show_entry\": True,\n                },\n                f,\n                indent=2,\n            )\n\n        self._upload_entry_fn(log_dir)\n</code></pre>"},{"location":"api/#nearai.agents.analytics.AnalyticsCollector._get_agent_metadata","title":"_get_agent_metadata","text":"<pre><code>_get_agent_metadata() -&gt; Dict[str, Any]\n</code></pre> <p>Extract metadata about the agent and run environment.</p> Source code in <code>nearai/agents/analytics.py</code> <pre><code>def _get_agent_metadata(self) -&gt; Dict[str, Any]:\n    \"\"\"Extract metadata about the agent and run environment.\"\"\"\n    try:\n        agent_metadata = self._agent.metadata\n        return {\n            \"agent_full_name\": self._agent.get_full_name(),\n            \"agent_namespace\": self._agent.namespace,\n            \"agent_name\": self._agent.name,\n            \"agent_version\": self._agent.version,\n            \"model\": self._agent.model,\n            \"model_provider\": self._agent.model_provider,\n            \"model_temperature\": self._agent.model_temperature,\n            \"model_max_tokens\": self._agent.model_max_tokens,\n            \"framework\": agent_metadata.get(\"details\", {}).get(\"agent\", {}).get(\"framework\", \"minimal\"),\n            \"local\": self._agent.is_local,\n        }\n    except Exception as e:\n        return {\"agent_metadata_error\": str(e)}\n</code></pre>"},{"location":"api/#nearai.agents.analytics.AnalyticsCollector.add_custom_metric","title":"add_custom_metric","text":"<pre><code>add_custom_metric(key: str, value: Any, description: str)\n</code></pre> <p>Add a custom metric.</p> Source code in <code>nearai/agents/analytics.py</code> <pre><code>def add_custom_metric(self, key: str, value: Any, description: str):\n    \"\"\"Add a custom metric.\"\"\"\n    self.custom_metrics[key] = {\"value\": value, \"description\": description}\n</code></pre>"},{"location":"api/#nearai.agents.analytics.AnalyticsCollector.flush_to_file","title":"flush_to_file","text":"<pre><code>flush_to_file(log_dir: Path)\n</code></pre> <p>Write metrics to the logs/metrics.json file.</p> Source code in <code>nearai/agents/analytics.py</code> <pre><code>def flush_to_file(self, log_dir: Path):\n    \"\"\"Write metrics to the logs/metrics.json file.\"\"\"\n    try:\n        metrics_data = self.generate_metrics()\n\n        # Ensure logs directory exists\n        log_dir.mkdir(parents=True, exist_ok=True)\n\n        # Write to metrics.json\n        metrics_file = log_dir / \"metrics.json\"\n        with open(metrics_file, \"w\") as f:\n            json.dump(metrics_data, f, indent=2)\n\n    except Exception as e:\n        print(f\"Failed to write analytics metrics: {e}\")\n</code></pre>"},{"location":"api/#nearai.agents.analytics.AnalyticsCollector.generate_metrics","title":"generate_metrics","text":"<pre><code>generate_metrics() -&gt; Dict[str, Any]\n</code></pre> <p>Generate the final metrics structure.</p> Source code in <code>nearai/agents/analytics.py</code> <pre><code>def generate_metrics(self) -&gt; Dict[str, Any]:\n    \"\"\"Generate the final metrics structure.\"\"\"\n    end_time = time.time()\n    end_time_utc = datetime.now(timezone.utc)\n    end_time_local = datetime.now()\n\n    # Build metadata\n    metadata = {\n        # Time information\n        \"start_env_run_time_utc\": self.start_time_utc.isoformat(),\n        \"end_env_run_time_utc\": end_time_utc.isoformat(),\n        \"start_env_run_time_local\": self.start_time_local.isoformat(),\n        \"end_env_run_time_local\": end_time_local.isoformat(),\n        \"debug_mode\": self._debug_mode,\n    }\n    metadata.update(self._get_agent_metadata())\n\n    total_env_run_time_ms = (end_time - self.start_time) * 1000\n    # Build metrics\n    metrics = {\n        \"performance/latency/total_env_run_ms\": {\n            \"value\": total_env_run_time_ms,\n            \"description\": \"Total agent run time in milliseconds, not including any initialization\",\n        }\n    }\n\n    env_init_latency_ms = self.env_init_metrics.latency_ms\n    metrics[\"performance/latency/env_init_latency_ms\"] = {\n        \"value\": env_init_latency_ms,\n        \"description\": \"Environment initialization time\",\n    }\n    env_init_percentage = (env_init_latency_ms / (env_init_latency_ms + total_env_run_time_ms)) * 100\n    metrics[\"performance/env_init_time_percentage\"] = {\n        \"value\": round(env_init_percentage, 2),\n        \"description\": \"env_init_latency/(env_init_latency+env_run_time)\",\n    }\n\n    if self.runner_metrics:\n        runner_latency_ms = self.runner_metrics.latency_ms\n        metrics[\"performance/latency/runner_latency_ms\"] = {\n            \"value\": runner_latency_ms,\n            \"description\": \"Runner start time.\",\n        }\n        runner_percentage = (runner_latency_ms / (runner_latency_ms + total_env_run_time_ms)) * 100\n        metrics[\"performance/runner_time_percentage\"] = {\n            \"value\": round(runner_percentage, 2),\n            \"description\": \"runner_latency/(runner_latency+env_run_time)\",\n        }\n\n    total_init_and_env_run_time_ms = total_env_run_time_ms\n    if self.runner_metrics:\n        # Environment init time is included in runner time.\n        total_init_and_env_run_time_ms += runner_latency_ms\n    else:\n        total_init_and_env_run_time_ms += env_init_latency_ms\n    metrics[\"performance/latency/total_init_and_env_run_ms\"] = {\n        \"value\": total_init_and_env_run_time_ms,\n        \"description\": \"Total agent init and run time in milliseconds, including runner start time\",\n    }\n\n    # Add API call metrics\n    total_api_calls = 0\n    total_api_latency = 0\n    total_completion_api_latency = 0\n    total_successful_calls = 0\n    total_failed_calls = 0\n    all_errors = []\n    error_summary: dict[str, Any] = {}\n\n    if self.env_init_metrics.num_api_calls &gt; 0:\n        self.add_custom_metric(\n            \"api_calls/env_init/count\", self.env_init_metrics.num_api_calls, \"Num api calls during Environment init\"\n        )\n\n    for method_name, calls in self.api_calls.items():\n        call_count = len(calls)\n        successful_calls = len([c for c in calls if c[\"success\"]])\n        failed_calls = call_count - successful_calls\n        total_latency = sum(c[\"latency_ms\"] for c in calls)\n        avg_latency = total_latency / call_count if call_count &gt; 0 else 0\n        min_latency = min(c[\"latency_ms\"] for c in calls) if call_count &gt; 0 else 0\n        max_latency = max(c[\"latency_ms\"] for c in calls) if call_count &gt; 0 else 0\n\n        # Collect errors for this method\n        method_errors = [c[\"error\"] for c in calls if c[\"error\"] is not None]\n        all_errors.extend(method_errors)\n\n        # Count error types for this method\n        method_error_counts: dict[str, Any] = {}\n        for error in method_errors:\n            error_type = (\n                type(error).__name__\n                if hasattr(error, \"__class__\")\n                else str(error).split(\":\")[0]\n                if \":\" in str(error)\n                else \"UnknownError\"\n            )\n            method_error_counts[error_type] = method_error_counts.get(error_type, 0) + 1\n            error_summary[error_type] = error_summary.get(error_type, 0) + 1\n\n        # Clean method name for metrics key (replace dots with underscores)\n        clean_method_name = method_name.replace(\".\", \"_\")\n        completion_call = \"_completion\" in clean_method_name\n        base_key = f\"api_calls/{clean_method_name}\"\n\n        # Update totals\n        total_api_calls += call_count\n        total_api_latency += total_latency\n        if completion_call:\n            total_completion_api_latency += total_latency\n        total_successful_calls += successful_calls\n        total_failed_calls += failed_calls\n\n        metrics[f\"{base_key}/count\"] = {\"value\": call_count, \"description\": f\"Number of calls to {method_name}\"}\n\n        metrics[f\"{base_key}/successful_count\"] = {\n            \"value\": successful_calls,\n            \"description\": f\"Number of successful calls to {method_name}\",\n        }\n\n        metrics[f\"{base_key}/failed_count\"] = {\n            \"value\": failed_calls,\n            \"description\": f\"Number of failed calls to {method_name}\",\n        }\n\n        metrics[f\"{base_key}/total_latency_ms\"] = {\n            \"value\": total_latency,\n            \"description\": f\"Total latency for all calls to {method_name} in milliseconds\",\n        }\n\n        metrics[f\"{base_key}/avg_latency_ms\"] = {\n            \"value\": round(avg_latency, 2),\n            \"description\": f\"Average latency for calls to {method_name} in milliseconds\",\n        }\n\n        metrics[f\"{base_key}/min_latency_ms\"] = {\n            \"value\": round(min_latency, 2),\n            \"description\": f\"Minimum latency for calls to {method_name} in milliseconds\",\n        }\n\n        metrics[f\"{base_key}/max_latency_ms\"] = {\n            \"value\": round(max_latency, 2),\n            \"description\": f\"Maximum latency for calls to {method_name} in milliseconds\",\n        }\n\n        # Add error metrics for this method if there are any errors\n        if method_errors:\n            metrics[f\"{base_key}/errors/total_count\"] = {\n                \"value\": len(method_errors),\n                \"description\": f\"Total number of errors for {method_name}\",\n            }\n\n            # Add error type breakdown for this method\n            for error_type, count in method_error_counts.items():\n                safe_error_type = error_type.replace(\".\", \"_\").replace(\" \", \"_\")\n                metrics[f\"{base_key}/errors/by_type/{safe_error_type}\"] = {\n                    \"value\": count,\n                    \"description\": f\"Number of {error_type} errors for {method_name}\",\n                }\n\n            # Add sample error messages (first 3 unique errors)\n            unique_errors = list(set(method_errors))[:3]\n            for i, error in enumerate(unique_errors):\n                error_msg = str(error)[:200] + (\"...\" if len(str(error)) &gt; 200 else \"\")  # Truncate long errors\n                metrics[f\"{base_key}/errors/samples/error_{i + 1}\"] = {\n                    \"value\": error_msg,\n                    \"description\": f\"Sample error message {i + 1} for {method_name}\",\n                }\n\n    # Add summary metrics\n    metrics[\"api_calls/summary/total_calls\"] = {\n        \"value\": total_api_calls,\n        \"description\": \"Total number of API calls made\",\n    }\n\n    metrics[\"api_calls/summary/total_successful_calls\"] = {\n        \"value\": total_successful_calls,\n        \"description\": \"Total number of successful API calls\",\n    }\n\n    metrics[\"api_calls/summary/total_failed_calls\"] = {\n        \"value\": total_failed_calls,\n        \"description\": \"Total number of failed API calls\",\n    }\n\n    metrics[\"api_calls/summary/total_api_latency_ms\"] = {\n        \"value\": round(total_api_latency, 2),\n        \"description\": \"Total latency for all API calls in milliseconds\",\n    }\n\n    if total_api_calls &gt; 0:\n        metrics[\"api_calls/summary/avg_api_latency_ms\"] = {\n            \"value\": round(total_api_latency / total_api_calls, 2),\n            \"description\": \"Average latency per API call in milliseconds\",\n        }\n\n        metrics[\"api_calls/summary/success_rate\"] = {\n            \"value\": round(total_successful_calls / total_api_calls, 4),\n            \"description\": \"Success rate of API calls (0.0 to 1.0)\",\n        }\n\n        metrics[\"api_calls/summary/failure_rate\"] = {\n            \"value\": round(total_failed_calls / total_api_calls, 4),\n            \"description\": \"Failure rate of API calls (0.0 to 1.0)\",\n        }\n\n    # Add error summary metrics\n    if all_errors:\n        metrics[\"errors/summary/total_error_count\"] = {\n            \"value\": len(all_errors),\n            \"description\": \"Total number of errors across all API calls\",\n        }\n\n        metrics[\"errors/summary/unique_error_count\"] = {\n            \"value\": len(set(all_errors)),\n            \"description\": \"Number of unique error messages\",\n        }\n\n        # Add error type summary\n        for error_type, count in error_summary.items():\n            safe_error_type = error_type.replace(\".\", \"_\").replace(\" \", \"_\")\n            metrics[f\"errors/summary/by_type/{safe_error_type}\"] = {\n                \"value\": count,\n                \"description\": f\"Total number of {error_type} errors across all methods\",\n            }\n\n        # Add most common errors (top 5)\n        most_common_errors = sorted(error_summary.items(), key=lambda x: x[1], reverse=True)[:5]\n        for i, (error_type, count) in enumerate(most_common_errors):\n            safe_error_type = error_type.replace(\".\", \"_\").replace(\" \", \"_\")\n            metrics[f\"errors/summary/most_common/rank_{i + 1}\"] = {\n                \"value\": f\"{error_type} ({count} occurrences)\",\n                \"description\": f\"#{i + 1} most common error type\",\n            }\n\n        # Add sample of unique error messages (first 5)\n        unique_error_samples = list(set(all_errors))[:5]\n        for i, error in enumerate(unique_error_samples):\n            error_msg = str(error)[:300] + (\"...\" if len(str(error)) &gt; 300 else \"\")  # Truncate very long errors\n            metrics[f\"errors/summary/samples/error_{i + 1}\"] = {\n                \"value\": error_msg,\n                \"description\": f\"Sample error message #{i + 1}\",\n            }\n    else:\n        metrics[\"errors/summary/total_error_count\"] = {\n            \"value\": 0,\n            \"description\": \"Total number of errors across all API calls\",\n        }\n\n    # Calculate API percentage\n    if total_env_run_time_ms &gt; 0:\n        api_percentage = (total_api_latency / total_env_run_time_ms) * 100\n        metrics[\"performance/api_time_percentage\"] = {\n            \"value\": round(api_percentage, 2),\n            \"description\": \"total_api_latency/total_env_run_time\",\n        }\n        completion_api_percentage = (total_completion_api_latency / total_env_run_time_ms) * 100\n        metrics[\"performance/completion_api_time_percentage\"] = {\n            \"value\": round(completion_api_percentage, 2),\n            \"description\": \"total_completion_api_latency/total_env_run_time\",\n        }\n\n    # Add custom metrics\n    metrics.update(self.custom_metrics)\n\n    return {\"metadata\": metadata, \"metrics\": metrics}\n</code></pre>"},{"location":"api/#nearai.agents.analytics.AnalyticsCollector.record_api_call","title":"record_api_call","text":"<pre><code>record_api_call(method_name: str, latency_ms: float, success: bool, error: Optional[str] = None)\n</code></pre> <p>Record an API call with its metrics.</p> Source code in <code>nearai/agents/analytics.py</code> <pre><code>def record_api_call(self, method_name: str, latency_ms: float, success: bool, error: Optional[str] = None):\n    \"\"\"Record an API call with its metrics.\"\"\"\n    if method_name not in self.api_calls:\n        self.api_calls[method_name] = []\n\n    self.api_calls[method_name].append(\n        {\"timestamp\": time.time(), \"latency_ms\": latency_ms, \"success\": success, \"error\": error}\n    )\n    if self.env_init_metrics.ongoing():\n        self.env_init_metrics.num_api_calls += 1\n</code></pre>"},{"location":"api/#nearai.agents.analytics.AnalyticsCollector.upload","title":"upload","text":"<pre><code>upload(thread_dir: Path)\n</code></pre> <p>Upload analytics to registry.</p> Source code in <code>nearai/agents/analytics.py</code> <pre><code>def upload(self, thread_dir: Path):\n    \"\"\"Upload analytics to registry.\"\"\"\n    import shutil\n\n    log_dir = thread_dir / \"logs\"\n    try:\n        shutil.rmtree(log_dir)\n    except Exception:\n        pass\n    self.flush_to_file(log_dir)\n\n    if not self._upload_entry_fn:\n        print(\"Upload function not available for analytics entry upload\")\n        return\n\n    if self._debug_mode:\n        for log_file in thread_dir.glob(\"*_log.txt\"):\n            log_filename = log_file.name\n            source_path = thread_dir / log_filename\n            dest_path = log_dir / log_filename\n            try:\n                shutil.copy2(source_path, dest_path)\n            except Exception as e:\n                print(f\"Failed to copy {log_filename}: {e}\")\n\n    # Create logs name using agent name and timestamp\n    agent_name = self._agent.get_full_name()\n    timestamp = self.start_time_utc.strftime(\"%Y%m%d_%H%M%S\")\n    # Create a clean name for the logs entry\n    # Format: logs_{agent_name}_{timestamp}\n    logs_name = f\"logs_{agent_name}_{timestamp}\"\n    logs_name = re.sub(r\"[^a-zA-Z0-9_\\-.]\", \"_\", logs_name)\n    logs_description = f\"Analytics and logs for {agent_name} run at {self.start_time_utc.isoformat()}\"\n\n    metadata_path = log_dir / \"metadata.json\"\n    with open(metadata_path, \"w\") as f:\n        json.dump(\n            {\n                \"name\": logs_name,\n                \"version\": \"0.0.1\",\n                \"description\": logs_description,\n                \"category\": \"logs\",\n                \"tags\": [],\n                \"details\": {\"private_source\": True},\n                \"show_entry\": True,\n            },\n            f,\n            indent=2,\n        )\n\n    self._upload_entry_fn(log_dir)\n</code></pre>"},{"location":"api/#nearai.agents.analytics.AnalyticsWrapper","title":"AnalyticsWrapper","text":"<p>Wrapper that tracks method calls and latencies for any client object.</p> Source code in <code>nearai/agents/analytics.py</code> <pre><code>class AnalyticsWrapper:\n    \"\"\"Wrapper that tracks method calls and latencies for any client object.\"\"\"\n\n    def __init__(self, client: Any, client_name: str, analytics_collector: \"AnalyticsCollector\"):  # noqa: D107\n        self._client = client\n        self._client_name = client_name\n        self._analytics_collector = analytics_collector\n\n        # Store original client for direct access if needed\n        self.__dict__[\"_client\"] = client\n        self.__dict__[\"_client_name\"] = client_name\n        self.__dict__[\"_analytics_collector\"] = analytics_collector\n\n    def __getattr__(self, name: str) -&gt; Any:\n        \"\"\"Intercept attribute access to wrap methods or return original attributes.\"\"\"\n        attr = getattr(self._client, name)\n\n        if callable(attr):\n            # Wrap callable methods\n            return self._wrap_method(attr, name)\n        else:\n            # For non-callable attributes that are objects, wrap them but give them access to root client\n            if hasattr(attr, \"__dict__\") and not isinstance(attr, (str, int, float, bool, type(None))):\n                nested_client_name = f\"{self._client_name}.{name}\"\n                return AnalyticsWrapper(attr, nested_client_name, self._analytics_collector)\n            else:\n                # Return simple attributes unwrapped\n                return attr\n\n    def _wrap_method(self, method: Any, method_name: str):\n        \"\"\"Wrap a method to track calls and latency.\"\"\"\n\n        @functools.wraps(method)\n        def wrapper(*args, **kwargs):\n            full_method_name = f\"{self._client_name}.{method_name}\"\n            start_time = time.time()\n\n            try:\n                result = method(*args, **kwargs)\n                end_time = time.time()\n                latency_ms = (end_time - start_time) * 1000\n\n                self._analytics_collector.record_api_call(\n                    method_name=full_method_name, latency_ms=latency_ms, success=True\n                )\n\n                return result\n            except Exception as e:\n                end_time = time.time()\n                latency_ms = (end_time - start_time) * 1000\n\n                self._analytics_collector.record_api_call(\n                    method_name=full_method_name, latency_ms=latency_ms, success=False, error=str(e)\n                )\n\n                raise\n\n        return wrapper\n</code></pre>"},{"location":"api/#nearai.agents.analytics.AnalyticsWrapper.__getattr__","title":"__getattr__","text":"<pre><code>__getattr__(name: str) -&gt; Any\n</code></pre> <p>Intercept attribute access to wrap methods or return original attributes.</p> Source code in <code>nearai/agents/analytics.py</code> <pre><code>def __getattr__(self, name: str) -&gt; Any:\n    \"\"\"Intercept attribute access to wrap methods or return original attributes.\"\"\"\n    attr = getattr(self._client, name)\n\n    if callable(attr):\n        # Wrap callable methods\n        return self._wrap_method(attr, name)\n    else:\n        # For non-callable attributes that are objects, wrap them but give them access to root client\n        if hasattr(attr, \"__dict__\") and not isinstance(attr, (str, int, float, bool, type(None))):\n            nested_client_name = f\"{self._client_name}.{name}\"\n            return AnalyticsWrapper(attr, nested_client_name, self._analytics_collector)\n        else:\n            # Return simple attributes unwrapped\n            return attr\n</code></pre>"},{"location":"api/#nearai.agents.analytics.AnalyticsWrapper._wrap_method","title":"_wrap_method","text":"<pre><code>_wrap_method(method: Any, method_name: str)\n</code></pre> <p>Wrap a method to track calls and latency.</p> Source code in <code>nearai/agents/analytics.py</code> <pre><code>def _wrap_method(self, method: Any, method_name: str):\n    \"\"\"Wrap a method to track calls and latency.\"\"\"\n\n    @functools.wraps(method)\n    def wrapper(*args, **kwargs):\n        full_method_name = f\"{self._client_name}.{method_name}\"\n        start_time = time.time()\n\n        try:\n            result = method(*args, **kwargs)\n            end_time = time.time()\n            latency_ms = (end_time - start_time) * 1000\n\n            self._analytics_collector.record_api_call(\n                method_name=full_method_name, latency_ms=latency_ms, success=True\n            )\n\n            return result\n        except Exception as e:\n            end_time = time.time()\n            latency_ms = (end_time - start_time) * 1000\n\n            self._analytics_collector.record_api_call(\n                method_name=full_method_name, latency_ms=latency_ms, success=False, error=str(e)\n            )\n\n            raise\n\n    return wrapper\n</code></pre>"},{"location":"api/#nearai.agents.analytics.EnvInitMetrics","title":"EnvInitMetrics","text":"<p>Environment Init Metrics.</p> Source code in <code>nearai/agents/analytics.py</code> <pre><code>class EnvInitMetrics:\n    \"\"\"Environment Init Metrics.\"\"\"\n\n    def __init__(self):  # noqa: D107\n        self.start_time = time.time()\n        self.latency_ms = None\n        self.num_api_calls = 0\n\n    def ongoing(self) -&gt; bool:  # noqa: D102\n        return self.latency_ms is None\n\n    def notify_of_next_step(self, cur_time: Optional[float] = None):  # noqa: D102\n        if self.ongoing():\n            if not cur_time:\n                cur_time = time.time()\n            self.latency_ms = (cur_time - self.start_time) * 1000\n</code></pre>"},{"location":"api/#nearai.agents.analytics.RunnerMetrics","title":"RunnerMetrics","text":"<p>Runner Metrics.</p> Source code in <code>nearai/agents/analytics.py</code> <pre><code>class RunnerMetrics:\n    \"\"\"Runner Metrics.\"\"\"\n\n    def __init__(self):  # noqa: D107\n        self.start_time = time.time()\n        self.latency_ms = None\n\n    def ongoing(self) -&gt; bool:  # noqa: D102\n        return self.latency_ms is None\n\n    def notify_of_next_step(self, cur_time: Optional[float] = None):  # noqa: D102\n        if self.ongoing():\n            if not cur_time:\n                cur_time = time.time()\n            self.latency_ms = (cur_time - self.start_time) * 1000\n</code></pre>"},{"location":"api/#nearai.agents.analytics.create_analytics_wrapper","title":"create_analytics_wrapper","text":"<pre><code>create_analytics_wrapper(client: Any, client_name: str, analytics_collector: AnalyticsCollector) -&gt; Any\n</code></pre> <p>Create an analytics wrapper for a client.</p> Source code in <code>nearai/agents/analytics.py</code> <pre><code>def create_analytics_wrapper(client: Any, client_name: str, analytics_collector: AnalyticsCollector) -&gt; Any:\n    \"\"\"Create an analytics wrapper for a client.\"\"\"\n    return AnalyticsWrapper(client, client_name, analytics_collector)\n</code></pre>"},{"location":"api/#nearai.agents.environment","title":"environment","text":""},{"location":"api/#nearai.agents.environment.Environment","title":"Environment","text":"<p>               Bases: <code>object</code></p> Source code in <code>nearai/agents/environment.py</code> <pre><code>class Environment(object):\n    def __init__(  # noqa: D107\n        self,\n        agents: List[Agent],\n        client: InferenceClient,\n        hub_client: OpenAI,\n        thread_id: str,\n        run_id: str,\n        env_vars: Optional[Dict[str, Any]] = None,\n        tool_resources: Optional[Dict[str, Any]] = None,\n        print_system_log: bool = False,\n        agent_runner_user: Optional[str] = None,\n        fastnear_api_key: Optional[str] = None,\n        approvals=None,\n        upload_entry_fn=None,\n    ) -&gt; None:\n        # Warning: never expose `client` or `_hub_client` to agent's environment\n\n        self._initialized = False\n\n        self.base_url = client._config.base_url\n\n        # user_auth is used to authenticate the user in the ts_runner. It will be removed after that in\n        # `nearai/agents/agent.py`\n        auth = client._auth\n        self.user_auth = auth\n\n        # Initialize secure openai clients\n        openai_client_params = {\n            \"api_key\": auth,\n            \"base_url\": client._config.base_url,\n            \"default_headers\": {\"Authorization\": f\"Bearer {auth}\"},\n        }\n        self.openai = SecureOpenAI(**openai_client_params)\n        self.async_openai = SecureAsyncOpenAI(**openai_client_params)\n\n        # Placeholder for solver\n        self.client: Optional[InferenceClient] = None\n\n        self._agents = agents\n        self._pending_ext_agent = False\n        self.env_vars: Dict[str, Any] = env_vars if env_vars else {}\n        self._last_used_model = \"\"\n        self.tool_resources: Dict[str, Any] = tool_resources if tool_resources else {}\n        self.print_system_log = print_system_log\n        self.agent_runner_user = agent_runner_user\n        self._approvals = approvals if approvals else default_approvals\n        self._thread_id = thread_id\n        self._run_id = run_id\n        self._debug_mode = is_debug_mode(self.env_vars)\n        self._async_api_calls = self.env_vars.get(\"ASYNC_API_CALLS\", \"true\").lower() in (\"true\", \"1\", \"yes\", \"on\")\n\n        # Initialize caches\n        self._messages_cache: Optional[List[Message]] = None\n        self._files_from_thread_cache: Optional[List[FileObject]] = None\n\n        # Initialize analytics collection if enabled\n        self.logs_collection_mode = is_logs_collection_mode(self.env_vars)\n        env_init_metrics = EnvInitMetrics()\n        print(f\"logs_collection_mode: {self.logs_collection_mode}\")\n        self.analytics_collector: Optional[AnalyticsCollector] = None\n        if self.logs_collection_mode:\n            self.analytics_collector = AnalyticsCollector(\n                agent=agents[0],\n                debug_mode=self._debug_mode,\n                upload_entry_fn=upload_entry_fn,\n                env_init_metrics=env_init_metrics,\n            )\n            # Wrap clients with analytics\n            client = create_analytics_wrapper(client, \"inference_client\", self.analytics_collector)\n            hub_client = create_analytics_wrapper(hub_client, \"hub_client\", self.analytics_collector)\n            self.openai = create_analytics_wrapper(self.openai, \"openai_client\", self.analytics_collector)\n            self.async_openai = create_analytics_wrapper(\n                self.async_openai, \"async_openai_client\", self.analytics_collector\n            )\n\n        # Expose the NEAR account_id of a user that signs this request to run an agent.\n        self.signer_account_id: str = client._config.auth.account_id if client._config.auth else \"\"\n\n        if fastnear_api_key:\n            default_mainnet_rpc = f\"https://{fastnear_api_key}@rpc.mainnet.fastnear.com\"\n        else:\n            default_mainnet_rpc = \"https://rpc.mainnet.near.org\"\n\n        class NearAccount(Account):\n            user_rpc_addr: Union[str, None]\n\n            async def view(\n                self,\n                contract_id: str,\n                method_name: str,\n                args: dict,\n                block_id: Optional[int] = None,\n                threshold: Optional[int] = None,\n                max_retries: int = 3,\n            ):\n                \"\"\"Wrapper for the view method of the Account class, adding multiple retry attempts.\n\n                Parameters\n                ----------\n                contract_id : str\n                    The ID of the contract to call.\n                method_name : str\n                    The name of the method to invoke on the contract.\n                args : dict\n                    The arguments to pass to the contract method.\n                block_id : Optional[int]\n                    The block ID to query at.\n                threshold : Optional[int]\n                    The threshold for the view function.\n                max_retries : int\n                    The maximum number of retry attempts.\n\n                Returns\n                -------\n                The result of the contract method call.\n\n                Raises\n                ------\n                Exception\n                    If all retry attempts fail, the exception is propagated.\n\n                \"\"\"\n                acc = Account(self.account_id, self.private_key, self.user_rpc_addr or default_mainnet_rpc)\n                await acc.startup()\n                max_retries = min(max_retries, 10)\n\n                for attempt in range(1, max_retries + 1):\n                    try:\n                        # Attempt to read the contract view method\n                        return await acc.view_function(contract_id, method_name, args, block_id, threshold)\n                    except Exception as e:\n                        # Log the error message for the current attempt\n                        print(\n                            f\"Attempt {attempt}/{max_retries} to view method '{method_name}' on contract \"\n                            f\"'{contract_id}' failed with error: {e}\"\n                        )\n\n                        # If it's the last attempt, re-raise the exception\n                        if attempt == max_retries:\n                            raise\n\n            async def call(\n                self,\n                contract_id: str,\n                method_name: str,\n                args: dict,\n                gas: int = DEFAULT_ATTACHED_GAS,\n                amount: int = 0,\n                nowait: bool = False,\n                included: bool = False,\n                max_retries: int = 1,\n            ):\n                \"\"\"Wrapper for the call method of the Account class, adding multiple retry attempts.\n\n                Parameters\n                ----------\n                contract_id : str\n                    The ID of the contract to call.\n                method_name : str\n                    The name of the method to invoke on the contract.\n                args : dict\n                    The arguments to pass to the contract method.\n                gas : int\n                    The amount of gas to attach to the call.\n                amount : int\n                    The amount of tokens to attach to the call.\n                nowait : bool\n                    If nowait is True, return transaction hash, else wait execution.\n                included : bool\n                    If included is True, return transaction hash, else wait execution\n                max_retries : int\n                    The maximum number of retry attempts.\n\n                Returns\n                -------\n                The result of the contract method call.\n\n                Raises\n                ------\n                Exception\n                    If all retry attempts fail, the exception is propagated.\n\n                \"\"\"\n                acc = Account(self.account_id, self.private_key, self.user_rpc_addr or default_mainnet_rpc)\n                await acc.startup()\n                max_retries = min(max_retries, 10)\n\n                for attempt in range(1, max_retries + 1):\n                    try:\n                        # Attempt to call the contract method\n                        return await acc.function_call(contract_id, method_name, args, gas, amount, nowait, included)\n                    except Exception as e:\n                        # Log the error message for the current attempt\n                        print(\n                            f\"Attempt {attempt}/{max_retries} to call method '{method_name}' on contract \"\n                            f\"'{contract_id}' failed with error: {e}\"\n                        )\n\n                        # If it's the last attempt, re-raise the exception\n                        if attempt == max_retries:\n                            raise\n\n            async def get_balance(self, account_id: Optional[str] = None) -&gt; int:\n                \"\"\"Retrieves the balance of the specified NEAR account.\n\n                Parameters\n                ----------\n                account_id : Optional[str]\n                    The ID of the account to retrieve the balance for. If not provided, the balance of the current\n                    account is retrieved.\n\n                Returns\n                -------\n                int\n                    The balance of the specified account in yoctoNEAR.\n\n                Raises\n                ------\n                Exception\n                    If there is an error retrieving the balance.\n\n                \"\"\"\n                acc = Account(self.account_id, self.private_key, self.user_rpc_addr or default_mainnet_rpc)\n                await acc.startup()\n                return await acc.get_balance(account_id)\n\n            def __init__(\n                self,\n                account_id: Optional[str] = None,\n                private_key: Optional[Union[List[Union[str, bytes]], str, bytes]] = None,\n                rpc_addr: Optional[str] = None,\n            ):\n                self.user_rpc_addr = rpc_addr\n                self.account_id = account_id\n                self.private_key = private_key\n                super().__init__(account_id, private_key, rpc_addr)\n\n        self.set_near = NearAccount\n\n        self._tools = ToolRegistry()\n\n        # Protected client methods\n        def query_vector_store(vector_store_id: str, query: str, full_files: bool = False):\n            \"\"\"Queries a vector store.\n\n            vector_store_id: The id of the vector store to query.\n            query: The query to search for.\n            \"\"\"\n            return client.query_vector_store(vector_store_id, query, full_files)\n\n        self.query_vector_store = query_vector_store\n\n        def upload_file(\n            file_content: str,\n            purpose: Literal[\"assistants\", \"batch\", \"fine-tune\", \"vision\"] = \"assistants\",\n            encoding: Optional[str] = \"utf-8\",\n            file_name: Optional[str] = \"file.txt\",\n            file_type: Optional[str] = \"text/plain\",\n        ):\n            \"\"\"Uploads a file to the registry.\"\"\"\n            return client.upload_file(\n                file_content, purpose, encoding=encoding, file_name=file_name, file_type=file_type\n            )\n\n        self.upload_file = upload_file\n\n        def remove_file(file_id: str):\n            \"\"\"Removes a file from the registry.\"\"\"\n            return client.remove_file(file_id)\n\n        self.remove_file = remove_file\n\n        def create_vector_store_from_source(\n            name: str,\n            source: Union[GitHubSource, GitLabSource],\n            source_auth: Optional[str] = None,\n            chunking_strategy: Optional[ChunkingStrategy] = None,\n            expires_after: Optional[ExpiresAfter] = None,\n            metadata: Optional[Dict[str, str]] = None,\n        ) -&gt; VectorStore:\n            \"\"\"Creates a vector store from the given source.\n\n            Args:\n            ----\n                name: The name of the vector store.\n                source: The source from which to create the vector store.\n                source_auth: The source authentication token.\n                chunking_strategy: The chunking strategy to use.\n                expires_after: The expiration policy.\n                metadata: Additional metadata.\n\n            Returns:\n            -------\n                VectorStore: The created vector store.\n\n            \"\"\"\n            return client.create_vector_store_from_source(\n                name=name,\n                source=source,\n                source_auth=source_auth,\n                chunking_strategy=chunking_strategy,\n                expires_after=expires_after,\n                metadata=metadata,\n            )\n\n        self.create_vector_store_from_source = create_vector_store_from_source\n\n        def add_file_to_vector_store(vector_store_id: str, file_id: str):\n            \"\"\"Adds a file to the vector store.\"\"\"\n            return client.add_file_to_vector_store(vector_store_id, file_id)\n\n        self.add_file_to_vector_store = add_file_to_vector_store\n\n        # positional arguments are not allowed because arguments list will be updated\n        def find_agents(\n            *,\n            owner_id: Optional[str] = None,\n            with_capabilities: Optional[bool] = False,\n            latest_versions_only: Optional[bool] = True,\n            limit: Optional[int] = None,\n            offset: Optional[int] = None,\n        ):\n            \"\"\"Find agents based on various parameters.\"\"\"\n            return client.find_agents(owner_id, with_capabilities, latest_versions_only, limit, offset)\n\n        self.find_agents = find_agents\n\n        def create_vector_store(\n            name: str,\n            file_ids: list,\n            expires_after: Union[ExpiresAfter, NotGiven] = NOT_GIVEN,\n            chunking_strategy: Union[\n                AutoFileChunkingStrategyParam, StaticFileChunkingStrategyObjectParam, NotGiven\n            ] = NOT_GIVEN,\n            metadata: Optional[Dict[str, str]] = None,\n        ) -&gt; VectorStore:\n            \"\"\"Creates a vector store.\n\n            Args:\n            ----\n                name: The name of the vector store.\n                file_ids: List of file ids to create the vector store.\n                chunking_strategy: The chunking strategy to use.\n                expires_after: The expiration policy.\n                metadata: Additional metadata.\n\n            Returns:\n            -------\n                VectorStore: The created vector store.\n\n            \"\"\"\n            return client.create_vector_store(\n                name=name,\n                file_ids=file_ids,\n                chunking_strategy=chunking_strategy,\n                expires_after=expires_after,\n                metadata=metadata,\n            )\n\n        self.create_vector_store = create_vector_store\n\n        def get_vector_store(vector_store_id: str) -&gt; VectorStore:\n            \"\"\"Gets a vector store by id.\"\"\"\n            return client.get_vector_store(vector_store_id)\n\n        self.get_vector_store = get_vector_store\n\n        def get_vector_store_files(vector_store_id: str) -&gt; Optional[List[VectorStoreFile]]:\n            \"\"\"Gets a list of vector store files.\"\"\"\n            return client.get_vector_store_files(vector_store_id)\n\n        self.get_vector_store_files = get_vector_store_files\n\n        # Save cache of requested models for inference to avoid extra server calls\n        self.cached_models_for_inference: Dict[str, str] = {}\n\n        def get_model_for_inference(model: str = \"\") -&gt; str:\n            \"\"\"Returns 'provider::model_full_path'.\"\"\"\n            if self.cached_models_for_inference.get(model, None) is None:\n                provider = self.get_primary_agent().model_provider if self._agents else \"\"\n                if model == \"\":\n                    model = self.get_primary_agent().model if self._agents else \"\"\n                if model == \"\":\n                    return DEFAULT_PROVIDER_MODEL\n\n                _, model_for_inference = client.provider_models.match_provider_model(model, provider)\n\n                self.cached_models_for_inference[model] = model_for_inference\n\n            return self.cached_models_for_inference[model]\n\n        self.get_model_for_inference = get_model_for_inference\n\n        def _run_inference_completions(\n            messages: Union[Iterable[ChatCompletionMessageParam], str],\n            model: Union[Iterable[ChatCompletionMessageParam], str],\n            stream: bool,\n            **kwargs: Any,\n        ) -&gt; Union[ModelResponse, CustomStreamWrapper]:\n            \"\"\"Run inference completions for given parameters.\"\"\"\n            params, kwargs = self.get_inference_parameters(messages, model, stream, **kwargs)\n\n            completions = client.completions(\n                params.model, params.messages, params.stream, params.temperature, params.max_tokens, **kwargs\n            )\n\n            return completions\n\n        self._run_inference_completions = _run_inference_completions\n\n        def get_agent_public_key():\n            \"\"\"Returns public key of the agent.\"\"\"\n            agent_name = self.get_primary_agent().get_full_name()\n\n            return client.get_agent_public_key(agent_name)\n\n        self.get_agent_public_key = get_agent_public_key\n\n        def run_agent(\n            agent_id: str,\n            query: Optional[str] = None,\n            thread_mode: ThreadMode = ThreadMode.FORK,\n            run_mode: RunMode = RunMode.SIMPLE,\n        ):\n            \"\"\"Runs a child agent on the thread.\"\"\"\n            child_thread_id = self._thread_id\n\n            if thread_mode == ThreadMode.SAME:\n                pass\n            elif thread_mode == ThreadMode.FORK:\n                child_thread_id = client.threads_fork(self._thread_id).id\n                self.add_system_log(f\"Forked thread {child_thread_id}\", logging.INFO)\n            elif thread_mode == ThreadMode.CHILD:\n                child_thread_id = client.create_subthread(self._thread_id).id\n                self.add_system_log(f\"Created subthread {child_thread_id}\", logging.INFO)\n\n            if query:\n                client.threads_messages_create(thread_id=child_thread_id, content=query, role=\"user\")\n\n            self.add_system_log(f\"Running agent {agent_id}\", logging.INFO)\n            client.run_agent(\n                parent_run_id=self._run_id,\n                run_on_thread_id=child_thread_id,\n                assistant_id=agent_id,\n                run_mode=run_mode,\n            )\n            self._pending_ext_agent = True\n\n            return child_thread_id\n\n        self.run_agent = run_agent\n\n        def schedule_run(\n            agent: str,\n            input_message: str,\n            run_at: datetime,\n            run_params: Optional[Dict[str, str]] = None,\n            thread_id: Optional[str] = None,\n        ):\n            \"\"\"Schedules a run.\"\"\"\n            return client.schedule_run(agent, input_message, thread_id, run_params, run_at)\n\n        self.schedule_run = schedule_run\n\n        # TODO(https://github.com/nearai/nearai/issues/549): Allow only a subset of agents to access/update user memory.\n        def add_user_memory(memory: str):\n            \"\"\"Add user memory.\"\"\"\n            return client.add_user_memory(memory)\n\n        self.add_user_memory = add_user_memory\n\n        def query_user_memory(query: str):\n            \"\"\"Query user memory.\"\"\"\n            return client.query_user_memory(query)\n\n        self.query_user_memory = query_user_memory\n\n        def generate_image(prompt: str, model: Optional[str] = None):\n            \"\"\"Generate an image using the specified model or the default if none is provided.\"\"\"\n            return client.generate_image(prompt=prompt, model=model)\n\n        self.generate_image = generate_image\n\n        def save_agent_data(key, data: Dict[str, Any]):\n            \"\"\"Save agent data.\"\"\"\n            try:\n                return client.save_agent_data(key, data)\n            except Exception as ex:\n                self.add_system_log(f\"Error saving agent data by key {key}: {ex}\", logging.ERROR)\n                return None\n\n        self.save_agent_data = save_agent_data\n\n        def get_agent_data():\n            \"\"\"Get agent data.\"\"\"\n            return client.get_agent_data()\n\n        self.get_agent_data = get_agent_data\n\n        def get_agent_data_by_key(key, default=None):\n            \"\"\"Get agent data by key.\"\"\"\n            namespace = self.get_primary_agent().namespace\n            name = self.get_primary_agent().name\n            try:\n                result = client.get_agent_data_by_key(key)\n            except Exception as ex:\n                self.add_system_log(f\"Error getting agent data by key {key}: {ex}\", logging.ERROR)\n                result = None\n            return (\n                result\n                if result\n                else {\n                    \"value\": default,\n                    \"namespace\": namespace,\n                    \"key\": key,\n                    \"name\": name,\n                    \"updated_at\": \"\",\n                    \"created_at\": \"\",\n                }\n            )\n\n        self.get_agent_data_by_key = get_agent_data_by_key\n\n        def _execute_sync_or_async(callback_func):\n            \"\"\"Helper to execute callback with sync or async API based on configuration.\"\"\"\n            if not self._async_api_calls:\n                callback_func()\n                return\n\n            # Execute in thread pool to avoid event loop conflicts\n            def run_in_thread():\n                return callback_func()\n\n            # Use a thread pool to avoid event loop issues\n            if not hasattr(self, \"_thread_pool\"):\n                self._thread_pool = ThreadPoolExecutor(max_workers=4)\n\n            future = self._thread_pool.submit(run_in_thread)\n\n            # Track the future instead of async tasks\n            if not hasattr(self, \"_pending_futures\"):\n                self._pending_futures = set()\n\n            self._pending_futures.add(future)\n\n            # Remove future when done\n            def remove_future(f):\n                self._pending_futures.discard(f)\n\n            future.add_done_callback(remove_future)\n\n            # Don't return anything - the future runs in background\n\n        self._execute_sync_or_async = _execute_sync_or_async\n\n        async def _await_pending_async_tasks():\n            \"\"\"Await all pending async tasks.\"\"\"\n            if not hasattr(self, \"_pending_futures\"):\n                return\n            try:\n                for future in list(self._pending_futures):\n                    future.result(timeout=10)  # Wait max 10 seconds per task\n            except Exception as e:\n                self.add_system_log(f\"Error waiting for async tasks: {e}\", logging.ERROR)\n\n        self._await_pending_async_tasks = _await_pending_async_tasks\n\n        # HubClient methods\n        def add_reply(\n            message: str,\n            attachments: Optional[Iterable[Attachment]] = None,\n            message_type: Optional[str] = None,\n            thread_id: str = self._thread_id,\n        ):\n            \"\"\"Assistant adds a message to the environment.\"\"\"\n            # NOTE: message from `user` are not stored in the memory\n\n            def create_and_cache_message():\n                \"\"\"Create message and update cache.\"\"\"\n                new_message = hub_client.beta.threads.messages.create(\n                    thread_id=thread_id,\n                    role=\"assistant\",\n                    content=message,\n                    extra_body={\n                        \"assistant_id\": self.get_primary_agent().identifier,\n                        \"run_id\": self._run_id,\n                    },\n                    attachments=attachments,\n                    metadata={\"message_type\": message_type} if message_type else None,\n                )\n                # Update cache for main thread\n                if thread_id == self._thread_id and self._messages_cache is not None:\n                    self._messages_cache.append(new_message)\n                return new_message\n\n            if self._debug_mode and not message_type:\n                self.add_chat_log(\"assistant\", message)\n\n            self._execute_sync_or_async(create_and_cache_message)\n\n        self.add_reply = add_reply\n\n        def get_thread(thread_id=self._thread_id):\n            \"\"\"Returns the current Thread object or the requested Thread.\"\"\"\n            return client.get_thread(thread_id)\n\n        self.get_thread = get_thread\n\n        def _add_message(\n            role: str,\n            message: str,\n            attachments: Optional[Iterable[Attachment]] = None,\n            **kwargs: Any,\n        ):\n            \"\"\"Add a message to the thread.\"\"\"\n\n            def create_and_cache_message():\n                \"\"\"Create message and update cache.\"\"\"\n                new_message = hub_client.beta.threads.messages.create(\n                    thread_id=self._thread_id,\n                    role=role,  # type: ignore\n                    content=message,\n                    extra_body={\n                        \"assistant_id\": self.get_primary_agent().identifier,\n                        \"run_id\": self._run_id,\n                    },\n                    metadata=kwargs,\n                    attachments=attachments,\n                )\n\n                # Update cache when adding messages to the main thread\n                if self._messages_cache is not None:\n                    # Add new message to the end of the cache (most recent)\n                    self._messages_cache.append(new_message)\n\n                return new_message\n\n            if self._debug_mode:\n                self.add_chat_log(role, message)\n\n            self._execute_sync_or_async(create_and_cache_message)\n\n        self._add_message = _add_message\n\n        def _list_messages(\n            limit: Union[int, NotGiven] = LIST_MESSAGES_LIMIT,\n            order: Literal[\"asc\", \"desc\"] = \"asc\",\n            thread_id: Optional[str] = None,\n        ) -&gt; List[Message]:\n            \"\"\"Returns messages from the environment.\"\"\"\n            # Use cache if available and we're querying the same thread\n            target_thread_id = thread_id or self._thread_id\n            if self._messages_cache is not None and target_thread_id == self._thread_id:\n                # Return cached messages, applying limit and order\n                cached_messages = self._messages_cache.copy()\n                if order == \"desc\":\n                    cached_messages.reverse()\n                if isinstance(limit, int):\n                    cached_messages = cached_messages[:limit]\n                self.add_system_log(f\"Retrieved {len(cached_messages)} messages from cache\")\n                return cached_messages\n\n            # Fetch from API\n            messages = hub_client.beta.threads.messages.list(thread_id=target_thread_id, limit=limit, order=order)\n            self.add_system_log(f\"Retrieved {len(messages.data)} messages from NEAR AI Hub\")\n\n            # Cache messages if this is for the main thread\n            if target_thread_id == self._thread_id:\n                # Store in ascending order for consistency\n                if order == \"desc\":\n                    self._messages_cache = list(reversed(messages.data))\n                else:\n                    self._messages_cache = messages.data.copy()\n\n            return messages.data\n\n        self._list_messages = _list_messages\n\n        def list_files_from_thread(\n            order: Literal[\"asc\", \"desc\"] = \"desc\", thread_id: Optional[str] = None\n        ) -&gt; List[FileObject]:\n            \"\"\"Lists files in the thread.\"\"\"\n            target_thread_id = thread_id or self._thread_id\n            if self._files_from_thread_cache is not None and target_thread_id == self._thread_id:\n                # Return cached files, applying order\n                files_from_thread_cache = self._files_from_thread_cache.copy()\n                if order == \"asc\":\n                    files_from_thread_cache.reverse()\n                self.add_system_log(f\"Retrieved {len(files_from_thread_cache)} files from cache\")\n                return files_from_thread_cache\n\n            messages = self._list_messages(order=order, thread_id=target_thread_id)\n            # Extract attachments from messages\n            attachments = [a for m in messages if m.attachments for a in m.attachments]\n            # Extract files from attachments\n            file_ids = [a.file_id for a in attachments]\n            files = [hub_client.files.retrieve(f) for f in file_ids if f]\n\n            # Cache files if this is for the main thread\n            if target_thread_id == self._thread_id:\n                # Store in descending order by default\n                if order == \"asc\":\n                    self._files_from_thread_cache = list(reversed(files))\n                else:\n                    self._files_from_thread_cache = files.copy()\n\n            return files\n\n        self.list_files_from_thread = list_files_from_thread\n\n        def read_file_by_id(file_id: str, decode: Union[str, None] = \"utf-8\"):\n            \"\"\"Read a file from the thread.\"\"\"\n            content = hub_client.files.content(file_id).content\n\n            if decode:\n                return content.decode(decode)\n\n            return content\n\n        self.read_file_by_id = read_file_by_id\n\n        def write_file(\n            filename: str,\n            content: Union[str, bytes],\n            encoding: Union[str, None] = \"utf-8\",\n            filetype: str = \"text/plain\",\n            write_to_disk: bool = True,\n            logging: bool = True,\n        ) -&gt; None:\n            \"\"\"Writes a file to the environment.\n\n            filename: The name of the file to write to\n            content: The content to write to the file\n            encoding: The encoding to use when writing the file (default is utf-8)\n            filetype: The MIME type of the file (default is text/plain)\n            write_to_disk: If True, write locally to disk (default is True)\n            \"\"\"\n\n            def create_and_upload_file():\n                \"\"\"Create file locally and upload to Hub.\"\"\"\n                if write_to_disk:\n                    # Write locally\n                    path = Path(self.get_primary_agent_temp_dir()) / filename\n                    path.parent.mkdir(parents=True, exist_ok=True)\n                    if isinstance(content, bytes):\n                        with open(path, \"wb\") as f:\n                            f.write(content)\n                    else:\n                        with open(path, \"w\", encoding=encoding) as f:\n                            f.write(content)\n\n                if isinstance(content, bytes):\n                    file_data = content\n                else:\n                    file_data = io.BytesIO(content.encode(encoding))  # type:ignore\n\n                # Upload to Hub\n                file = hub_client.files.create(file=(filename, file_data, filetype), purpose=\"assistants\")\n\n                self.add_reply(\n                    message=f\"Output file: {filename}\",\n                    attachments=[{\"file_id\": file.id}],\n                    message_type=\"system:output_file\",\n                )\n\n                if logging:\n                    self.add_system_log(f\"Uploaded file {filename} with {len(content)} characters, id: {file.id}\")\n\n                return file\n\n            self._execute_sync_or_async(create_and_upload_file)\n\n        self.write_file = write_file\n\n        def mark_done() -&gt; None:  # noqa: D102\n            hub_client.beta.threads.runs.update(\n                thread_id=self._thread_id,\n                run_id=self._run_id,\n                extra_body={\n                    \"status\": \"completed\",\n                    \"completed_at\": datetime.now().isoformat(),\n                },\n            )\n\n        self.mark_done = mark_done\n\n        def _invalidate_caches() -&gt; None:\n            \"\"\"Invalidate caches.\"\"\"\n            self._messages_cache = None\n            self._files_from_thread_cache = None\n\n        def mark_failed() -&gt; None:\n            \"\"\"Deprecated. Do not use.\"\"\"\n            pass\n\n        self.mark_failed = mark_failed\n\n        def request_user_input() -&gt; None:\n            \"\"\"Deprecated. Do not use.\"\"\"\n            pass\n\n        self.request_user_input = request_user_input\n\n        def request_agent_input() -&gt; Run:\n            \"\"\"Mark the run as ready for input from another agent.\"\"\"\n            return hub_client.beta.threads.runs.update(\n                thread_id=self._thread_id,\n                run_id=self._run_id,\n                extra_body={\"status\": \"requires_action\", \"required_action\": {\"type\": \"agent_input\"}},\n            )\n\n        self.request_agent_input = request_agent_input\n\n        self._invalidate_caches = _invalidate_caches\n\n        # Must be placed after method definitions\n        self.register_standard_tools()\n\n        if self._debug_mode:\n            # Try to load existing logs from thread if they don't exist locally\n            self._load_log_from_thread(SYSTEM_LOG_FILENAME)\n            self._load_log_from_thread(AGENT_LOG_FILENAME)\n            self._load_log_from_thread(CHAT_HISTORY_FILENAME)\n        logger = logging.getLogger(\"system_logger\")\n        logger.handlers = []\n        logger = logging.getLogger(\"agent_logger\")\n        logger.handlers = []\n        logger = logging.getLogger(\"chat_logger\")\n        logger.handlers = []\n\n        self._initialized = True\n        env_init_metrics.notify_of_next_step()\n\n    # end of protected client methods\n\n    def get_tool_registry(self, new: bool = False) -&gt; ToolRegistry:\n        \"\"\"Returns the tool registry, a dictionary of tools that can be called by the agent.\"\"\"\n        if new:\n            self._tools = ToolRegistry()\n        return self._tools\n\n    def register_standard_tools(self) -&gt; None:  # noqa: D102\n        reg = self.get_tool_registry()\n        reg.register_tool(self.read_file)\n        reg.register_tool(self.write_file)\n        reg.register_tool(self.list_files)\n        reg.register_tool(self.query_vector_store)\n\n    def get_last_message(self, role: str = \"user\"):\n        \"\"\"Reads last message from the given role and returns it.\"\"\"\n        for message in reversed(self.list_messages()):\n            if message.get(\"role\") == role:\n                return message\n\n        return None\n\n    def add_message(\n        self,\n        role: str,\n        message: str,\n        attachments: Optional[Iterable[Attachment]] = None,\n        **kwargs: Any,\n    ):\n        \"\"\"Deprecated. Please use `add_reply` instead. Assistant adds a message to the environment.\"\"\"\n        # Prevent agent to save messages on behalf of `user` to avoid adding false memory\n        role = \"assistant\"\n\n        return self._add_message(role, message, attachments, **kwargs)\n\n    def add_system_log(self, log: str, level: int = logging.INFO) -&gt; None:\n        \"\"\"Add system log with timestamp and log level.\"\"\"\n        if not self._initialized:\n            return\n        # NOTE: Do not call prints in this function.\n        logger = logging.getLogger(\"system_logger\")\n        if not logger.handlers:\n            # Configure the logger if it hasn't been set up yet\n            logger.setLevel(logging.DEBUG)\n            file_handler = logging.FileHandler(os.path.join(self.get_primary_agent_temp_dir(), SYSTEM_LOG_FILENAME))\n            formatter = logging.Formatter(\"%(asctime)s - %(levelname)s - %(message)s\", datefmt=\"%Y-%m-%d %H:%M:%S\")\n            file_handler.setFormatter(formatter)\n            logger.addHandler(file_handler)\n\n            if self.print_system_log:\n                console_handler = logging.StreamHandler()\n                console_handler.setFormatter(formatter)\n                logger.addHandler(console_handler)\n\n            # Add Thread log handler\n            if self._debug_mode:\n                custom_handler = CustomLogHandler(self.add_reply, \"system\")\n                custom_handler.setFormatter(formatter)\n                logger.addHandler(custom_handler)\n\n        # Log the message\n        logger.log(level, log)\n        # Force the handler to write to disk\n        for handler in logger.handlers:\n            handler.flush()\n\n        if self._debug_mode:\n            self._save_logs_to_thread(SYSTEM_LOG_FILENAME)\n\n    def add_agent_log(self, log: str, level: int = logging.INFO) -&gt; None:\n        \"\"\"Add agent log with timestamp and log level.\"\"\"\n        if not self._initialized:\n            return\n        logger = logging.getLogger(\"agent_logger\")\n        if not logger.handlers:\n            # Configure the logger if it hasn't been set up yet\n            logger.setLevel(logging.DEBUG)\n            file_handler = logging.FileHandler(os.path.join(self.get_primary_agent_temp_dir(), AGENT_LOG_FILENAME))\n            formatter = logging.Formatter(\"%(asctime)s - %(levelname)s - %(message)s\", datefmt=\"%Y-%m-%d %H:%M:%S\")\n            file_handler.setFormatter(formatter)\n            logger.addHandler(file_handler)\n\n            # Add Thread log handler\n            if self._debug_mode:\n                custom_handler = CustomLogHandler(self.add_reply, \"agent\")\n                custom_handler.setFormatter(formatter)\n                logger.addHandler(custom_handler)\n\n        # Log the message\n        logger.log(level, log)\n        # Force the handler to write to disk\n        for handler in logger.handlers:\n            handler.flush()\n\n        if self._debug_mode:\n            self._save_logs_to_thread(AGENT_LOG_FILENAME)\n\n    def add_chat_log(self, role: str, content: str, level: int = logging.INFO) -&gt; None:\n        \"\"\"Add chat history to log file when in debug mode.\"\"\"\n        if not self._initialized:\n            return\n        if not self._debug_mode:\n            return\n        if not isinstance(content, str):\n            content = \"content is not str\"\n        logger = logging.getLogger(\"chat_logger\")\n        if not logger.handlers:\n            # Configure the logger if it hasn't been set up yet\n            logger.setLevel(logging.DEBUG)\n            file_handler = logging.FileHandler(os.path.join(self.get_primary_agent_temp_dir(), CHAT_HISTORY_FILENAME))\n            formatter = logging.Formatter(\"%(asctime)s - %(levelname)s - %(message)s\", datefmt=\"%Y-%m-%d %H:%M:%S\")\n            file_handler.setFormatter(formatter)\n            logger.addHandler(file_handler)\n\n        # Log the message with role prefix\n        message = f\"{role.upper()}: {content}\"\n        logger.log(level, message)\n        # Force the handler to write to disk\n        for handler in logger.handlers:\n            handler.flush()\n\n        if self._debug_mode:\n            self._save_logs_to_thread(CHAT_HISTORY_FILENAME)\n\n    def add_agent_start_system_log(self, agent_idx: int) -&gt; None:\n        \"\"\"Adds agent start system log.\"\"\"\n        agent = self._agents[agent_idx]\n        message = f\"Running agent {agent.name}\"\n        if agent.model != \"\":\n            model = self.get_model_for_inference(agent.model)\n            self._last_used_model = model\n            message += f\" that will connect to {model}\"\n            if agent.model_temperature:\n                message += f\", temperature={agent.model_temperature}\"\n            if agent.model_max_tokens:\n                message += f\", max_tokens={agent.model_max_tokens}\"\n        self.add_system_log(message)\n\n    def list_messages(\n        self,\n        thread_id: Optional[str] = None,\n        limit: Union[int, NotGiven] = LIST_MESSAGES_LIMIT,\n        order: Literal[\"asc\", \"desc\"] = \"asc\",\n    ):\n        \"\"\"Backwards compatibility for chat_completions messages.\"\"\"\n        messages = self._list_messages(thread_id=thread_id, limit=limit, order=order)\n\n        # Filter out system and agent log messages when running in debug mode. Agent behavior shouldn't change based on logs.  # noqa: E501\n        messages = [\n            m\n            for m in messages\n            if not (\n                m.metadata\n                and any(m.metadata.get(\"message_type\", \"\").startswith(prefix) for prefix in [\"system:\", \"agent:\"])\n            )\n        ]\n\n        legacy_messages = [\n            {\n                \"id\": m.id,\n                \"content\": \"\\n\".join([c.text.value for c in m.content]),  # type: ignore\n                \"role\": m.role,\n                \"attachments\": m.attachments,\n            }\n            for m in messages\n        ]\n        return legacy_messages\n\n    def verify_message(\n        self,\n        account_id: str,\n        public_key: str,\n        signature: str,\n        message: str,\n        nonce: str,\n        callback_url: str,\n    ) -&gt; near.SignatureVerificationResult:\n        \"\"\"Verifies that the user message is signed with NEAR Account.\"\"\"\n        return near.verify_signed_message(\n            account_id,\n            public_key,\n            signature,\n            message,\n            nonce,\n            self.get_primary_agent().name,\n            callback_url,\n        )\n\n    def list_files(self, path: str, order: Literal[\"asc\", \"desc\"] = \"asc\") -&gt; List[str]:\n        \"\"\"Lists files in the environment.\"\"\"\n        return os.listdir(os.path.join(self.get_primary_agent_temp_dir(), path))\n\n    def get_agent_temp_path(self) -&gt; Path:\n        \"\"\"Returns temp dir for primary agent where execution happens.\"\"\"\n        return self.get_primary_agent_temp_dir()\n\n    def read_file(self, filename: str, decode: Union[str, None] = \"utf-8\") -&gt; Optional[Union[bytes, str]]:\n        \"\"\"Reads a file from the environment or thread.\"\"\"\n        file_content: Optional[Union[bytes, str]] = None\n        # First try to read from local filesystem\n        local_path = os.path.join(self.get_primary_agent_temp_dir(), filename)\n        if os.path.exists(local_path):\n            print(f\"Reading file {filename} from local path: {local_path}\")\n            try:\n                with open(local_path, \"rb\") as local_path_file:\n                    local_file_content = local_path_file.read()\n                    file_content = local_file_content\n                    if decode:\n                        file_content = file_content.decode(decode)\n            except Exception as e:\n                print(f\"Error with read_file: {e}\")\n\n        if not file_content:\n            # Next check files written out by the agent.\n            # Agent output files take precedence over files packaged with the agent\n            thread_files = self.list_files_from_thread(order=\"desc\")\n\n            # Then try to read from thread, starting from the most recent\n            for f in thread_files:\n                if f.filename == filename:\n                    file_content = self.read_file_by_id(f.id, decode)\n                    break\n\n            if not file_content:\n                # Next check agent file cache\n                # Agent output files &amp; thread files take precedence over cached files\n                file_cache = self.get_primary_agent().file_cache\n                if file_cache:\n                    file_content = file_cache.get(filename, None)\n\n            # Write the file content from the thread or cache to the local filesystem\n            # This allows exec_command to operate on the file\n            if file_content:\n                if not os.path.exists(os.path.dirname(local_path)):\n                    os.makedirs(os.path.dirname(local_path))\n\n                with open(local_path, \"wb\") as local_file:\n                    if isinstance(file_content, bytes):\n                        local_file.write(file_content)\n                    else:\n                        local_file.write(file_content.encode(\"utf-8\"))\n\n        if not file_content:\n            self.add_system_log(f\"Warn: File {filename} not found during read_file operation\")\n\n        return file_content\n\n    def get_inference_parameters(\n        self,\n        messages: Union[Iterable[ChatCompletionMessageParam], str],\n        model: Union[Iterable[ChatCompletionMessageParam], str],\n        stream: bool,\n        **kwargs: Any,\n    ) -&gt; Tuple[InferenceParameters, Any]:\n        \"\"\"Run inference parameters to run completions.\"\"\"\n        if isinstance(messages, str):\n            self.add_system_log(\n                \"Deprecated completions call. Pass `messages` as a first parameter.\",\n                logging.WARNING,\n            )\n            messages_or_model = messages\n            model_or_messages = model\n            model = cast(str, messages_or_model)\n            messages = cast(Iterable[ChatCompletionMessageParam], model_or_messages)\n        else:\n            model = cast(str, model)\n            messages = cast(Iterable[ChatCompletionMessageParam], messages)\n        model = self.get_model_for_inference(model)\n        if model != self._last_used_model:\n            self._last_used_model = model\n            self.add_system_log(f\"Connecting to {model}\")\n\n        temperature = kwargs.pop(\"temperature\", self.get_primary_agent().model_temperature if self._agents else None)\n        max_tokens = kwargs.pop(\"max_tokens\", self.get_primary_agent().model_max_tokens if self._agents else None)\n\n        params = InferenceParameters(\n            model=model,\n            messages=messages,\n            stream=stream,\n            temperature=temperature,\n            max_tokens=max_tokens,\n        )\n\n        return params, kwargs\n\n    # TODO(286): `messages` may be model and `model` may be messages temporarily to support deprecated API.\n    def completions(\n        self,\n        messages: Union[Iterable[ChatCompletionMessageParam], str],\n        model: Union[Iterable[ChatCompletionMessageParam], str] = \"\",\n        stream: bool = False,\n        thread_id: Optional[str] = None,\n        attachments: Optional[Iterable[Attachment]] = None,\n        message_type: Optional[str] = None,\n        **kwargs: Any,\n    ) -&gt; Union[ModelResponse, CustomStreamWrapper]:\n        \"\"\"Returns all completions for given messages using the given model.\n\n        Always returns a ModelResponse object. When stream=True, aggregates the streamed\n        content into a ModelResponse. When stream=False, returns the ModelResponse directly.\n        \"\"\"\n        params, kwargs = self.get_inference_parameters(messages, model, stream, **kwargs)\n        if stream:\n            message_id = None\n            kwargs.setdefault(\"extra_headers\", {}).update(\n                {\n                    k: v\n                    for k, v in {\n                        \"run_id\": self._run_id,\n                        \"thread_id\": thread_id if thread_id else self._thread_id,\n                        \"message_id\": message_id,\n                    }.items()\n                    if v is not None\n                }\n            )\n\n            # Pass thread_id, attachments, and message_type to the server\n            stream_results = self._run_inference_completions(\n                messages, model, True, thread_id=thread_id, attachments=attachments, message_type=message_type, **kwargs\n            )\n            full_content = \"\"\n            for chunk in stream_results:\n                if not isinstance(chunk, (tuple, str)) and hasattr(chunk, \"choices\"):\n                    if chunk.choices and hasattr(chunk.choices[0], \"delta\"):\n                        delta = chunk.choices[0].delta\n                        if hasattr(delta, \"content\") and delta.content:\n                            full_content += delta.content\n\n            response = ModelResponse(\n                id=\"streamed_completion\",\n                object=\"chat.completion\",\n                created=int(time.time()),\n                model=params.model,\n                choices=[\n                    Choices(index=0, message={\"role\": \"assistant\", \"content\": full_content}, finish_reason=\"stop\")\n                ],\n                usage={\"prompt_tokens\": 0, \"completion_tokens\": 0, \"total_tokens\": 0},\n            )\n            return response\n        else:\n            return self._run_inference_completions(messages, model, False, **kwargs)\n\n    def verify_signed_message(\n        self,\n        completion: str,\n        messages: Union[Iterable[ChatCompletionMessageParam], str],\n        public_key: Union[str, None] = None,\n        signature: Union[str, None] = None,\n        model: Union[Iterable[ChatCompletionMessageParam], str] = \"\",\n        **kwargs: Any,\n    ) -&gt; bool:\n        \"\"\"Verifies a signed message.\"\"\"\n        if public_key is None or signature is None:\n            return False\n\n        params, _ = self.get_inference_parameters(messages, model, False, **kwargs)\n\n        messages_without_ids = [{k: v for k, v in item.items() if k != \"id\"} for item in params.messages]\n        ordered_messages_without_ids = [\n            {\"role\": str(item[\"role\"]), \"content\": str(item[\"content\"])} for item in messages_without_ids\n        ]\n\n        return validate_completion_signature(\n            public_key,\n            signature,\n            CompletionSignaturePayload(\n                agent_name=self.get_primary_agent().get_full_name(),\n                completion=completion,\n                model=params.model,\n                messages=ordered_messages_without_ids,\n                temperature=params.temperature,\n                max_tokens=params.max_tokens,\n            ),\n        )\n\n    def completions_and_run_tools(\n        self,\n        messages: List[ChatCompletionMessageParam],\n        model: str = \"\",\n        tools: Optional[List] = None,\n        add_responses_to_messages: bool = True,\n        agent_role_name=\"assistant\",\n        tool_role_name=\"tool\",\n        **kwargs: Any,\n    ) -&gt; ModelResponse:\n        \"\"\"Returns all completions for given messages using the given model and runs tools.\"\"\"\n        if self._use_llama_tool_syntax(model, tools):\n            tool_prompt = self._llama_tool_prompt(tools)\n            messages.append({\"role\": \"system\", \"content\": tool_prompt})\n        raw_response = self._run_inference_completions(messages, model, stream=False, tools=tools, **kwargs)\n        assert isinstance(raw_response, ModelResponse), \"Expected ModelResponse\"\n        response: ModelResponse = raw_response\n        assert all(map(lambda choice: isinstance(choice, Choices), response.choices)), \"Expected Choices\"\n        choices: List[Choices] = response.choices  # type: ignore\n        response_message = choices[0].message\n\n        self._handle_tool_calls(response_message, add_responses_to_messages, agent_role_name, tool_role_name)\n\n        return response\n\n    def _handle_tool_calls(\n        self,\n        response_message,\n        add_responses_to_messages,\n        agent_role_name,\n        tool_role_name,\n    ):\n        (message_without_tool_call, tool_calls) = self._parse_tool_call(response_message)\n        if add_responses_to_messages and response_message.content:\n            self.add_message(agent_role_name, message_without_tool_call)\n        if tool_calls:\n            for tool_call in tool_calls:\n                function_name = tool_call.function.name\n                try:\n                    assert function_name, \"Tool call must have a function name\"\n                    function_signature = self.get_tool_registry().get_tool_definition(function_name)\n                    assert function_signature, f\"Tool {function_name} not found\"\n                    args = tool_call.function.arguments\n                    function_args = tool_json_helper.parse_json_args(function_signature, args)\n                    self.add_system_log(f\"Calling tool {function_name} with args {function_args}\")\n                    function_response = self._tools.call_tool(function_name, **function_args if function_args else {})\n\n                    if function_response:\n                        try:\n                            function_response_json = json.dumps(function_response) if function_response else \"\"\n                            if add_responses_to_messages:\n                                self.add_message(\n                                    tool_role_name,\n                                    function_response_json,\n                                    tool_call_id=tool_call.id,\n                                    name=function_name,\n                                )\n                        except Exception as e:\n                            # some tool responses may not be serializable\n                            error_message = f\"Unable to add tool output as a message {function_name}: {e}\"\n                            self.add_system_log(error_message, level=logging.INFO)\n                except Exception as e:\n                    error_message = f\"Error calling tool {function_name}: {e}\"\n                    self.add_system_log(error_message, level=logging.ERROR)\n                    if add_responses_to_messages:\n                        self.add_message(\n                            tool_role_name,\n                            error_message,\n                            tool_call_id=tool_call.id,\n                            name=function_name,\n                        )\n\n    @staticmethod\n    def _parse_tool_call(\n        response_message,\n    ) -&gt; Tuple[Optional[str], Optional[List[ChatCompletionMessageToolCall]]]:\n        if hasattr(response_message, \"tool_calls\") and response_message.tool_calls:\n            return response_message.content, response_message.tool_calls\n        content = response_message.content\n        if content is None:\n            return None, None\n        content = response_message.content\n        llama_matches = LLAMA_TOOL_FORMAT_PATTERN.findall(content)\n        if llama_matches:\n            text = \"\"\n            tool_calls = []\n            for llama_match in llama_matches:\n                before_call_text, function_name, args, end_tag, after_call_text = llama_match\n                function = Function(name=function_name, arguments=args)\n                tool_call = ChatCompletionMessageToolCall(id=str(uuid.uuid4()), function=function)\n                text += before_call_text + after_call_text\n                tool_calls.append(tool_call)\n            return text, tool_calls\n\n        llama_matches = LLAMA_TOOL_FORMAT_PATTERN2.findall(content)\n        if llama_matches:\n            text = \"\"\n            tool_calls = []\n            for llama_match in llama_matches:\n                before_call_text, function_name_and_args, after_call_text = llama_match\n                try:\n                    parsed_function_name_and_args = json.loads(function_name_and_args)\n                    function_name = parsed_function_name_and_args.get(\"name\")\n                    args = parsed_function_name_and_args.get(\"arguments\")\n                    function = Function(name=function_name, arguments=args)\n                    tool_call = ChatCompletionMessageToolCall(id=str(uuid.uuid4()), function=function)\n                    text += before_call_text + after_call_text\n                    tool_calls.append(tool_call)\n                except json.JSONDecodeError:\n                    print(f\"Error parsing tool_call function name and args: {function_name_and_args}\")\n                    continue\n            return text, tool_calls\n\n        return content, None\n\n    @staticmethod\n    def _use_llama_tool_syntax(model: str, tools: Optional[List]) -&gt; bool:\n        return tools is not None and \"llama\" in model\n\n    @staticmethod\n    def _llama_tool_prompt(tools: Optional[List]) -&gt; str:\n        return (\n            \"\"\"Answer the user's question by making use of the following functions if needed.\n            If none of the function can be used, please say so.\n            Here is a list of functions in JSON format:\"\"\"\n            + json.dumps(tools)\n            + \"\"\"Think very carefully before calling functions.\n            If you choose to call a function ONLY reply in the following format with no prefix or suffix:\n\n            &lt;function=example_function_name&gt;{\"example_name\": \"example_value\"}&lt;/function&gt;\n\n            Reminder:\n            - Function calls MUST follow the specified format, start with &lt;function= and end with &lt;/function&gt;\n            - Function arguments MUST be in JSON format using double quotes\n            - Required parameters MUST be specified\n            - Multiple functions can be called in one message as long as they are on separate lines.\n            - Put the entire function call reply on one line\n        \"\"\"\n        )\n\n    # TODO(286): `messages` may be model and `model` may be messages temporarily to support deprecated API.\n    def completion(\n        self,\n        messages: Union[Iterable[ChatCompletionMessageParam], str],\n        model: Union[Iterable[ChatCompletionMessageParam], str] = \"\",\n        **kwargs: Any,\n    ) -&gt; str:\n        \"\"\"Returns a completion for the given messages using the given model.\"\"\"\n        raw_response = self.completions(messages, model, **kwargs)\n        assert isinstance(raw_response, ModelResponse), \"Expected ModelResponse\"\n        response: ModelResponse = raw_response\n        assert all(map(lambda choice: isinstance(choice, Choices), response.choices)), \"Expected Choices\"\n        choices: List[Choices] = response.choices  # type: ignore\n        response_message = choices[0].message.content\n        assert response_message, \"No completions returned\"\n        return response_message\n\n    def signed_completion(\n        self,\n        messages: Union[Iterable[ChatCompletionMessageParam], str],\n        model: Union[Iterable[ChatCompletionMessageParam], str] = \"\",\n        **kwargs: Any,\n    ) -&gt; Dict[str, Any]:\n        \"\"\"Returns a completion for the given messages using the given model with the agent signature.\"\"\"\n        # TODO Return signed completions for non-latest versions only?\n        agent_name = self.get_primary_agent().get_full_name()\n        raw_response = self.completions(messages, model, agent_name=agent_name, **kwargs)\n        assert isinstance(raw_response, ModelResponse), \"Expected ModelResponse\"\n        response: ModelResponse = raw_response\n\n        signature_data = json.loads(response.system_fingerprint) if response.system_fingerprint else {}\n\n        assert all(map(lambda choice: isinstance(choice, Choices), response.choices)), \"Expected Choices\"\n        choices: List[Choices] = response.choices  # type: ignore\n        response_message = choices[0].message.content\n        assert response_message, \"No completions returned\"\n\n        return {\n            \"response\": response_message,\n            \"signature\": signature_data.get(\"signature\", None),\n            \"public_key\": signature_data.get(\"public_key\", None),\n        }\n\n    def completion_and_get_tools_calls(\n        self,\n        messages: List[ChatCompletionMessageParam],\n        model: str = \"\",\n        **kwargs: Any,\n    ) -&gt; SimpleNamespace:\n        \"\"\"Returns completion message and/or tool calls from OpenAI or Llama tool formats.\"\"\"\n        raw_response = self._run_inference_completions(messages, model, stream=False, **kwargs)\n\n        assert isinstance(raw_response, ModelResponse), \"Expected ModelResponse\"\n        response: ModelResponse = raw_response\n        assert all(map(lambda choice: isinstance(choice, Choices), response.choices)), \"Expected Choices\"\n        choices: List[Choices] = response.choices  # type: ignore\n\n        (message_without_tool_call, tool_calls) = self._parse_tool_call(choices[0].message)\n\n        if message_without_tool_call is None:\n            response_message = choices[0].message.content\n            message_without_tool_call = response_message\n\n        return SimpleNamespace(message=message_without_tool_call, tool_calls=tool_calls)\n\n    def completion_and_run_tools(\n        self,\n        messages: List[ChatCompletionMessageParam],\n        model: str = \"\",\n        tools: Optional[List] = None,\n        **kwargs: Any,\n    ) -&gt; Optional[str]:\n        \"\"\"Returns a completion for the given messages using the given model and runs tools.\"\"\"\n        completion_tools_response = self.completions_and_run_tools(messages, model, tools, **kwargs)\n        assert all(\n            map(\n                lambda choice: isinstance(choice, Choices),\n                completion_tools_response.choices,\n            )\n        ), \"Expected Choices\"\n        choices: List[Choices] = completion_tools_response.choices  # type: ignore\n        response_content = choices[0].message.content\n        return response_content\n\n    def call_agent(self, agent_index: int, task: str) -&gt; None:\n        \"\"\"Calls agent with given task.\"\"\"\n        self._agents[agent_index].run(self, task=task)\n\n    def get_agents(self) -&gt; List[Agent]:\n        \"\"\"Returns list of agents available in environment.\"\"\"\n        return self._agents\n\n    def get_primary_agent(self) -&gt; Agent:\n        \"\"\"Returns the agent that is invoked first.\"\"\"\n        return self._agents[0]\n\n    def get_primary_agent_temp_dir(self) -&gt; Path:\n        \"\"\"Returns temp dir for primary agent.\"\"\"\n        return Path(self.get_primary_agent().temp_dir)\n\n    def environment_run_info(self, base_id, run_type) -&gt; dict:\n        \"\"\"Returns the environment run information.\"\"\"\n        if not self._agents or not self.get_primary_agent():\n            raise ValueError(\"Agent not found\")\n        primary_agent = self.get_primary_agent()\n\n        full_agent_name = \"/\".join([primary_agent.namespace, primary_agent.name, primary_agent.version])\n        safe_agent_name = full_agent_name.replace(\"/\", \"_\")\n        uid = uuid.uuid4().hex\n        generated_name = f\"environment_run_{safe_agent_name}_{uid}\"\n        name = generated_name\n\n        timestamp = datetime.now(timezone.utc).isoformat()\n        return {\n            \"name\": name,\n            \"version\": \"0\",\n            \"description\": f\"Agent {run_type} {full_agent_name} {uid} {timestamp}\",\n            \"category\": \"environment\",\n            \"tags\": [\"environment\"],\n            \"details\": {\n                \"base_id\": base_id,\n                \"timestamp\": timestamp,\n                \"agents\": [agent.name for agent in self._agents],\n                \"primary_agent_namespace\": primary_agent.namespace,\n                \"primary_agent_name\": primary_agent.name,\n                \"primary_agent_version\": primary_agent.version,\n                \"run_id\": self._run_id,\n                \"run_type\": run_type,\n            },\n            \"show_entry\": True,\n        }\n\n    def clear_temp_agent_files(self, verbose=True) -&gt; None:\n        \"\"\"Remove temp agent files created to be used in `runpy`.\"\"\"\n        for agent in self._agents:\n            if os.path.exists(agent.temp_dir):\n                if verbose:\n                    print(\"removed agent.temp_files\", agent.temp_dir)\n                shutil.rmtree(agent.temp_dir)\n\n    def set_next_actor(self, who: str) -&gt; None:\n        \"\"\"Set the next actor / action in the dialogue.\"\"\"\n        next_action_fn = os.path.join(self.get_primary_agent_temp_dir(), \".next_action\")\n\n        with open(next_action_fn, \"w\") as f:\n            f.write(who)\n\n    def get_next_actor(self) -&gt; str:  # noqa: D102\n        next_action_fn = os.path.join(self.get_primary_agent_temp_dir(), \".next_action\")\n\n        if os.path.exists(next_action_fn):\n            with open(next_action_fn) as f:\n                return f.read().strip(\" \\n\")\n        else:\n            # By default the user starts the conversation.\n            return \"user\"\n\n    def run(self, new_message: Optional[str] = None, runner_metrics: Optional[RunnerMetrics] = None) -&gt; None:\n        \"\"\"Runs agent(s) against a new or previously created environment.\"\"\"\n        if self.logs_collection_mode and self.analytics_collector:\n            self.analytics_collector.init_env_run_metrics(runner_metrics=runner_metrics)\n        if new_message:\n            self._add_message(\"user\", new_message)\n            # Await any pending async tasks before proceeding\n            if self._async_api_calls:\n                try:\n                    asyncio.run(self._await_pending_async_tasks())\n                except Exception as e:\n                    self.add_system_log(f\"Error awaiting pending async tasks: {e}\", logging.ERROR)\n        elif self._debug_mode:\n            last_user_message = self.get_last_message(role=\"user\")\n            if last_user_message:\n                content = last_user_message[\"content\"]\n                self.add_chat_log(\"user\", content)\n\n        self.set_next_actor(\"agent\")\n\n        try:\n            # Create a logging callback for agent output\n            def agent_output_logger(msg, level=logging.INFO):\n                self.add_system_log(msg, level)\n\n            error_message, traceback_message = self.get_primary_agent().run(\n                self,\n                task=new_message,\n                log_stdout_callback=agent_output_logger if self._debug_mode else None,\n                log_stderr_callback=agent_output_logger,\n            )\n            if self._debug_mode and (error_message or traceback_message):\n                message_parts = []\n\n                if error_message:\n                    message_parts.append(f\"Error: \\n ```\\n{error_message}\\n```\")\n\n                if traceback_message:\n                    message_parts.append(f\"Error Traceback: \\n ```\\n{traceback_message}\\n```\")\n\n                self.add_system_log(\"\\n\\n\".join(message_parts))\n\n        except Exception as e:\n            self.add_system_log(f\"Environment run failed: {e}\", logging.ERROR)\n            self.mark_failed()\n            raise e\n        finally:\n            # Await any pending async tasks before cleanup\n            if self._async_api_calls:\n                try:\n                    asyncio.run(self._await_pending_async_tasks())\n                except Exception as e:\n                    self.add_system_log(f\"Error awaiting pending async tasks: {e}\", logging.ERROR)\n\n            # Upload analytics data if collection is enabled\n            if self.logs_collection_mode and self.analytics_collector:\n                try:\n                    self.analytics_collector.upload(thread_dir=self.get_primary_agent_temp_dir())\n                except Exception as e:\n                    print(f\"Failed to upload analytics data: {e}\")\n\n            # Invalidate caches when run ends (whether successful or failed)\n            self._invalidate_caches()\n\n        if not self._pending_ext_agent:\n            # If no external agent was called, mark the whole run as done.\n            # Else this environment will stop for now but this run will be continued later.\n            self.mark_done()\n\n    def generate_folder_hash_id(self, path: str) -&gt; str:\n        \"\"\"Returns hash based on files and their contents in path, including subfolders.\"\"\"  # noqa: E501\n        hash_obj = hashlib.md5()\n\n        for root, _dirs, files in os.walk(path):\n            for file in sorted(files):\n                file_path = os.path.join(root, file)\n                with open(file_path, \"rb\") as f:\n                    while chunk := f.read(8192):\n                        hash_obj.update(chunk)\n\n        return hash_obj.hexdigest()\n\n    def _load_log_from_thread(self, filename: str) -&gt; Optional[str]:\n        \"\"\"Load log file from thread if it doesn't exist locally.\"\"\"\n        local_path = os.path.join(self.get_primary_agent_temp_dir(), filename)\n        print(f\"Logging {filename} at: {local_path}\")\n        if not os.path.exists(local_path):\n            try:\n                content = self.read_file(filename, decode=\"utf-8\")\n                if content and isinstance(content, str):  # Type guard to ensure it's a string\n                    with open(os.path.join(local_path), \"w\") as f:\n                        f.write(content)\n                    return content\n            except Exception:\n                pass\n        return None\n\n    def _save_logs_to_thread(self, log_file: str):\n        \"\"\"Save log file to thread.\"\"\"\n        if not self._debug_mode:\n            return\n        log_path = os.path.join(self.get_primary_agent_temp_dir(), log_file)\n        if os.path.exists(log_path):\n            try:\n                with open(log_path, \"r\") as f:\n                    content = f.read()\n                # Only upload if there's content\n                if content:\n                    self.write_file(log_file, content, write_to_disk=False, logging=False)\n            except Exception as e:\n                print(f\"Failed to save {log_file} to thread: {e}\")\n</code></pre>"},{"location":"api/#nearai.agents.environment.Environment.__init__","title":"__init__","text":"<pre><code>__init__(agents: List[Agent], client: InferenceClient, hub_client: OpenAI, thread_id: str, run_id: str, env_vars: Optional[Dict[str, Any]] = None, tool_resources: Optional[Dict[str, Any]] = None, print_system_log: bool = False, agent_runner_user: Optional[str] = None, fastnear_api_key: Optional[str] = None, approvals=None, upload_entry_fn=None) -&gt; None\n</code></pre> Source code in <code>nearai/agents/environment.py</code> <pre><code>def __init__(  # noqa: D107\n    self,\n    agents: List[Agent],\n    client: InferenceClient,\n    hub_client: OpenAI,\n    thread_id: str,\n    run_id: str,\n    env_vars: Optional[Dict[str, Any]] = None,\n    tool_resources: Optional[Dict[str, Any]] = None,\n    print_system_log: bool = False,\n    agent_runner_user: Optional[str] = None,\n    fastnear_api_key: Optional[str] = None,\n    approvals=None,\n    upload_entry_fn=None,\n) -&gt; None:\n    # Warning: never expose `client` or `_hub_client` to agent's environment\n\n    self._initialized = False\n\n    self.base_url = client._config.base_url\n\n    # user_auth is used to authenticate the user in the ts_runner. It will be removed after that in\n    # `nearai/agents/agent.py`\n    auth = client._auth\n    self.user_auth = auth\n\n    # Initialize secure openai clients\n    openai_client_params = {\n        \"api_key\": auth,\n        \"base_url\": client._config.base_url,\n        \"default_headers\": {\"Authorization\": f\"Bearer {auth}\"},\n    }\n    self.openai = SecureOpenAI(**openai_client_params)\n    self.async_openai = SecureAsyncOpenAI(**openai_client_params)\n\n    # Placeholder for solver\n    self.client: Optional[InferenceClient] = None\n\n    self._agents = agents\n    self._pending_ext_agent = False\n    self.env_vars: Dict[str, Any] = env_vars if env_vars else {}\n    self._last_used_model = \"\"\n    self.tool_resources: Dict[str, Any] = tool_resources if tool_resources else {}\n    self.print_system_log = print_system_log\n    self.agent_runner_user = agent_runner_user\n    self._approvals = approvals if approvals else default_approvals\n    self._thread_id = thread_id\n    self._run_id = run_id\n    self._debug_mode = is_debug_mode(self.env_vars)\n    self._async_api_calls = self.env_vars.get(\"ASYNC_API_CALLS\", \"true\").lower() in (\"true\", \"1\", \"yes\", \"on\")\n\n    # Initialize caches\n    self._messages_cache: Optional[List[Message]] = None\n    self._files_from_thread_cache: Optional[List[FileObject]] = None\n\n    # Initialize analytics collection if enabled\n    self.logs_collection_mode = is_logs_collection_mode(self.env_vars)\n    env_init_metrics = EnvInitMetrics()\n    print(f\"logs_collection_mode: {self.logs_collection_mode}\")\n    self.analytics_collector: Optional[AnalyticsCollector] = None\n    if self.logs_collection_mode:\n        self.analytics_collector = AnalyticsCollector(\n            agent=agents[0],\n            debug_mode=self._debug_mode,\n            upload_entry_fn=upload_entry_fn,\n            env_init_metrics=env_init_metrics,\n        )\n        # Wrap clients with analytics\n        client = create_analytics_wrapper(client, \"inference_client\", self.analytics_collector)\n        hub_client = create_analytics_wrapper(hub_client, \"hub_client\", self.analytics_collector)\n        self.openai = create_analytics_wrapper(self.openai, \"openai_client\", self.analytics_collector)\n        self.async_openai = create_analytics_wrapper(\n            self.async_openai, \"async_openai_client\", self.analytics_collector\n        )\n\n    # Expose the NEAR account_id of a user that signs this request to run an agent.\n    self.signer_account_id: str = client._config.auth.account_id if client._config.auth else \"\"\n\n    if fastnear_api_key:\n        default_mainnet_rpc = f\"https://{fastnear_api_key}@rpc.mainnet.fastnear.com\"\n    else:\n        default_mainnet_rpc = \"https://rpc.mainnet.near.org\"\n\n    class NearAccount(Account):\n        user_rpc_addr: Union[str, None]\n\n        async def view(\n            self,\n            contract_id: str,\n            method_name: str,\n            args: dict,\n            block_id: Optional[int] = None,\n            threshold: Optional[int] = None,\n            max_retries: int = 3,\n        ):\n            \"\"\"Wrapper for the view method of the Account class, adding multiple retry attempts.\n\n            Parameters\n            ----------\n            contract_id : str\n                The ID of the contract to call.\n            method_name : str\n                The name of the method to invoke on the contract.\n            args : dict\n                The arguments to pass to the contract method.\n            block_id : Optional[int]\n                The block ID to query at.\n            threshold : Optional[int]\n                The threshold for the view function.\n            max_retries : int\n                The maximum number of retry attempts.\n\n            Returns\n            -------\n            The result of the contract method call.\n\n            Raises\n            ------\n            Exception\n                If all retry attempts fail, the exception is propagated.\n\n            \"\"\"\n            acc = Account(self.account_id, self.private_key, self.user_rpc_addr or default_mainnet_rpc)\n            await acc.startup()\n            max_retries = min(max_retries, 10)\n\n            for attempt in range(1, max_retries + 1):\n                try:\n                    # Attempt to read the contract view method\n                    return await acc.view_function(contract_id, method_name, args, block_id, threshold)\n                except Exception as e:\n                    # Log the error message for the current attempt\n                    print(\n                        f\"Attempt {attempt}/{max_retries} to view method '{method_name}' on contract \"\n                        f\"'{contract_id}' failed with error: {e}\"\n                    )\n\n                    # If it's the last attempt, re-raise the exception\n                    if attempt == max_retries:\n                        raise\n\n        async def call(\n            self,\n            contract_id: str,\n            method_name: str,\n            args: dict,\n            gas: int = DEFAULT_ATTACHED_GAS,\n            amount: int = 0,\n            nowait: bool = False,\n            included: bool = False,\n            max_retries: int = 1,\n        ):\n            \"\"\"Wrapper for the call method of the Account class, adding multiple retry attempts.\n\n            Parameters\n            ----------\n            contract_id : str\n                The ID of the contract to call.\n            method_name : str\n                The name of the method to invoke on the contract.\n            args : dict\n                The arguments to pass to the contract method.\n            gas : int\n                The amount of gas to attach to the call.\n            amount : int\n                The amount of tokens to attach to the call.\n            nowait : bool\n                If nowait is True, return transaction hash, else wait execution.\n            included : bool\n                If included is True, return transaction hash, else wait execution\n            max_retries : int\n                The maximum number of retry attempts.\n\n            Returns\n            -------\n            The result of the contract method call.\n\n            Raises\n            ------\n            Exception\n                If all retry attempts fail, the exception is propagated.\n\n            \"\"\"\n            acc = Account(self.account_id, self.private_key, self.user_rpc_addr or default_mainnet_rpc)\n            await acc.startup()\n            max_retries = min(max_retries, 10)\n\n            for attempt in range(1, max_retries + 1):\n                try:\n                    # Attempt to call the contract method\n                    return await acc.function_call(contract_id, method_name, args, gas, amount, nowait, included)\n                except Exception as e:\n                    # Log the error message for the current attempt\n                    print(\n                        f\"Attempt {attempt}/{max_retries} to call method '{method_name}' on contract \"\n                        f\"'{contract_id}' failed with error: {e}\"\n                    )\n\n                    # If it's the last attempt, re-raise the exception\n                    if attempt == max_retries:\n                        raise\n\n        async def get_balance(self, account_id: Optional[str] = None) -&gt; int:\n            \"\"\"Retrieves the balance of the specified NEAR account.\n\n            Parameters\n            ----------\n            account_id : Optional[str]\n                The ID of the account to retrieve the balance for. If not provided, the balance of the current\n                account is retrieved.\n\n            Returns\n            -------\n            int\n                The balance of the specified account in yoctoNEAR.\n\n            Raises\n            ------\n            Exception\n                If there is an error retrieving the balance.\n\n            \"\"\"\n            acc = Account(self.account_id, self.private_key, self.user_rpc_addr or default_mainnet_rpc)\n            await acc.startup()\n            return await acc.get_balance(account_id)\n\n        def __init__(\n            self,\n            account_id: Optional[str] = None,\n            private_key: Optional[Union[List[Union[str, bytes]], str, bytes]] = None,\n            rpc_addr: Optional[str] = None,\n        ):\n            self.user_rpc_addr = rpc_addr\n            self.account_id = account_id\n            self.private_key = private_key\n            super().__init__(account_id, private_key, rpc_addr)\n\n    self.set_near = NearAccount\n\n    self._tools = ToolRegistry()\n\n    # Protected client methods\n    def query_vector_store(vector_store_id: str, query: str, full_files: bool = False):\n        \"\"\"Queries a vector store.\n\n        vector_store_id: The id of the vector store to query.\n        query: The query to search for.\n        \"\"\"\n        return client.query_vector_store(vector_store_id, query, full_files)\n\n    self.query_vector_store = query_vector_store\n\n    def upload_file(\n        file_content: str,\n        purpose: Literal[\"assistants\", \"batch\", \"fine-tune\", \"vision\"] = \"assistants\",\n        encoding: Optional[str] = \"utf-8\",\n        file_name: Optional[str] = \"file.txt\",\n        file_type: Optional[str] = \"text/plain\",\n    ):\n        \"\"\"Uploads a file to the registry.\"\"\"\n        return client.upload_file(\n            file_content, purpose, encoding=encoding, file_name=file_name, file_type=file_type\n        )\n\n    self.upload_file = upload_file\n\n    def remove_file(file_id: str):\n        \"\"\"Removes a file from the registry.\"\"\"\n        return client.remove_file(file_id)\n\n    self.remove_file = remove_file\n\n    def create_vector_store_from_source(\n        name: str,\n        source: Union[GitHubSource, GitLabSource],\n        source_auth: Optional[str] = None,\n        chunking_strategy: Optional[ChunkingStrategy] = None,\n        expires_after: Optional[ExpiresAfter] = None,\n        metadata: Optional[Dict[str, str]] = None,\n    ) -&gt; VectorStore:\n        \"\"\"Creates a vector store from the given source.\n\n        Args:\n        ----\n            name: The name of the vector store.\n            source: The source from which to create the vector store.\n            source_auth: The source authentication token.\n            chunking_strategy: The chunking strategy to use.\n            expires_after: The expiration policy.\n            metadata: Additional metadata.\n\n        Returns:\n        -------\n            VectorStore: The created vector store.\n\n        \"\"\"\n        return client.create_vector_store_from_source(\n            name=name,\n            source=source,\n            source_auth=source_auth,\n            chunking_strategy=chunking_strategy,\n            expires_after=expires_after,\n            metadata=metadata,\n        )\n\n    self.create_vector_store_from_source = create_vector_store_from_source\n\n    def add_file_to_vector_store(vector_store_id: str, file_id: str):\n        \"\"\"Adds a file to the vector store.\"\"\"\n        return client.add_file_to_vector_store(vector_store_id, file_id)\n\n    self.add_file_to_vector_store = add_file_to_vector_store\n\n    # positional arguments are not allowed because arguments list will be updated\n    def find_agents(\n        *,\n        owner_id: Optional[str] = None,\n        with_capabilities: Optional[bool] = False,\n        latest_versions_only: Optional[bool] = True,\n        limit: Optional[int] = None,\n        offset: Optional[int] = None,\n    ):\n        \"\"\"Find agents based on various parameters.\"\"\"\n        return client.find_agents(owner_id, with_capabilities, latest_versions_only, limit, offset)\n\n    self.find_agents = find_agents\n\n    def create_vector_store(\n        name: str,\n        file_ids: list,\n        expires_after: Union[ExpiresAfter, NotGiven] = NOT_GIVEN,\n        chunking_strategy: Union[\n            AutoFileChunkingStrategyParam, StaticFileChunkingStrategyObjectParam, NotGiven\n        ] = NOT_GIVEN,\n        metadata: Optional[Dict[str, str]] = None,\n    ) -&gt; VectorStore:\n        \"\"\"Creates a vector store.\n\n        Args:\n        ----\n            name: The name of the vector store.\n            file_ids: List of file ids to create the vector store.\n            chunking_strategy: The chunking strategy to use.\n            expires_after: The expiration policy.\n            metadata: Additional metadata.\n\n        Returns:\n        -------\n            VectorStore: The created vector store.\n\n        \"\"\"\n        return client.create_vector_store(\n            name=name,\n            file_ids=file_ids,\n            chunking_strategy=chunking_strategy,\n            expires_after=expires_after,\n            metadata=metadata,\n        )\n\n    self.create_vector_store = create_vector_store\n\n    def get_vector_store(vector_store_id: str) -&gt; VectorStore:\n        \"\"\"Gets a vector store by id.\"\"\"\n        return client.get_vector_store(vector_store_id)\n\n    self.get_vector_store = get_vector_store\n\n    def get_vector_store_files(vector_store_id: str) -&gt; Optional[List[VectorStoreFile]]:\n        \"\"\"Gets a list of vector store files.\"\"\"\n        return client.get_vector_store_files(vector_store_id)\n\n    self.get_vector_store_files = get_vector_store_files\n\n    # Save cache of requested models for inference to avoid extra server calls\n    self.cached_models_for_inference: Dict[str, str] = {}\n\n    def get_model_for_inference(model: str = \"\") -&gt; str:\n        \"\"\"Returns 'provider::model_full_path'.\"\"\"\n        if self.cached_models_for_inference.get(model, None) is None:\n            provider = self.get_primary_agent().model_provider if self._agents else \"\"\n            if model == \"\":\n                model = self.get_primary_agent().model if self._agents else \"\"\n            if model == \"\":\n                return DEFAULT_PROVIDER_MODEL\n\n            _, model_for_inference = client.provider_models.match_provider_model(model, provider)\n\n            self.cached_models_for_inference[model] = model_for_inference\n\n        return self.cached_models_for_inference[model]\n\n    self.get_model_for_inference = get_model_for_inference\n\n    def _run_inference_completions(\n        messages: Union[Iterable[ChatCompletionMessageParam], str],\n        model: Union[Iterable[ChatCompletionMessageParam], str],\n        stream: bool,\n        **kwargs: Any,\n    ) -&gt; Union[ModelResponse, CustomStreamWrapper]:\n        \"\"\"Run inference completions for given parameters.\"\"\"\n        params, kwargs = self.get_inference_parameters(messages, model, stream, **kwargs)\n\n        completions = client.completions(\n            params.model, params.messages, params.stream, params.temperature, params.max_tokens, **kwargs\n        )\n\n        return completions\n\n    self._run_inference_completions = _run_inference_completions\n\n    def get_agent_public_key():\n        \"\"\"Returns public key of the agent.\"\"\"\n        agent_name = self.get_primary_agent().get_full_name()\n\n        return client.get_agent_public_key(agent_name)\n\n    self.get_agent_public_key = get_agent_public_key\n\n    def run_agent(\n        agent_id: str,\n        query: Optional[str] = None,\n        thread_mode: ThreadMode = ThreadMode.FORK,\n        run_mode: RunMode = RunMode.SIMPLE,\n    ):\n        \"\"\"Runs a child agent on the thread.\"\"\"\n        child_thread_id = self._thread_id\n\n        if thread_mode == ThreadMode.SAME:\n            pass\n        elif thread_mode == ThreadMode.FORK:\n            child_thread_id = client.threads_fork(self._thread_id).id\n            self.add_system_log(f\"Forked thread {child_thread_id}\", logging.INFO)\n        elif thread_mode == ThreadMode.CHILD:\n            child_thread_id = client.create_subthread(self._thread_id).id\n            self.add_system_log(f\"Created subthread {child_thread_id}\", logging.INFO)\n\n        if query:\n            client.threads_messages_create(thread_id=child_thread_id, content=query, role=\"user\")\n\n        self.add_system_log(f\"Running agent {agent_id}\", logging.INFO)\n        client.run_agent(\n            parent_run_id=self._run_id,\n            run_on_thread_id=child_thread_id,\n            assistant_id=agent_id,\n            run_mode=run_mode,\n        )\n        self._pending_ext_agent = True\n\n        return child_thread_id\n\n    self.run_agent = run_agent\n\n    def schedule_run(\n        agent: str,\n        input_message: str,\n        run_at: datetime,\n        run_params: Optional[Dict[str, str]] = None,\n        thread_id: Optional[str] = None,\n    ):\n        \"\"\"Schedules a run.\"\"\"\n        return client.schedule_run(agent, input_message, thread_id, run_params, run_at)\n\n    self.schedule_run = schedule_run\n\n    # TODO(https://github.com/nearai/nearai/issues/549): Allow only a subset of agents to access/update user memory.\n    def add_user_memory(memory: str):\n        \"\"\"Add user memory.\"\"\"\n        return client.add_user_memory(memory)\n\n    self.add_user_memory = add_user_memory\n\n    def query_user_memory(query: str):\n        \"\"\"Query user memory.\"\"\"\n        return client.query_user_memory(query)\n\n    self.query_user_memory = query_user_memory\n\n    def generate_image(prompt: str, model: Optional[str] = None):\n        \"\"\"Generate an image using the specified model or the default if none is provided.\"\"\"\n        return client.generate_image(prompt=prompt, model=model)\n\n    self.generate_image = generate_image\n\n    def save_agent_data(key, data: Dict[str, Any]):\n        \"\"\"Save agent data.\"\"\"\n        try:\n            return client.save_agent_data(key, data)\n        except Exception as ex:\n            self.add_system_log(f\"Error saving agent data by key {key}: {ex}\", logging.ERROR)\n            return None\n\n    self.save_agent_data = save_agent_data\n\n    def get_agent_data():\n        \"\"\"Get agent data.\"\"\"\n        return client.get_agent_data()\n\n    self.get_agent_data = get_agent_data\n\n    def get_agent_data_by_key(key, default=None):\n        \"\"\"Get agent data by key.\"\"\"\n        namespace = self.get_primary_agent().namespace\n        name = self.get_primary_agent().name\n        try:\n            result = client.get_agent_data_by_key(key)\n        except Exception as ex:\n            self.add_system_log(f\"Error getting agent data by key {key}: {ex}\", logging.ERROR)\n            result = None\n        return (\n            result\n            if result\n            else {\n                \"value\": default,\n                \"namespace\": namespace,\n                \"key\": key,\n                \"name\": name,\n                \"updated_at\": \"\",\n                \"created_at\": \"\",\n            }\n        )\n\n    self.get_agent_data_by_key = get_agent_data_by_key\n\n    def _execute_sync_or_async(callback_func):\n        \"\"\"Helper to execute callback with sync or async API based on configuration.\"\"\"\n        if not self._async_api_calls:\n            callback_func()\n            return\n\n        # Execute in thread pool to avoid event loop conflicts\n        def run_in_thread():\n            return callback_func()\n\n        # Use a thread pool to avoid event loop issues\n        if not hasattr(self, \"_thread_pool\"):\n            self._thread_pool = ThreadPoolExecutor(max_workers=4)\n\n        future = self._thread_pool.submit(run_in_thread)\n\n        # Track the future instead of async tasks\n        if not hasattr(self, \"_pending_futures\"):\n            self._pending_futures = set()\n\n        self._pending_futures.add(future)\n\n        # Remove future when done\n        def remove_future(f):\n            self._pending_futures.discard(f)\n\n        future.add_done_callback(remove_future)\n\n        # Don't return anything - the future runs in background\n\n    self._execute_sync_or_async = _execute_sync_or_async\n\n    async def _await_pending_async_tasks():\n        \"\"\"Await all pending async tasks.\"\"\"\n        if not hasattr(self, \"_pending_futures\"):\n            return\n        try:\n            for future in list(self._pending_futures):\n                future.result(timeout=10)  # Wait max 10 seconds per task\n        except Exception as e:\n            self.add_system_log(f\"Error waiting for async tasks: {e}\", logging.ERROR)\n\n    self._await_pending_async_tasks = _await_pending_async_tasks\n\n    # HubClient methods\n    def add_reply(\n        message: str,\n        attachments: Optional[Iterable[Attachment]] = None,\n        message_type: Optional[str] = None,\n        thread_id: str = self._thread_id,\n    ):\n        \"\"\"Assistant adds a message to the environment.\"\"\"\n        # NOTE: message from `user` are not stored in the memory\n\n        def create_and_cache_message():\n            \"\"\"Create message and update cache.\"\"\"\n            new_message = hub_client.beta.threads.messages.create(\n                thread_id=thread_id,\n                role=\"assistant\",\n                content=message,\n                extra_body={\n                    \"assistant_id\": self.get_primary_agent().identifier,\n                    \"run_id\": self._run_id,\n                },\n                attachments=attachments,\n                metadata={\"message_type\": message_type} if message_type else None,\n            )\n            # Update cache for main thread\n            if thread_id == self._thread_id and self._messages_cache is not None:\n                self._messages_cache.append(new_message)\n            return new_message\n\n        if self._debug_mode and not message_type:\n            self.add_chat_log(\"assistant\", message)\n\n        self._execute_sync_or_async(create_and_cache_message)\n\n    self.add_reply = add_reply\n\n    def get_thread(thread_id=self._thread_id):\n        \"\"\"Returns the current Thread object or the requested Thread.\"\"\"\n        return client.get_thread(thread_id)\n\n    self.get_thread = get_thread\n\n    def _add_message(\n        role: str,\n        message: str,\n        attachments: Optional[Iterable[Attachment]] = None,\n        **kwargs: Any,\n    ):\n        \"\"\"Add a message to the thread.\"\"\"\n\n        def create_and_cache_message():\n            \"\"\"Create message and update cache.\"\"\"\n            new_message = hub_client.beta.threads.messages.create(\n                thread_id=self._thread_id,\n                role=role,  # type: ignore\n                content=message,\n                extra_body={\n                    \"assistant_id\": self.get_primary_agent().identifier,\n                    \"run_id\": self._run_id,\n                },\n                metadata=kwargs,\n                attachments=attachments,\n            )\n\n            # Update cache when adding messages to the main thread\n            if self._messages_cache is not None:\n                # Add new message to the end of the cache (most recent)\n                self._messages_cache.append(new_message)\n\n            return new_message\n\n        if self._debug_mode:\n            self.add_chat_log(role, message)\n\n        self._execute_sync_or_async(create_and_cache_message)\n\n    self._add_message = _add_message\n\n    def _list_messages(\n        limit: Union[int, NotGiven] = LIST_MESSAGES_LIMIT,\n        order: Literal[\"asc\", \"desc\"] = \"asc\",\n        thread_id: Optional[str] = None,\n    ) -&gt; List[Message]:\n        \"\"\"Returns messages from the environment.\"\"\"\n        # Use cache if available and we're querying the same thread\n        target_thread_id = thread_id or self._thread_id\n        if self._messages_cache is not None and target_thread_id == self._thread_id:\n            # Return cached messages, applying limit and order\n            cached_messages = self._messages_cache.copy()\n            if order == \"desc\":\n                cached_messages.reverse()\n            if isinstance(limit, int):\n                cached_messages = cached_messages[:limit]\n            self.add_system_log(f\"Retrieved {len(cached_messages)} messages from cache\")\n            return cached_messages\n\n        # Fetch from API\n        messages = hub_client.beta.threads.messages.list(thread_id=target_thread_id, limit=limit, order=order)\n        self.add_system_log(f\"Retrieved {len(messages.data)} messages from NEAR AI Hub\")\n\n        # Cache messages if this is for the main thread\n        if target_thread_id == self._thread_id:\n            # Store in ascending order for consistency\n            if order == \"desc\":\n                self._messages_cache = list(reversed(messages.data))\n            else:\n                self._messages_cache = messages.data.copy()\n\n        return messages.data\n\n    self._list_messages = _list_messages\n\n    def list_files_from_thread(\n        order: Literal[\"asc\", \"desc\"] = \"desc\", thread_id: Optional[str] = None\n    ) -&gt; List[FileObject]:\n        \"\"\"Lists files in the thread.\"\"\"\n        target_thread_id = thread_id or self._thread_id\n        if self._files_from_thread_cache is not None and target_thread_id == self._thread_id:\n            # Return cached files, applying order\n            files_from_thread_cache = self._files_from_thread_cache.copy()\n            if order == \"asc\":\n                files_from_thread_cache.reverse()\n            self.add_system_log(f\"Retrieved {len(files_from_thread_cache)} files from cache\")\n            return files_from_thread_cache\n\n        messages = self._list_messages(order=order, thread_id=target_thread_id)\n        # Extract attachments from messages\n        attachments = [a for m in messages if m.attachments for a in m.attachments]\n        # Extract files from attachments\n        file_ids = [a.file_id for a in attachments]\n        files = [hub_client.files.retrieve(f) for f in file_ids if f]\n\n        # Cache files if this is for the main thread\n        if target_thread_id == self._thread_id:\n            # Store in descending order by default\n            if order == \"asc\":\n                self._files_from_thread_cache = list(reversed(files))\n            else:\n                self._files_from_thread_cache = files.copy()\n\n        return files\n\n    self.list_files_from_thread = list_files_from_thread\n\n    def read_file_by_id(file_id: str, decode: Union[str, None] = \"utf-8\"):\n        \"\"\"Read a file from the thread.\"\"\"\n        content = hub_client.files.content(file_id).content\n\n        if decode:\n            return content.decode(decode)\n\n        return content\n\n    self.read_file_by_id = read_file_by_id\n\n    def write_file(\n        filename: str,\n        content: Union[str, bytes],\n        encoding: Union[str, None] = \"utf-8\",\n        filetype: str = \"text/plain\",\n        write_to_disk: bool = True,\n        logging: bool = True,\n    ) -&gt; None:\n        \"\"\"Writes a file to the environment.\n\n        filename: The name of the file to write to\n        content: The content to write to the file\n        encoding: The encoding to use when writing the file (default is utf-8)\n        filetype: The MIME type of the file (default is text/plain)\n        write_to_disk: If True, write locally to disk (default is True)\n        \"\"\"\n\n        def create_and_upload_file():\n            \"\"\"Create file locally and upload to Hub.\"\"\"\n            if write_to_disk:\n                # Write locally\n                path = Path(self.get_primary_agent_temp_dir()) / filename\n                path.parent.mkdir(parents=True, exist_ok=True)\n                if isinstance(content, bytes):\n                    with open(path, \"wb\") as f:\n                        f.write(content)\n                else:\n                    with open(path, \"w\", encoding=encoding) as f:\n                        f.write(content)\n\n            if isinstance(content, bytes):\n                file_data = content\n            else:\n                file_data = io.BytesIO(content.encode(encoding))  # type:ignore\n\n            # Upload to Hub\n            file = hub_client.files.create(file=(filename, file_data, filetype), purpose=\"assistants\")\n\n            self.add_reply(\n                message=f\"Output file: {filename}\",\n                attachments=[{\"file_id\": file.id}],\n                message_type=\"system:output_file\",\n            )\n\n            if logging:\n                self.add_system_log(f\"Uploaded file {filename} with {len(content)} characters, id: {file.id}\")\n\n            return file\n\n        self._execute_sync_or_async(create_and_upload_file)\n\n    self.write_file = write_file\n\n    def mark_done() -&gt; None:  # noqa: D102\n        hub_client.beta.threads.runs.update(\n            thread_id=self._thread_id,\n            run_id=self._run_id,\n            extra_body={\n                \"status\": \"completed\",\n                \"completed_at\": datetime.now().isoformat(),\n            },\n        )\n\n    self.mark_done = mark_done\n\n    def _invalidate_caches() -&gt; None:\n        \"\"\"Invalidate caches.\"\"\"\n        self._messages_cache = None\n        self._files_from_thread_cache = None\n\n    def mark_failed() -&gt; None:\n        \"\"\"Deprecated. Do not use.\"\"\"\n        pass\n\n    self.mark_failed = mark_failed\n\n    def request_user_input() -&gt; None:\n        \"\"\"Deprecated. Do not use.\"\"\"\n        pass\n\n    self.request_user_input = request_user_input\n\n    def request_agent_input() -&gt; Run:\n        \"\"\"Mark the run as ready for input from another agent.\"\"\"\n        return hub_client.beta.threads.runs.update(\n            thread_id=self._thread_id,\n            run_id=self._run_id,\n            extra_body={\"status\": \"requires_action\", \"required_action\": {\"type\": \"agent_input\"}},\n        )\n\n    self.request_agent_input = request_agent_input\n\n    self._invalidate_caches = _invalidate_caches\n\n    # Must be placed after method definitions\n    self.register_standard_tools()\n\n    if self._debug_mode:\n        # Try to load existing logs from thread if they don't exist locally\n        self._load_log_from_thread(SYSTEM_LOG_FILENAME)\n        self._load_log_from_thread(AGENT_LOG_FILENAME)\n        self._load_log_from_thread(CHAT_HISTORY_FILENAME)\n    logger = logging.getLogger(\"system_logger\")\n    logger.handlers = []\n    logger = logging.getLogger(\"agent_logger\")\n    logger.handlers = []\n    logger = logging.getLogger(\"chat_logger\")\n    logger.handlers = []\n\n    self._initialized = True\n    env_init_metrics.notify_of_next_step()\n</code></pre>"},{"location":"api/#nearai.agents.environment.Environment._load_log_from_thread","title":"_load_log_from_thread","text":"<pre><code>_load_log_from_thread(filename: str) -&gt; Optional[str]\n</code></pre> <p>Load log file from thread if it doesn't exist locally.</p> Source code in <code>nearai/agents/environment.py</code> <pre><code>def _load_log_from_thread(self, filename: str) -&gt; Optional[str]:\n    \"\"\"Load log file from thread if it doesn't exist locally.\"\"\"\n    local_path = os.path.join(self.get_primary_agent_temp_dir(), filename)\n    print(f\"Logging {filename} at: {local_path}\")\n    if not os.path.exists(local_path):\n        try:\n            content = self.read_file(filename, decode=\"utf-8\")\n            if content and isinstance(content, str):  # Type guard to ensure it's a string\n                with open(os.path.join(local_path), \"w\") as f:\n                    f.write(content)\n                return content\n        except Exception:\n            pass\n    return None\n</code></pre>"},{"location":"api/#nearai.agents.environment.Environment._save_logs_to_thread","title":"_save_logs_to_thread","text":"<pre><code>_save_logs_to_thread(log_file: str)\n</code></pre> <p>Save log file to thread.</p> Source code in <code>nearai/agents/environment.py</code> <pre><code>def _save_logs_to_thread(self, log_file: str):\n    \"\"\"Save log file to thread.\"\"\"\n    if not self._debug_mode:\n        return\n    log_path = os.path.join(self.get_primary_agent_temp_dir(), log_file)\n    if os.path.exists(log_path):\n        try:\n            with open(log_path, \"r\") as f:\n                content = f.read()\n            # Only upload if there's content\n            if content:\n                self.write_file(log_file, content, write_to_disk=False, logging=False)\n        except Exception as e:\n            print(f\"Failed to save {log_file} to thread: {e}\")\n</code></pre>"},{"location":"api/#nearai.agents.environment.Environment.add_agent_log","title":"add_agent_log","text":"<pre><code>add_agent_log(log: str, level: int = INFO) -&gt; None\n</code></pre> <p>Add agent log with timestamp and log level.</p> Source code in <code>nearai/agents/environment.py</code> <pre><code>def add_agent_log(self, log: str, level: int = logging.INFO) -&gt; None:\n    \"\"\"Add agent log with timestamp and log level.\"\"\"\n    if not self._initialized:\n        return\n    logger = logging.getLogger(\"agent_logger\")\n    if not logger.handlers:\n        # Configure the logger if it hasn't been set up yet\n        logger.setLevel(logging.DEBUG)\n        file_handler = logging.FileHandler(os.path.join(self.get_primary_agent_temp_dir(), AGENT_LOG_FILENAME))\n        formatter = logging.Formatter(\"%(asctime)s - %(levelname)s - %(message)s\", datefmt=\"%Y-%m-%d %H:%M:%S\")\n        file_handler.setFormatter(formatter)\n        logger.addHandler(file_handler)\n\n        # Add Thread log handler\n        if self._debug_mode:\n            custom_handler = CustomLogHandler(self.add_reply, \"agent\")\n            custom_handler.setFormatter(formatter)\n            logger.addHandler(custom_handler)\n\n    # Log the message\n    logger.log(level, log)\n    # Force the handler to write to disk\n    for handler in logger.handlers:\n        handler.flush()\n\n    if self._debug_mode:\n        self._save_logs_to_thread(AGENT_LOG_FILENAME)\n</code></pre>"},{"location":"api/#nearai.agents.environment.Environment.add_agent_start_system_log","title":"add_agent_start_system_log","text":"<pre><code>add_agent_start_system_log(agent_idx: int) -&gt; None\n</code></pre> <p>Adds agent start system log.</p> Source code in <code>nearai/agents/environment.py</code> <pre><code>def add_agent_start_system_log(self, agent_idx: int) -&gt; None:\n    \"\"\"Adds agent start system log.\"\"\"\n    agent = self._agents[agent_idx]\n    message = f\"Running agent {agent.name}\"\n    if agent.model != \"\":\n        model = self.get_model_for_inference(agent.model)\n        self._last_used_model = model\n        message += f\" that will connect to {model}\"\n        if agent.model_temperature:\n            message += f\", temperature={agent.model_temperature}\"\n        if agent.model_max_tokens:\n            message += f\", max_tokens={agent.model_max_tokens}\"\n    self.add_system_log(message)\n</code></pre>"},{"location":"api/#nearai.agents.environment.Environment.add_chat_log","title":"add_chat_log","text":"<pre><code>add_chat_log(role: str, content: str, level: int = INFO) -&gt; None\n</code></pre> <p>Add chat history to log file when in debug mode.</p> Source code in <code>nearai/agents/environment.py</code> <pre><code>def add_chat_log(self, role: str, content: str, level: int = logging.INFO) -&gt; None:\n    \"\"\"Add chat history to log file when in debug mode.\"\"\"\n    if not self._initialized:\n        return\n    if not self._debug_mode:\n        return\n    if not isinstance(content, str):\n        content = \"content is not str\"\n    logger = logging.getLogger(\"chat_logger\")\n    if not logger.handlers:\n        # Configure the logger if it hasn't been set up yet\n        logger.setLevel(logging.DEBUG)\n        file_handler = logging.FileHandler(os.path.join(self.get_primary_agent_temp_dir(), CHAT_HISTORY_FILENAME))\n        formatter = logging.Formatter(\"%(asctime)s - %(levelname)s - %(message)s\", datefmt=\"%Y-%m-%d %H:%M:%S\")\n        file_handler.setFormatter(formatter)\n        logger.addHandler(file_handler)\n\n    # Log the message with role prefix\n    message = f\"{role.upper()}: {content}\"\n    logger.log(level, message)\n    # Force the handler to write to disk\n    for handler in logger.handlers:\n        handler.flush()\n\n    if self._debug_mode:\n        self._save_logs_to_thread(CHAT_HISTORY_FILENAME)\n</code></pre>"},{"location":"api/#nearai.agents.environment.Environment.add_message","title":"add_message","text":"<pre><code>add_message(role: str, message: str, attachments: Optional[Iterable[Attachment]] = None, **kwargs: Any)\n</code></pre> <p>Deprecated. Please use <code>add_reply</code> instead. Assistant adds a message to the environment.</p> Source code in <code>nearai/agents/environment.py</code> <pre><code>def add_message(\n    self,\n    role: str,\n    message: str,\n    attachments: Optional[Iterable[Attachment]] = None,\n    **kwargs: Any,\n):\n    \"\"\"Deprecated. Please use `add_reply` instead. Assistant adds a message to the environment.\"\"\"\n    # Prevent agent to save messages on behalf of `user` to avoid adding false memory\n    role = \"assistant\"\n\n    return self._add_message(role, message, attachments, **kwargs)\n</code></pre>"},{"location":"api/#nearai.agents.environment.Environment.add_system_log","title":"add_system_log","text":"<pre><code>add_system_log(log: str, level: int = INFO) -&gt; None\n</code></pre> <p>Add system log with timestamp and log level.</p> Source code in <code>nearai/agents/environment.py</code> <pre><code>def add_system_log(self, log: str, level: int = logging.INFO) -&gt; None:\n    \"\"\"Add system log with timestamp and log level.\"\"\"\n    if not self._initialized:\n        return\n    # NOTE: Do not call prints in this function.\n    logger = logging.getLogger(\"system_logger\")\n    if not logger.handlers:\n        # Configure the logger if it hasn't been set up yet\n        logger.setLevel(logging.DEBUG)\n        file_handler = logging.FileHandler(os.path.join(self.get_primary_agent_temp_dir(), SYSTEM_LOG_FILENAME))\n        formatter = logging.Formatter(\"%(asctime)s - %(levelname)s - %(message)s\", datefmt=\"%Y-%m-%d %H:%M:%S\")\n        file_handler.setFormatter(formatter)\n        logger.addHandler(file_handler)\n\n        if self.print_system_log:\n            console_handler = logging.StreamHandler()\n            console_handler.setFormatter(formatter)\n            logger.addHandler(console_handler)\n\n        # Add Thread log handler\n        if self._debug_mode:\n            custom_handler = CustomLogHandler(self.add_reply, \"system\")\n            custom_handler.setFormatter(formatter)\n            logger.addHandler(custom_handler)\n\n    # Log the message\n    logger.log(level, log)\n    # Force the handler to write to disk\n    for handler in logger.handlers:\n        handler.flush()\n\n    if self._debug_mode:\n        self._save_logs_to_thread(SYSTEM_LOG_FILENAME)\n</code></pre>"},{"location":"api/#nearai.agents.environment.Environment.call_agent","title":"call_agent","text":"<pre><code>call_agent(agent_index: int, task: str) -&gt; None\n</code></pre> <p>Calls agent with given task.</p> Source code in <code>nearai/agents/environment.py</code> <pre><code>def call_agent(self, agent_index: int, task: str) -&gt; None:\n    \"\"\"Calls agent with given task.\"\"\"\n    self._agents[agent_index].run(self, task=task)\n</code></pre>"},{"location":"api/#nearai.agents.environment.Environment.clear_temp_agent_files","title":"clear_temp_agent_files","text":"<pre><code>clear_temp_agent_files(verbose=True) -&gt; None\n</code></pre> <p>Remove temp agent files created to be used in <code>runpy</code>.</p> Source code in <code>nearai/agents/environment.py</code> <pre><code>def clear_temp_agent_files(self, verbose=True) -&gt; None:\n    \"\"\"Remove temp agent files created to be used in `runpy`.\"\"\"\n    for agent in self._agents:\n        if os.path.exists(agent.temp_dir):\n            if verbose:\n                print(\"removed agent.temp_files\", agent.temp_dir)\n            shutil.rmtree(agent.temp_dir)\n</code></pre>"},{"location":"api/#nearai.agents.environment.Environment.completion","title":"completion","text":"<pre><code>completion(messages: Union[Iterable[ChatCompletionMessageParam], str], model: Union[Iterable[ChatCompletionMessageParam], str] = '', **kwargs: Any) -&gt; str\n</code></pre> <p>Returns a completion for the given messages using the given model.</p> Source code in <code>nearai/agents/environment.py</code> <pre><code>def completion(\n    self,\n    messages: Union[Iterable[ChatCompletionMessageParam], str],\n    model: Union[Iterable[ChatCompletionMessageParam], str] = \"\",\n    **kwargs: Any,\n) -&gt; str:\n    \"\"\"Returns a completion for the given messages using the given model.\"\"\"\n    raw_response = self.completions(messages, model, **kwargs)\n    assert isinstance(raw_response, ModelResponse), \"Expected ModelResponse\"\n    response: ModelResponse = raw_response\n    assert all(map(lambda choice: isinstance(choice, Choices), response.choices)), \"Expected Choices\"\n    choices: List[Choices] = response.choices  # type: ignore\n    response_message = choices[0].message.content\n    assert response_message, \"No completions returned\"\n    return response_message\n</code></pre>"},{"location":"api/#nearai.agents.environment.Environment.completion_and_get_tools_calls","title":"completion_and_get_tools_calls","text":"<pre><code>completion_and_get_tools_calls(messages: List[ChatCompletionMessageParam], model: str = '', **kwargs: Any) -&gt; SimpleNamespace\n</code></pre> <p>Returns completion message and/or tool calls from OpenAI or Llama tool formats.</p> Source code in <code>nearai/agents/environment.py</code> <pre><code>def completion_and_get_tools_calls(\n    self,\n    messages: List[ChatCompletionMessageParam],\n    model: str = \"\",\n    **kwargs: Any,\n) -&gt; SimpleNamespace:\n    \"\"\"Returns completion message and/or tool calls from OpenAI or Llama tool formats.\"\"\"\n    raw_response = self._run_inference_completions(messages, model, stream=False, **kwargs)\n\n    assert isinstance(raw_response, ModelResponse), \"Expected ModelResponse\"\n    response: ModelResponse = raw_response\n    assert all(map(lambda choice: isinstance(choice, Choices), response.choices)), \"Expected Choices\"\n    choices: List[Choices] = response.choices  # type: ignore\n\n    (message_without_tool_call, tool_calls) = self._parse_tool_call(choices[0].message)\n\n    if message_without_tool_call is None:\n        response_message = choices[0].message.content\n        message_without_tool_call = response_message\n\n    return SimpleNamespace(message=message_without_tool_call, tool_calls=tool_calls)\n</code></pre>"},{"location":"api/#nearai.agents.environment.Environment.completion_and_run_tools","title":"completion_and_run_tools","text":"<pre><code>completion_and_run_tools(messages: List[ChatCompletionMessageParam], model: str = '', tools: Optional[List] = None, **kwargs: Any) -&gt; Optional[str]\n</code></pre> <p>Returns a completion for the given messages using the given model and runs tools.</p> Source code in <code>nearai/agents/environment.py</code> <pre><code>def completion_and_run_tools(\n    self,\n    messages: List[ChatCompletionMessageParam],\n    model: str = \"\",\n    tools: Optional[List] = None,\n    **kwargs: Any,\n) -&gt; Optional[str]:\n    \"\"\"Returns a completion for the given messages using the given model and runs tools.\"\"\"\n    completion_tools_response = self.completions_and_run_tools(messages, model, tools, **kwargs)\n    assert all(\n        map(\n            lambda choice: isinstance(choice, Choices),\n            completion_tools_response.choices,\n        )\n    ), \"Expected Choices\"\n    choices: List[Choices] = completion_tools_response.choices  # type: ignore\n    response_content = choices[0].message.content\n    return response_content\n</code></pre>"},{"location":"api/#nearai.agents.environment.Environment.completions","title":"completions","text":"<pre><code>completions(messages: Union[Iterable[ChatCompletionMessageParam], str], model: Union[Iterable[ChatCompletionMessageParam], str] = '', stream: bool = False, thread_id: Optional[str] = None, attachments: Optional[Iterable[Attachment]] = None, message_type: Optional[str] = None, **kwargs: Any) -&gt; Union[ModelResponse, CustomStreamWrapper]\n</code></pre> <p>Returns all completions for given messages using the given model.</p> <p>Always returns a ModelResponse object. When stream=True, aggregates the streamed content into a ModelResponse. When stream=False, returns the ModelResponse directly.</p> Source code in <code>nearai/agents/environment.py</code> <pre><code>def completions(\n    self,\n    messages: Union[Iterable[ChatCompletionMessageParam], str],\n    model: Union[Iterable[ChatCompletionMessageParam], str] = \"\",\n    stream: bool = False,\n    thread_id: Optional[str] = None,\n    attachments: Optional[Iterable[Attachment]] = None,\n    message_type: Optional[str] = None,\n    **kwargs: Any,\n) -&gt; Union[ModelResponse, CustomStreamWrapper]:\n    \"\"\"Returns all completions for given messages using the given model.\n\n    Always returns a ModelResponse object. When stream=True, aggregates the streamed\n    content into a ModelResponse. When stream=False, returns the ModelResponse directly.\n    \"\"\"\n    params, kwargs = self.get_inference_parameters(messages, model, stream, **kwargs)\n    if stream:\n        message_id = None\n        kwargs.setdefault(\"extra_headers\", {}).update(\n            {\n                k: v\n                for k, v in {\n                    \"run_id\": self._run_id,\n                    \"thread_id\": thread_id if thread_id else self._thread_id,\n                    \"message_id\": message_id,\n                }.items()\n                if v is not None\n            }\n        )\n\n        # Pass thread_id, attachments, and message_type to the server\n        stream_results = self._run_inference_completions(\n            messages, model, True, thread_id=thread_id, attachments=attachments, message_type=message_type, **kwargs\n        )\n        full_content = \"\"\n        for chunk in stream_results:\n            if not isinstance(chunk, (tuple, str)) and hasattr(chunk, \"choices\"):\n                if chunk.choices and hasattr(chunk.choices[0], \"delta\"):\n                    delta = chunk.choices[0].delta\n                    if hasattr(delta, \"content\") and delta.content:\n                        full_content += delta.content\n\n        response = ModelResponse(\n            id=\"streamed_completion\",\n            object=\"chat.completion\",\n            created=int(time.time()),\n            model=params.model,\n            choices=[\n                Choices(index=0, message={\"role\": \"assistant\", \"content\": full_content}, finish_reason=\"stop\")\n            ],\n            usage={\"prompt_tokens\": 0, \"completion_tokens\": 0, \"total_tokens\": 0},\n        )\n        return response\n    else:\n        return self._run_inference_completions(messages, model, False, **kwargs)\n</code></pre>"},{"location":"api/#nearai.agents.environment.Environment.completions_and_run_tools","title":"completions_and_run_tools","text":"<pre><code>completions_and_run_tools(messages: List[ChatCompletionMessageParam], model: str = '', tools: Optional[List] = None, add_responses_to_messages: bool = True, agent_role_name='assistant', tool_role_name='tool', **kwargs: Any) -&gt; ModelResponse\n</code></pre> <p>Returns all completions for given messages using the given model and runs tools.</p> Source code in <code>nearai/agents/environment.py</code> <pre><code>def completions_and_run_tools(\n    self,\n    messages: List[ChatCompletionMessageParam],\n    model: str = \"\",\n    tools: Optional[List] = None,\n    add_responses_to_messages: bool = True,\n    agent_role_name=\"assistant\",\n    tool_role_name=\"tool\",\n    **kwargs: Any,\n) -&gt; ModelResponse:\n    \"\"\"Returns all completions for given messages using the given model and runs tools.\"\"\"\n    if self._use_llama_tool_syntax(model, tools):\n        tool_prompt = self._llama_tool_prompt(tools)\n        messages.append({\"role\": \"system\", \"content\": tool_prompt})\n    raw_response = self._run_inference_completions(messages, model, stream=False, tools=tools, **kwargs)\n    assert isinstance(raw_response, ModelResponse), \"Expected ModelResponse\"\n    response: ModelResponse = raw_response\n    assert all(map(lambda choice: isinstance(choice, Choices), response.choices)), \"Expected Choices\"\n    choices: List[Choices] = response.choices  # type: ignore\n    response_message = choices[0].message\n\n    self._handle_tool_calls(response_message, add_responses_to_messages, agent_role_name, tool_role_name)\n\n    return response\n</code></pre>"},{"location":"api/#nearai.agents.environment.Environment.environment_run_info","title":"environment_run_info","text":"<pre><code>environment_run_info(base_id, run_type) -&gt; dict\n</code></pre> <p>Returns the environment run information.</p> Source code in <code>nearai/agents/environment.py</code> <pre><code>def environment_run_info(self, base_id, run_type) -&gt; dict:\n    \"\"\"Returns the environment run information.\"\"\"\n    if not self._agents or not self.get_primary_agent():\n        raise ValueError(\"Agent not found\")\n    primary_agent = self.get_primary_agent()\n\n    full_agent_name = \"/\".join([primary_agent.namespace, primary_agent.name, primary_agent.version])\n    safe_agent_name = full_agent_name.replace(\"/\", \"_\")\n    uid = uuid.uuid4().hex\n    generated_name = f\"environment_run_{safe_agent_name}_{uid}\"\n    name = generated_name\n\n    timestamp = datetime.now(timezone.utc).isoformat()\n    return {\n        \"name\": name,\n        \"version\": \"0\",\n        \"description\": f\"Agent {run_type} {full_agent_name} {uid} {timestamp}\",\n        \"category\": \"environment\",\n        \"tags\": [\"environment\"],\n        \"details\": {\n            \"base_id\": base_id,\n            \"timestamp\": timestamp,\n            \"agents\": [agent.name for agent in self._agents],\n            \"primary_agent_namespace\": primary_agent.namespace,\n            \"primary_agent_name\": primary_agent.name,\n            \"primary_agent_version\": primary_agent.version,\n            \"run_id\": self._run_id,\n            \"run_type\": run_type,\n        },\n        \"show_entry\": True,\n    }\n</code></pre>"},{"location":"api/#nearai.agents.environment.Environment.generate_folder_hash_id","title":"generate_folder_hash_id","text":"<pre><code>generate_folder_hash_id(path: str) -&gt; str\n</code></pre> <p>Returns hash based on files and their contents in path, including subfolders.</p> Source code in <code>nearai/agents/environment.py</code> <pre><code>def generate_folder_hash_id(self, path: str) -&gt; str:\n    \"\"\"Returns hash based on files and their contents in path, including subfolders.\"\"\"  # noqa: E501\n    hash_obj = hashlib.md5()\n\n    for root, _dirs, files in os.walk(path):\n        for file in sorted(files):\n            file_path = os.path.join(root, file)\n            with open(file_path, \"rb\") as f:\n                while chunk := f.read(8192):\n                    hash_obj.update(chunk)\n\n    return hash_obj.hexdigest()\n</code></pre>"},{"location":"api/#nearai.agents.environment.Environment.get_agent_temp_path","title":"get_agent_temp_path","text":"<pre><code>get_agent_temp_path() -&gt; Path\n</code></pre> <p>Returns temp dir for primary agent where execution happens.</p> Source code in <code>nearai/agents/environment.py</code> <pre><code>def get_agent_temp_path(self) -&gt; Path:\n    \"\"\"Returns temp dir for primary agent where execution happens.\"\"\"\n    return self.get_primary_agent_temp_dir()\n</code></pre>"},{"location":"api/#nearai.agents.environment.Environment.get_agents","title":"get_agents","text":"<pre><code>get_agents() -&gt; List[Agent]\n</code></pre> <p>Returns list of agents available in environment.</p> Source code in <code>nearai/agents/environment.py</code> <pre><code>def get_agents(self) -&gt; List[Agent]:\n    \"\"\"Returns list of agents available in environment.\"\"\"\n    return self._agents\n</code></pre>"},{"location":"api/#nearai.agents.environment.Environment.get_inference_parameters","title":"get_inference_parameters","text":"<pre><code>get_inference_parameters(messages: Union[Iterable[ChatCompletionMessageParam], str], model: Union[Iterable[ChatCompletionMessageParam], str], stream: bool, **kwargs: Any) -&gt; Tuple[InferenceParameters, Any]\n</code></pre> <p>Run inference parameters to run completions.</p> Source code in <code>nearai/agents/environment.py</code> <pre><code>def get_inference_parameters(\n    self,\n    messages: Union[Iterable[ChatCompletionMessageParam], str],\n    model: Union[Iterable[ChatCompletionMessageParam], str],\n    stream: bool,\n    **kwargs: Any,\n) -&gt; Tuple[InferenceParameters, Any]:\n    \"\"\"Run inference parameters to run completions.\"\"\"\n    if isinstance(messages, str):\n        self.add_system_log(\n            \"Deprecated completions call. Pass `messages` as a first parameter.\",\n            logging.WARNING,\n        )\n        messages_or_model = messages\n        model_or_messages = model\n        model = cast(str, messages_or_model)\n        messages = cast(Iterable[ChatCompletionMessageParam], model_or_messages)\n    else:\n        model = cast(str, model)\n        messages = cast(Iterable[ChatCompletionMessageParam], messages)\n    model = self.get_model_for_inference(model)\n    if model != self._last_used_model:\n        self._last_used_model = model\n        self.add_system_log(f\"Connecting to {model}\")\n\n    temperature = kwargs.pop(\"temperature\", self.get_primary_agent().model_temperature if self._agents else None)\n    max_tokens = kwargs.pop(\"max_tokens\", self.get_primary_agent().model_max_tokens if self._agents else None)\n\n    params = InferenceParameters(\n        model=model,\n        messages=messages,\n        stream=stream,\n        temperature=temperature,\n        max_tokens=max_tokens,\n    )\n\n    return params, kwargs\n</code></pre>"},{"location":"api/#nearai.agents.environment.Environment.get_last_message","title":"get_last_message","text":"<pre><code>get_last_message(role: str = 'user')\n</code></pre> <p>Reads last message from the given role and returns it.</p> Source code in <code>nearai/agents/environment.py</code> <pre><code>def get_last_message(self, role: str = \"user\"):\n    \"\"\"Reads last message from the given role and returns it.\"\"\"\n    for message in reversed(self.list_messages()):\n        if message.get(\"role\") == role:\n            return message\n\n    return None\n</code></pre>"},{"location":"api/#nearai.agents.environment.Environment.get_primary_agent","title":"get_primary_agent","text":"<pre><code>get_primary_agent() -&gt; Agent\n</code></pre> <p>Returns the agent that is invoked first.</p> Source code in <code>nearai/agents/environment.py</code> <pre><code>def get_primary_agent(self) -&gt; Agent:\n    \"\"\"Returns the agent that is invoked first.\"\"\"\n    return self._agents[0]\n</code></pre>"},{"location":"api/#nearai.agents.environment.Environment.get_primary_agent_temp_dir","title":"get_primary_agent_temp_dir","text":"<pre><code>get_primary_agent_temp_dir() -&gt; Path\n</code></pre> <p>Returns temp dir for primary agent.</p> Source code in <code>nearai/agents/environment.py</code> <pre><code>def get_primary_agent_temp_dir(self) -&gt; Path:\n    \"\"\"Returns temp dir for primary agent.\"\"\"\n    return Path(self.get_primary_agent().temp_dir)\n</code></pre>"},{"location":"api/#nearai.agents.environment.Environment.get_tool_registry","title":"get_tool_registry","text":"<pre><code>get_tool_registry(new: bool = False) -&gt; ToolRegistry\n</code></pre> <p>Returns the tool registry, a dictionary of tools that can be called by the agent.</p> Source code in <code>nearai/agents/environment.py</code> <pre><code>def get_tool_registry(self, new: bool = False) -&gt; ToolRegistry:\n    \"\"\"Returns the tool registry, a dictionary of tools that can be called by the agent.\"\"\"\n    if new:\n        self._tools = ToolRegistry()\n    return self._tools\n</code></pre>"},{"location":"api/#nearai.agents.environment.Environment.list_files","title":"list_files","text":"<pre><code>list_files(path: str, order: Literal['asc', 'desc'] = 'asc') -&gt; List[str]\n</code></pre> <p>Lists files in the environment.</p> Source code in <code>nearai/agents/environment.py</code> <pre><code>def list_files(self, path: str, order: Literal[\"asc\", \"desc\"] = \"asc\") -&gt; List[str]:\n    \"\"\"Lists files in the environment.\"\"\"\n    return os.listdir(os.path.join(self.get_primary_agent_temp_dir(), path))\n</code></pre>"},{"location":"api/#nearai.agents.environment.Environment.list_messages","title":"list_messages","text":"<pre><code>list_messages(thread_id: Optional[str] = None, limit: Union[int, NotGiven] = LIST_MESSAGES_LIMIT, order: Literal['asc', 'desc'] = 'asc')\n</code></pre> <p>Backwards compatibility for chat_completions messages.</p> Source code in <code>nearai/agents/environment.py</code> <pre><code>def list_messages(\n    self,\n    thread_id: Optional[str] = None,\n    limit: Union[int, NotGiven] = LIST_MESSAGES_LIMIT,\n    order: Literal[\"asc\", \"desc\"] = \"asc\",\n):\n    \"\"\"Backwards compatibility for chat_completions messages.\"\"\"\n    messages = self._list_messages(thread_id=thread_id, limit=limit, order=order)\n\n    # Filter out system and agent log messages when running in debug mode. Agent behavior shouldn't change based on logs.  # noqa: E501\n    messages = [\n        m\n        for m in messages\n        if not (\n            m.metadata\n            and any(m.metadata.get(\"message_type\", \"\").startswith(prefix) for prefix in [\"system:\", \"agent:\"])\n        )\n    ]\n\n    legacy_messages = [\n        {\n            \"id\": m.id,\n            \"content\": \"\\n\".join([c.text.value for c in m.content]),  # type: ignore\n            \"role\": m.role,\n            \"attachments\": m.attachments,\n        }\n        for m in messages\n    ]\n    return legacy_messages\n</code></pre>"},{"location":"api/#nearai.agents.environment.Environment.read_file","title":"read_file","text":"<pre><code>read_file(filename: str, decode: Union[str, None] = 'utf-8') -&gt; Optional[Union[bytes, str]]\n</code></pre> <p>Reads a file from the environment or thread.</p> Source code in <code>nearai/agents/environment.py</code> <pre><code>def read_file(self, filename: str, decode: Union[str, None] = \"utf-8\") -&gt; Optional[Union[bytes, str]]:\n    \"\"\"Reads a file from the environment or thread.\"\"\"\n    file_content: Optional[Union[bytes, str]] = None\n    # First try to read from local filesystem\n    local_path = os.path.join(self.get_primary_agent_temp_dir(), filename)\n    if os.path.exists(local_path):\n        print(f\"Reading file {filename} from local path: {local_path}\")\n        try:\n            with open(local_path, \"rb\") as local_path_file:\n                local_file_content = local_path_file.read()\n                file_content = local_file_content\n                if decode:\n                    file_content = file_content.decode(decode)\n        except Exception as e:\n            print(f\"Error with read_file: {e}\")\n\n    if not file_content:\n        # Next check files written out by the agent.\n        # Agent output files take precedence over files packaged with the agent\n        thread_files = self.list_files_from_thread(order=\"desc\")\n\n        # Then try to read from thread, starting from the most recent\n        for f in thread_files:\n            if f.filename == filename:\n                file_content = self.read_file_by_id(f.id, decode)\n                break\n\n        if not file_content:\n            # Next check agent file cache\n            # Agent output files &amp; thread files take precedence over cached files\n            file_cache = self.get_primary_agent().file_cache\n            if file_cache:\n                file_content = file_cache.get(filename, None)\n\n        # Write the file content from the thread or cache to the local filesystem\n        # This allows exec_command to operate on the file\n        if file_content:\n            if not os.path.exists(os.path.dirname(local_path)):\n                os.makedirs(os.path.dirname(local_path))\n\n            with open(local_path, \"wb\") as local_file:\n                if isinstance(file_content, bytes):\n                    local_file.write(file_content)\n                else:\n                    local_file.write(file_content.encode(\"utf-8\"))\n\n    if not file_content:\n        self.add_system_log(f\"Warn: File {filename} not found during read_file operation\")\n\n    return file_content\n</code></pre>"},{"location":"api/#nearai.agents.environment.Environment.run","title":"run","text":"<pre><code>run(new_message: Optional[str] = None, runner_metrics: Optional[RunnerMetrics] = None) -&gt; None\n</code></pre> <p>Runs agent(s) against a new or previously created environment.</p> Source code in <code>nearai/agents/environment.py</code> <pre><code>def run(self, new_message: Optional[str] = None, runner_metrics: Optional[RunnerMetrics] = None) -&gt; None:\n    \"\"\"Runs agent(s) against a new or previously created environment.\"\"\"\n    if self.logs_collection_mode and self.analytics_collector:\n        self.analytics_collector.init_env_run_metrics(runner_metrics=runner_metrics)\n    if new_message:\n        self._add_message(\"user\", new_message)\n        # Await any pending async tasks before proceeding\n        if self._async_api_calls:\n            try:\n                asyncio.run(self._await_pending_async_tasks())\n            except Exception as e:\n                self.add_system_log(f\"Error awaiting pending async tasks: {e}\", logging.ERROR)\n    elif self._debug_mode:\n        last_user_message = self.get_last_message(role=\"user\")\n        if last_user_message:\n            content = last_user_message[\"content\"]\n            self.add_chat_log(\"user\", content)\n\n    self.set_next_actor(\"agent\")\n\n    try:\n        # Create a logging callback for agent output\n        def agent_output_logger(msg, level=logging.INFO):\n            self.add_system_log(msg, level)\n\n        error_message, traceback_message = self.get_primary_agent().run(\n            self,\n            task=new_message,\n            log_stdout_callback=agent_output_logger if self._debug_mode else None,\n            log_stderr_callback=agent_output_logger,\n        )\n        if self._debug_mode and (error_message or traceback_message):\n            message_parts = []\n\n            if error_message:\n                message_parts.append(f\"Error: \\n ```\\n{error_message}\\n```\")\n\n            if traceback_message:\n                message_parts.append(f\"Error Traceback: \\n ```\\n{traceback_message}\\n```\")\n\n            self.add_system_log(\"\\n\\n\".join(message_parts))\n\n    except Exception as e:\n        self.add_system_log(f\"Environment run failed: {e}\", logging.ERROR)\n        self.mark_failed()\n        raise e\n    finally:\n        # Await any pending async tasks before cleanup\n        if self._async_api_calls:\n            try:\n                asyncio.run(self._await_pending_async_tasks())\n            except Exception as e:\n                self.add_system_log(f\"Error awaiting pending async tasks: {e}\", logging.ERROR)\n\n        # Upload analytics data if collection is enabled\n        if self.logs_collection_mode and self.analytics_collector:\n            try:\n                self.analytics_collector.upload(thread_dir=self.get_primary_agent_temp_dir())\n            except Exception as e:\n                print(f\"Failed to upload analytics data: {e}\")\n\n        # Invalidate caches when run ends (whether successful or failed)\n        self._invalidate_caches()\n\n    if not self._pending_ext_agent:\n        # If no external agent was called, mark the whole run as done.\n        # Else this environment will stop for now but this run will be continued later.\n        self.mark_done()\n</code></pre>"},{"location":"api/#nearai.agents.environment.Environment.set_next_actor","title":"set_next_actor","text":"<pre><code>set_next_actor(who: str) -&gt; None\n</code></pre> <p>Set the next actor / action in the dialogue.</p> Source code in <code>nearai/agents/environment.py</code> <pre><code>def set_next_actor(self, who: str) -&gt; None:\n    \"\"\"Set the next actor / action in the dialogue.\"\"\"\n    next_action_fn = os.path.join(self.get_primary_agent_temp_dir(), \".next_action\")\n\n    with open(next_action_fn, \"w\") as f:\n        f.write(who)\n</code></pre>"},{"location":"api/#nearai.agents.environment.Environment.signed_completion","title":"signed_completion","text":"<pre><code>signed_completion(messages: Union[Iterable[ChatCompletionMessageParam], str], model: Union[Iterable[ChatCompletionMessageParam], str] = '', **kwargs: Any) -&gt; Dict[str, Any]\n</code></pre> <p>Returns a completion for the given messages using the given model with the agent signature.</p> Source code in <code>nearai/agents/environment.py</code> <pre><code>def signed_completion(\n    self,\n    messages: Union[Iterable[ChatCompletionMessageParam], str],\n    model: Union[Iterable[ChatCompletionMessageParam], str] = \"\",\n    **kwargs: Any,\n) -&gt; Dict[str, Any]:\n    \"\"\"Returns a completion for the given messages using the given model with the agent signature.\"\"\"\n    # TODO Return signed completions for non-latest versions only?\n    agent_name = self.get_primary_agent().get_full_name()\n    raw_response = self.completions(messages, model, agent_name=agent_name, **kwargs)\n    assert isinstance(raw_response, ModelResponse), \"Expected ModelResponse\"\n    response: ModelResponse = raw_response\n\n    signature_data = json.loads(response.system_fingerprint) if response.system_fingerprint else {}\n\n    assert all(map(lambda choice: isinstance(choice, Choices), response.choices)), \"Expected Choices\"\n    choices: List[Choices] = response.choices  # type: ignore\n    response_message = choices[0].message.content\n    assert response_message, \"No completions returned\"\n\n    return {\n        \"response\": response_message,\n        \"signature\": signature_data.get(\"signature\", None),\n        \"public_key\": signature_data.get(\"public_key\", None),\n    }\n</code></pre>"},{"location":"api/#nearai.agents.environment.Environment.verify_message","title":"verify_message","text":"<pre><code>verify_message(account_id: str, public_key: str, signature: str, message: str, nonce: str, callback_url: str) -&gt; SignatureVerificationResult\n</code></pre> <p>Verifies that the user message is signed with NEAR Account.</p> Source code in <code>nearai/agents/environment.py</code> <pre><code>def verify_message(\n    self,\n    account_id: str,\n    public_key: str,\n    signature: str,\n    message: str,\n    nonce: str,\n    callback_url: str,\n) -&gt; near.SignatureVerificationResult:\n    \"\"\"Verifies that the user message is signed with NEAR Account.\"\"\"\n    return near.verify_signed_message(\n        account_id,\n        public_key,\n        signature,\n        message,\n        nonce,\n        self.get_primary_agent().name,\n        callback_url,\n    )\n</code></pre>"},{"location":"api/#nearai.agents.environment.Environment.verify_signed_message","title":"verify_signed_message","text":"<pre><code>verify_signed_message(completion: str, messages: Union[Iterable[ChatCompletionMessageParam], str], public_key: Union[str, None] = None, signature: Union[str, None] = None, model: Union[Iterable[ChatCompletionMessageParam], str] = '', **kwargs: Any) -&gt; bool\n</code></pre> <p>Verifies a signed message.</p> Source code in <code>nearai/agents/environment.py</code> <pre><code>def verify_signed_message(\n    self,\n    completion: str,\n    messages: Union[Iterable[ChatCompletionMessageParam], str],\n    public_key: Union[str, None] = None,\n    signature: Union[str, None] = None,\n    model: Union[Iterable[ChatCompletionMessageParam], str] = \"\",\n    **kwargs: Any,\n) -&gt; bool:\n    \"\"\"Verifies a signed message.\"\"\"\n    if public_key is None or signature is None:\n        return False\n\n    params, _ = self.get_inference_parameters(messages, model, False, **kwargs)\n\n    messages_without_ids = [{k: v for k, v in item.items() if k != \"id\"} for item in params.messages]\n    ordered_messages_without_ids = [\n        {\"role\": str(item[\"role\"]), \"content\": str(item[\"content\"])} for item in messages_without_ids\n    ]\n\n    return validate_completion_signature(\n        public_key,\n        signature,\n        CompletionSignaturePayload(\n            agent_name=self.get_primary_agent().get_full_name(),\n            completion=completion,\n            model=params.model,\n            messages=ordered_messages_without_ids,\n            temperature=params.temperature,\n            max_tokens=params.max_tokens,\n        ),\n    )\n</code></pre>"},{"location":"api/#nearai.agents.tool_json_helper","title":"tool_json_helper","text":""},{"location":"api/#nearai.agents.tool_json_helper.parse_json_args","title":"parse_json_args","text":"<pre><code>parse_json_args(signature: dict, args: str)\n</code></pre> <p>Parses LLM generated JSON args, trying various repair strategies if args are not valid JSON.</p> Source code in <code>nearai/agents/tool_json_helper.py</code> <pre><code>def parse_json_args(signature: dict, args: str):\n    \"\"\"Parses LLM generated JSON args, trying various repair strategies if args are not valid JSON.\"\"\"\n    # if args is empty or an empty json object check if the function has no arguments\n    if not args or args == \"{}\":\n        if not signature[\"function\"][\"parameters\"][\"required\"]:\n            return {}\n        else:\n            raise ValueError(\"Function requires arguments\")\n\n    transforms = [\n        lambda x: json.loads(x),\n        _ending_transform,\n        lambda x: parse_json_args_based_on_signature(signature, x),\n    ]\n\n    for transform in transforms:\n        try:\n            result = transform(args)\n            # check that all result keys are valid properties in the signature\n            for key in result.keys():\n                if key not in signature[\"function\"][\"parameters\"][\"properties\"]:\n                    raise json.JSONDecodeError(f\"Unknown parameter {key}\", args, 0)\n            return result\n        except json.JSONDecodeError:\n            continue\n        except Exception as err:\n            raise json.JSONDecodeError(\"Error parsing function args\", args, 0) from err\n</code></pre>"},{"location":"api/#nearai.agents.tool_json_helper.parse_json_args_based_on_signature","title":"parse_json_args_based_on_signature","text":"<pre><code>parse_json_args_based_on_signature(signature: dict, args: str)\n</code></pre> <p>Finds parameter names based on the signature and tries to extract the values in between from the args string.</p> Source code in <code>nearai/agents/tool_json_helper.py</code> <pre><code>def parse_json_args_based_on_signature(signature: dict, args: str):\n    \"\"\"Finds parameter names based on the signature and tries to extract the values in between from the args string.\"\"\"\n    parameter_names = list(signature[\"function\"][\"parameters\"][\"properties\"].keys())\n    # find each parameter name in the args string\n    #   assuming each parameter name is surrounded by \"s, followed by a colon and optionally preceded by a comma,\n    #   extract the intervening values as values\n    parameter_positions = {}\n    parameter_values = {}\n    for param in parameter_names:\n        match = re.search(f',?\\\\s*\"({param})\"\\\\s*:', args)\n        if not match:\n            raise ValueError(f\"Parameter {param} not found in args {args}\")\n        parameter_positions[param] = (match.start(), match.end())\n    # sort the parameter positions by start position\n    sorted_positions = sorted(parameter_positions.items(), key=lambda x: x[1][0])\n    # for each parameter, extract the value from the args string\n    for i, (param, (start, end)) in enumerate(sorted_positions):  # noqa B007\n        # if this is the last parameter, extract the value from the start position to the end of the string\n        if i == len(sorted_positions) - 1:\n            raw_value = args[end:-1]\n            if raw_value.endswith(\"}\"):\n                raw_value = raw_value[:-1]\n        # otherwise, extract the value from the start position to the start position of the next parameter\n        else:\n            next_start = sorted_positions[i + 1][1][0]\n            raw_value = args[end:next_start]\n        raw_value = raw_value.strip()\n        if raw_value.startswith('\"') and raw_value.endswith('\"'):\n            raw_value = raw_value[1:-1]\n        parameter_values[param] = raw_value\n    return parameter_values\n</code></pre>"},{"location":"api/#nearai.agents.tool_registry","title":"tool_registry","text":""},{"location":"api/#nearai.agents.tool_registry.ToolRegistry","title":"ToolRegistry","text":"<p>A registry for tools that can be called by the agent.</p> <p>Tool definitions follow this structure:</p> <pre><code>{\n    \"type\": \"function\",\n    \"function\": {\n        \"name\": \"get_current_weather\",\n        \"description\": \"Get the current weather in a given location\",\n        \"parameters\": {\n            \"type\": \"object\",\n            \"properties\": {\n                \"location\": {\n                    \"type\": \"string\",\n                    \"description\": \"The city and state, e.g. San Francisco, CA\",\n                },\n                \"unit\": {\"type\": \"string\", \"enum\": [\"celsius\", \"fahrenheit\"]},\n            },\n            \"required\": [\"location\"],\n        },\n    },\n}\n</code></pre> Source code in <code>nearai/agents/tool_registry.py</code> <pre><code>class ToolRegistry:\n    \"\"\"A registry for tools that can be called by the agent.\n\n    Tool definitions follow this structure:\n\n        {\n            \"type\": \"function\",\n            \"function\": {\n                \"name\": \"get_current_weather\",\n                \"description\": \"Get the current weather in a given location\",\n                \"parameters\": {\n                    \"type\": \"object\",\n                    \"properties\": {\n                        \"location\": {\n                            \"type\": \"string\",\n                            \"description\": \"The city and state, e.g. San Francisco, CA\",\n                        },\n                        \"unit\": {\"type\": \"string\", \"enum\": [\"celsius\", \"fahrenheit\"]},\n                    },\n                    \"required\": [\"location\"],\n                },\n            },\n        }\n\n    \"\"\"\n\n    def __init__(self) -&gt; None:  # noqa: D107\n        self.tools: Dict[str, Callable] = {}\n\n    def register_tool(self, tool: Callable) -&gt; None:  # noqa: D102\n        \"\"\"Register a tool.\"\"\"\n        self.tools[tool.__name__] = tool\n\n    def register_mcp_tool(self, mcp_tool: MCPTool, call_tool: Callable) -&gt; None:  # noqa: D102\n        \"\"\"Register a tool callable from its definition.\"\"\"\n\n        async def tool(**kwargs):\n            try:\n                return await call_tool(mcp_tool.name, kwargs)\n            except Exception as e:\n                raise Exception(f\"Error calling tool {mcp_tool.name} with arguments {kwargs}: {e}\") from e\n\n        tool.__name__ = mcp_tool.name\n        tool.__doc__ = mcp_tool.description\n        tool.__setattr__(\"__schema__\", mcp_tool.inputSchema)\n\n        self.tools[mcp_tool.name] = tool\n\n    def get_tool(self, name: str) -&gt; Optional[Callable]:  # noqa: D102\n        \"\"\"Get a tool by name.\"\"\"\n        return self.tools.get(name)\n\n    def get_all_tools(self) -&gt; Dict[str, Callable]:  # noqa: D102\n        \"\"\"Get all tools.\"\"\"\n        return self.tools\n\n    def call_tool(self, name: str, **kwargs: Any) -&gt; Any:  # noqa: D102\n        \"\"\"Call a tool by name.\"\"\"\n        tool = self.get_tool(name)\n        if tool is None:\n            raise ValueError(f\"Tool '{name}' not found.\")\n        return tool(**kwargs)\n\n    def get_tool_definition(self, name: str) -&gt; Optional[Dict]:  # noqa: D102\n        \"\"\"Get the definition of a tool by name.\"\"\"\n        tool = self.get_tool(name)\n        if tool is None:\n            return None\n\n        assert tool.__doc__ is not None, f\"Docstring missing for tool '{name}'.\"\n        docstring = tool.__doc__.strip().split(\"\\n\")\n\n        # The first line of the docstring is the function description\n        function_description = docstring[0].strip()\n\n        # The rest of the lines contain parameter descriptions\n        param_descriptions = docstring[1:]\n\n        # Extract parameter names and types\n        signature = inspect.signature(tool)\n        type_hints = get_type_hints(tool)\n\n        parameters: Dict[str, Any] = {\"type\": \"object\", \"properties\": {}, \"required\": []}\n\n        if hasattr(tool, \"__schema__\"):\n            return {\n                \"type\": \"function\",\n                \"function\": {\"name\": tool.__name__, \"description\": function_description, \"parameters\": tool.__schema__},\n            }\n\n        # Iterate through function parameters\n        for param in signature.parameters.values():\n            param_name = param.name\n            param_type = type_hints.get(param_name, str)  # Default to str if type hint is missing\n            param_description = \"\"\n\n            # Find the parameter description in the docstring\n            for line in param_descriptions:\n                if line.strip().startswith(param_name):\n                    param_description = line.strip().split(\":\", 1)[1].strip()\n                    break\n\n            # Convert type hint to JSON Schema type\n            if isinstance(param_type, _GenericAlias) and param_type.__origin__ is Literal:\n                json_type = \"string\"\n            else:\n                json_type = param_type.__name__.lower()\n\n            if json_type == \"union\":\n                json_type = [t.__name__.lower() for t in param_type.__args__][0]\n\n            json_type = {\"int\": \"integer\", \"float\": \"number\", \"str\": \"string\", \"bool\": \"boolean\"}.get(\n                json_type, \"string\"\n            )\n\n            # Add parameter to the definition\n            parameters[\"properties\"][param_name] = {\"description\": param_description, \"type\": json_type}\n\n            # Params without default values are required params\n            if param.default == inspect.Parameter.empty:\n                parameters[\"required\"].append(param_name)\n\n        return {\n            \"type\": \"function\",\n            \"function\": {\"name\": tool.__name__, \"description\": function_description, \"parameters\": parameters},\n        }\n\n    def get_all_tool_definitions(self) -&gt; list[Dict]:  # noqa: D102\n        definitions = []\n        for tool_name, _tool in self.tools.items():\n            definition = self.get_tool_definition(tool_name)\n            if definition is not None:\n                definitions.append(definition)\n        return definitions\n</code></pre>"},{"location":"api/#nearai.agents.tool_registry.ToolRegistry.call_tool","title":"call_tool","text":"<pre><code>call_tool(name: str, **kwargs: Any) -&gt; Any\n</code></pre> <p>Call a tool by name.</p> Source code in <code>nearai/agents/tool_registry.py</code> <pre><code>def call_tool(self, name: str, **kwargs: Any) -&gt; Any:  # noqa: D102\n    \"\"\"Call a tool by name.\"\"\"\n    tool = self.get_tool(name)\n    if tool is None:\n        raise ValueError(f\"Tool '{name}' not found.\")\n    return tool(**kwargs)\n</code></pre>"},{"location":"api/#nearai.agents.tool_registry.ToolRegistry.get_all_tools","title":"get_all_tools","text":"<pre><code>get_all_tools() -&gt; Dict[str, Callable]\n</code></pre> <p>Get all tools.</p> Source code in <code>nearai/agents/tool_registry.py</code> <pre><code>def get_all_tools(self) -&gt; Dict[str, Callable]:  # noqa: D102\n    \"\"\"Get all tools.\"\"\"\n    return self.tools\n</code></pre>"},{"location":"api/#nearai.agents.tool_registry.ToolRegistry.get_tool","title":"get_tool","text":"<pre><code>get_tool(name: str) -&gt; Optional[Callable]\n</code></pre> <p>Get a tool by name.</p> Source code in <code>nearai/agents/tool_registry.py</code> <pre><code>def get_tool(self, name: str) -&gt; Optional[Callable]:  # noqa: D102\n    \"\"\"Get a tool by name.\"\"\"\n    return self.tools.get(name)\n</code></pre>"},{"location":"api/#nearai.agents.tool_registry.ToolRegistry.get_tool_definition","title":"get_tool_definition","text":"<pre><code>get_tool_definition(name: str) -&gt; Optional[Dict]\n</code></pre> <p>Get the definition of a tool by name.</p> Source code in <code>nearai/agents/tool_registry.py</code> <pre><code>def get_tool_definition(self, name: str) -&gt; Optional[Dict]:  # noqa: D102\n    \"\"\"Get the definition of a tool by name.\"\"\"\n    tool = self.get_tool(name)\n    if tool is None:\n        return None\n\n    assert tool.__doc__ is not None, f\"Docstring missing for tool '{name}'.\"\n    docstring = tool.__doc__.strip().split(\"\\n\")\n\n    # The first line of the docstring is the function description\n    function_description = docstring[0].strip()\n\n    # The rest of the lines contain parameter descriptions\n    param_descriptions = docstring[1:]\n\n    # Extract parameter names and types\n    signature = inspect.signature(tool)\n    type_hints = get_type_hints(tool)\n\n    parameters: Dict[str, Any] = {\"type\": \"object\", \"properties\": {}, \"required\": []}\n\n    if hasattr(tool, \"__schema__\"):\n        return {\n            \"type\": \"function\",\n            \"function\": {\"name\": tool.__name__, \"description\": function_description, \"parameters\": tool.__schema__},\n        }\n\n    # Iterate through function parameters\n    for param in signature.parameters.values():\n        param_name = param.name\n        param_type = type_hints.get(param_name, str)  # Default to str if type hint is missing\n        param_description = \"\"\n\n        # Find the parameter description in the docstring\n        for line in param_descriptions:\n            if line.strip().startswith(param_name):\n                param_description = line.strip().split(\":\", 1)[1].strip()\n                break\n\n        # Convert type hint to JSON Schema type\n        if isinstance(param_type, _GenericAlias) and param_type.__origin__ is Literal:\n            json_type = \"string\"\n        else:\n            json_type = param_type.__name__.lower()\n\n        if json_type == \"union\":\n            json_type = [t.__name__.lower() for t in param_type.__args__][0]\n\n        json_type = {\"int\": \"integer\", \"float\": \"number\", \"str\": \"string\", \"bool\": \"boolean\"}.get(\n            json_type, \"string\"\n        )\n\n        # Add parameter to the definition\n        parameters[\"properties\"][param_name] = {\"description\": param_description, \"type\": json_type}\n\n        # Params without default values are required params\n        if param.default == inspect.Parameter.empty:\n            parameters[\"required\"].append(param_name)\n\n    return {\n        \"type\": \"function\",\n        \"function\": {\"name\": tool.__name__, \"description\": function_description, \"parameters\": parameters},\n    }\n</code></pre>"},{"location":"api/#nearai.agents.tool_registry.ToolRegistry.register_mcp_tool","title":"register_mcp_tool","text":"<pre><code>register_mcp_tool(mcp_tool: MCPTool, call_tool: Callable) -&gt; None\n</code></pre> <p>Register a tool callable from its definition.</p> Source code in <code>nearai/agents/tool_registry.py</code> <pre><code>def register_mcp_tool(self, mcp_tool: MCPTool, call_tool: Callable) -&gt; None:  # noqa: D102\n    \"\"\"Register a tool callable from its definition.\"\"\"\n\n    async def tool(**kwargs):\n        try:\n            return await call_tool(mcp_tool.name, kwargs)\n        except Exception as e:\n            raise Exception(f\"Error calling tool {mcp_tool.name} with arguments {kwargs}: {e}\") from e\n\n    tool.__name__ = mcp_tool.name\n    tool.__doc__ = mcp_tool.description\n    tool.__setattr__(\"__schema__\", mcp_tool.inputSchema)\n\n    self.tools[mcp_tool.name] = tool\n</code></pre>"},{"location":"api/#nearai.agents.tool_registry.ToolRegistry.register_tool","title":"register_tool","text":"<pre><code>register_tool(tool: Callable) -&gt; None\n</code></pre> <p>Register a tool.</p> Source code in <code>nearai/agents/tool_registry.py</code> <pre><code>def register_tool(self, tool: Callable) -&gt; None:  # noqa: D102\n    \"\"\"Register a tool.\"\"\"\n    self.tools[tool.__name__] = tool\n</code></pre>"},{"location":"api/#nearai.cli","title":"cli","text":""},{"location":"api/#nearai.cli.AgentCli","title":"AgentCli","text":"<p>For creating and interacting with agents.</p> <p>Agent commands allow you to create and interact with agents by running them locally or via NEAR AI Cloud.</p> Commands <p>nearai agent create : Create a new agent   (--name, --description) nearai agent interactive : Run an agent interactively   (--agent, --thread-id, --tool-resources, --local, --verbose, --env-vars) nearai agent task : Run a single task with an agent   (--agent*, --task*, --thread-id, --tool-resources, --file-ids, --local, --verbose, --env-vars) nearai agent upload : Upload an agent to the registry   (--local-path, --bump, --minor-bump, --major-bump) nearai agent dev : Run local UI for development of agents nearai agent inspect : Inspect environment from given path   (*) Options <p>* (str) :   Path to the agent directory or agent ID --name (str) :   Name for the new agent --description (str) :   Description of the new agent --agent (str) :   Path to the agent directory or agent ID --thread-id (str) :   Thread ID to continue an existing conversation --tool-resources (dict) :   Tool resources to pass to the agent --file-ids (list) :   File IDs to attach to the message --local (bool) :   Run the agent locally instead of in the cloud --verbose (bool) :   Show detailed debug information during execution --env-vars (dict) :   Environment variables to pass to the agent --task (str) :   Task to run with the agent --bump (bool) :   Automatically increment patch version if it already exists --minor-bump (bool) :   Bump with minor version increment (0.1.0 \u2192 0.2.0) --major-bump (bool) :   Bump with major version increment (1.5.0 \u2192 2.0.0) --stream (bool) :   Stream the agent's output, only works with agents that stream completions <p>Examples:</p>"},{"location":"api/#nearai.cli.AgentCli--create-a-new-agent-interactively-step-by-step-prompts","title":"Create a new agent interactively (Step-by-step prompts)","text":"<p>nearai agent create</p>"},{"location":"api/#nearai.cli.AgentCli--create-a-new-agent-with-specific-name-and-description","title":"Create a new agent with specific name and description","text":"<p>nearai agent create --name my-agent --description \"My helpful assistant\"</p>"},{"location":"api/#nearai.cli.AgentCli--run-an-agent-interactively-choose-an-agent-from-the-list","title":"Run an agent interactively (Choose an agent from the list)","text":"<p>nearai agent interactive</p>"},{"location":"api/#nearai.cli.AgentCli--run-a-specific-agent-interactively-in-local-mode","title":"Run a specific agent interactively in local mode","text":"<p>nearai agent interactive --agent path/to/agent --local</p>"},{"location":"api/#nearai.cli.AgentCli--run-a-single-task-with-an-agent","title":"Run a single task with an agent","text":"<p>nearai agent task --agent example.near/agent-name/0.0.3 --task \"Summarize this article: https://example.com/article\"</p>"},{"location":"api/#nearai.cli.AgentCli--upload-an-agent-to-the-registry","title":"Upload an agent to the registry","text":"<p>nearai agent upload ./path/to/agent</p>"},{"location":"api/#nearai.cli.AgentCli--upload-an-agent-with-automatic-version-bumping","title":"Upload an agent with automatic version bumping","text":"<p>nearai agent upload ./path/to/agent --bump</p> Documentation <p>https://docs.near.ai/agents/quickstart</p> Source code in <code>nearai/cli.py</code> <pre><code>class AgentCli:\n    \"\"\"For creating and interacting with agents.\n\n    Agent commands allow you to create and interact with agents by running them locally or via NEAR AI Cloud.\n\n    Commands:\n      nearai agent create : Create a new agent\n        (--name, --description)\n      nearai agent interactive : Run an agent interactively\n        (--agent, --thread-id, --tool-resources, --local, --verbose, --env-vars)\n      nearai agent task : Run a single task with an agent\n        (--agent*, --task*, --thread-id, --tool-resources, --file-ids, --local, --verbose, --env-vars)\n      nearai agent upload : Upload an agent to the registry\n        (--local-path, --bump, --minor-bump, --major-bump)\n      nearai agent dev : Run local UI for development of agents\n      nearai agent inspect : Inspect environment from given path\n        (&lt;path&gt;*)\n\n    Options:\n      &lt;path&gt;* (str) :\n        Path to the agent directory or agent ID\n      --name (str) :\n        Name for the new agent\n      --description (str) :\n        Description of the new agent\n      --agent (str) :\n        Path to the agent directory or agent ID\n      --thread-id (str) :\n        Thread ID to continue an existing conversation\n      --tool-resources (dict) :\n        Tool resources to pass to the agent\n      --file-ids (list) :\n        File IDs to attach to the message\n      --local (bool) :\n        Run the agent locally instead of in the cloud\n      --verbose (bool) :\n        Show detailed debug information during execution\n      --env-vars (dict) :\n        Environment variables to pass to the agent\n      --task (str) :\n        Task to run with the agent\n      --bump (bool) :\n        Automatically increment patch version if it already exists\n      --minor-bump (bool) :\n        Bump with minor version increment (0.1.0 \u2192 0.2.0)\n      --major-bump (bool) :\n        Bump with major version increment (1.5.0 \u2192 2.0.0)\n      --stream (bool) :\n        Stream the agent's output, only works with agents that stream completions\n\n    Examples:\n      # Create a new agent interactively (Step-by-step prompts)\n      nearai agent create\n\n      # Create a new agent with specific name and description\n      nearai agent create --name my-agent --description \"My helpful assistant\"\n\n      # Run an agent interactively (Choose an agent from the list)\n      nearai agent interactive\n\n      # Run a specific agent interactively in local mode\n      nearai agent interactive --agent path/to/agent --local\n\n      # Run a single task with an agent\n      nearai agent task --agent example.near/agent-name/0.0.3 --task \"Summarize this article: https://example.com/article\"\n\n      # Upload an agent to the registry\n      nearai agent upload ./path/to/agent\n\n      # Upload an agent with automatic version bumping\n      nearai agent upload ./path/to/agent --bump\n\n    Documentation:\n      https://docs.near.ai/agents/quickstart\n\n    \"\"\"\n\n    def dev(self) -&gt; int:\n        \"\"\"Run a local development UI for agents that have their own UI.\n\n        This launches a local server for testing and developing agent functionality in a browser-based environment.\n\n        Examples:\n          # Start the local development server\n          nearai agent dev\n\n        \"\"\"\n        if not os.path.exists(\"hub/demo/.env\"):\n            shutil.copy(\"hub/demo/.env.example\", \"hub/demo/.env\")\n\n        ret_val = os.system(\"npm install --prefix hub/demo\")\n        if ret_val != 0:\n            print(\"Node.js is required to run the development server.\")\n            print(\"Please install Node.js from https://nodejs.org/\")\n        ret_val = os.system(\"npm run dev --prefix hub/demo\")\n        return ret_val\n\n    def inspect(self, path: str) -&gt; None:\n        \"\"\"Inspect the environment and contents of an agent at the specified path.\n\n        This launches a Streamlit interface showing the agent's structure, code, and metadata.\n\n        Args:\n          path : str\n            Path to the agent directory to inspect (required)\n\n        Examples:\n          # Inspect a local agent\n          nearai agent inspect ./path/to/agent\n\n          # Inspect a downloaded registry agent\n          nearai agent inspect .near-registry/your-namespace/agent-name/0.1.0\n\n        \"\"\"\n        import subprocess\n\n        filename = Path(os.path.abspath(__file__)).parent / \"streamlit_inspect.py\"\n        subprocess.call([\"streamlit\", \"run\", filename, \"--\", path])\n\n    def interactive(\n        self,\n        agent: Optional[str] = None,\n        thread_id: Optional[str] = None,\n        tool_resources: Optional[Dict[str, Any]] = None,\n        local: bool = False,\n        verbose: bool = False,\n        env_vars: Optional[Dict[str, Any]] = None,\n        stream: bool = False,\n    ) -&gt; None:\n        \"\"\"Run an agent interactively in a conversational interface.\n\n        If no agent is specified, you'll be presented with a list of available agents to choose from.\n        Specifying an agent will run the agent from NEAR AI Cloud. If you want to run a specific local agent,\n        pass the path to the agent followed by the --local flag. (See examples below.)\n\n        Args:\n          agent (str) :\n            Path to the agent directory or agent ID (optional)\n          thread_id (str) :\n            Thread ID to continue an existing conversation\n          tool_resources (dict) :\n            Tool resources to pass to the agent (JSON format)\n          local (bool) :\n            Run the agent locally instead of in the cloud\n          verbose (bool) :\n            Show detailed debug information during execution\n          env_vars (dict) :\n            Environment variables to pass to the agent (JSON format)\n          stream (bool) :\n            Whether to stream the agent's output, only works with agents that stream completions\n\n\n        Examples:\n          # Select from a list of agents to run that you created or downloaded\n          nearai agent interactive\n\n          # Run an agent hosted on NEAR AI Cloud\n          nearai agent interactive example.near/agent-name/latest\n\n          # Run a specific agent locally\n          nearai agent interactive ./path/to/local/agent --local\n\n          # Run an agent locally with verbose output\n          nearai agent interactive ./path/to/local/agent --local --verbose\n\n          # Continue an existing conversation\n          nearai agent interactive example.near/agent-name/0.0.3 --thread-id abc123\n\n        \"\"\"\n        assert_user_auth()\n\n        if agent is None:\n            local = True\n            # List available agents in the registry folder\n            registry_path = Path(get_registry_folder())\n            if not registry_path.exists():\n                print(\"Error: Registry folder not found. Please create an agent first.\")\n                return\n\n            agents = []\n            # Walk through registry to find agents\n            for namespace in registry_path.iterdir():\n                if namespace.is_dir():\n                    for agent_name in namespace.iterdir():\n                        if agent_name.is_dir():\n                            for version in agent_name.iterdir():\n                                if version.is_dir():\n                                    agents.append(version)\n\n            if not agents:\n                print(\"No agents found. Please create an agent first with 'nearai agent create'\")\n                return\n\n            # Sort agents by namespace then name\n            agents = sorted(agents, key=lambda x: (x.parts[-3], x.parts[-2]))\n            display_agents_in_columns(agents)\n\n            while True:\n                try:\n                    choice = int(Prompt.ask(\"[blue bold]Select an agent (enter number)\")) - 1\n                    if 0 &lt;= choice &lt; len(agents):\n                        agent = str(agents[choice])\n                        break\n                    print(\"Invalid selection. Please try again.\")\n                except ValueError:\n                    print(\"Please enter a valid number.\")\n                except KeyboardInterrupt:\n                    print(\"\\nOperation cancelled.\")\n                    return\n\n        # Convert agent path to Path object if it's a string\n        agent_path = Path(agent)\n        if local:\n            agent_path = resolve_local_path(agent_path)\n        else:\n            try:\n                parse_location(str(agent_path))\n            except Exception:\n                print(\n                    f'Registry entry format is &lt;namespace&gt;/&lt;name&gt;/&lt;version&gt;, but \"{agent_path}\" was provided. Did you mean to run with a flag --local?'  # noqa: E501\n                )\n                exit(1)\n\n        agent_id = get_agent_id(agent_path, local)\n\n        if local:\n            if stream:\n                print(\"NOTE: streaming in interactive --local is not implemented.\")\n            env_run = self._start_local_session(\n                agent=agent_id,\n                thread_id=thread_id,\n                tool_resources=tool_resources,\n                local_path=agent_path,\n                verbose=verbose,\n                env_vars=env_vars,\n            )\n\n        last_message_id = None\n        print(f\"\\n=== Starting interactive session with agent: {agent_id} ===\")\n        print(\"\")\n        print(\"Type 'exit' to end the session\")\n        print(\"Type 'multiline' to enter multiline mode\")\n        print(\"\")\n\n        metadata = get_metadata(agent_path, local)\n        title = metadata.get(\"details\", {}).get(\"agent\", {}).get(\"welcome\", {}).get(\"title\")\n        if title:\n            print(title)\n        description = metadata.get(\"details\", {}).get(\"agent\", {}).get(\"welcome\", {}).get(\"description\")\n        if description:\n            print(description)\n\n        multiline = False\n\n        def print_multiline_prompt():\n            print(\"On Linux/macOS: To submit, press Ctrl+D at the beginning of a new line after your prompt\")\n            print(\"On Windows: Press Ctrl+Z followed by Enter\")\n\n        while True:\n            first_line = input(\"&gt; \")\n            if first_line.lower() == \"exit\":\n                break\n            if not multiline and first_line.lower() == \"multiline\":\n                multiline = True\n                print_multiline_prompt()\n                continue\n            lines = [first_line]\n\n            # NOTE: the code below tries to catch copy-paste by calling has_pending_input().\n            # This is OS-specific functionality and has been tested on Unix/Linux/Mac:\n            # 1. Works well with blocks of text of 3 lines and more.\n            # 2. Alas, does not trigger with text of 2 lines or less.\n            pending_input_on_this_line = has_pending_input()\n            if multiline or pending_input_on_this_line:\n                try:\n                    pending_input_on_prev_line = pending_input_on_this_line\n                    while True:\n                        pending_input_on_this_line = has_pending_input()\n                        if pending_input_on_prev_line or pending_input_on_this_line:\n                            line = input(\"\")\n                        else:\n                            if not multiline:\n                                multiline = True\n                                print_multiline_prompt()\n                            line = input(\"&gt; \")\n                        lines.append(line)\n                        pending_input_on_prev_line = pending_input_on_this_line\n                except EOFError:\n                    print(\"\")\n\n            new_message = \"\\n\".join(lines)\n\n            if local:\n                last_message_id = self._run_local_session(\n                    task=new_message, env_run=env_run, last_message_id=last_message_id\n                )\n            else:\n                last_message_id = self._task(\n                    agent=agent_id,\n                    task=new_message,\n                    thread_id=thread_id,\n                    tool_resources=tool_resources,\n                    last_message_id=last_message_id,\n                    local_path=agent_path if local else None,\n                    verbose=verbose,\n                    env_vars=env_vars,\n                    streaming=stream,\n                )\n\n            # Update thread_id for the next iteration\n            if thread_id is None:\n                thread_id = self.last_thread_id\n\n    def task(\n        self,\n        agent: str,\n        task: str,\n        thread_id: Optional[str] = None,\n        tool_resources: Optional[Dict[str, Any]] = None,\n        file_ids: Optional[List[str]] = None,\n        local: bool = False,\n        verbose: bool = False,\n        env_vars: Optional[Dict[str, Any]] = None,\n        stream: bool = False,\n    ) -&gt; None:\n        \"\"\"Run a single non-interactive task with an agent.\n\n        The agent will process the task and return its response.\n        This is useful for automation or when you don't need an ongoing conversation.\n\n        Args:\n          agent (str) :\n            Path to the agent directory or agent ID (required)\n          task (str) :\n            The task or question to send to the agent (required)\n          thread_id (str) :\n            Thread ID to continue an existing conversation\n          tool_resources (dict) :\n            Tool resources to pass to the agent (JSON format)\n          file_ids (list) :\n            File IDs to attach to the message\n          local (bool) :\n            Run the agent locally instead of in the cloud\n          verbose (bool) :\n            Show detailed debug information during execution\n          env_vars (dict) :\n            Environment variables to pass to the agent (JSON format)\n          stream (bool) :\n            Stream the agent's output, only works with agents that stream completions\n\n        Examples:\n          # Send a simple task to an agent\n          nearai agent task --agent example.near/agent-name/0.0.3 --task \"Summarize this article: https://example.com/article\"\n\n          # Run a local agent with environment variables\n          nearai agent task --agent path/to/agent --task \"Generate a report\" --local --env-vars '{\"API_KEY\": \"secret\"}'\n\n          # Continue a conversation in an existing thread\n          nearai agent task --agent example.near/agent-name/0.0.3 --task \"Continue the analysis\" --thread-id abc123\n\n        \"\"\"\n        last_message_id = self._task(\n            agent=agent,\n            task=task,\n            thread_id=thread_id,\n            tool_resources=tool_resources,\n            file_ids=file_ids,\n            local_path=resolve_local_path(Path(agent)) if local else None,\n            verbose=verbose,\n            env_vars=env_vars,\n            streaming=stream,\n        )\n        if last_message_id:\n            print(f\"Task completed. Thread ID: {self.last_thread_id}\")\n            print(f\"Last message ID: {last_message_id}\")\n\n    def _start_local_session(\n        self,\n        agent: str,\n        thread_id: Optional[str] = None,\n        tool_resources: Optional[Dict[str, Any]] = None,\n        local_path: Optional[Path] = None,\n        verbose: bool = False,\n        env_vars: Optional[Dict[str, Any]] = None,\n    ) -&gt; EnvironmentRun:\n        \"\"\"Starts local session, no messages yet from user.\"\"\"\n        assert_user_auth()\n\n        hub_client = get_hub_client()\n        if thread_id:\n            thread = hub_client.beta.threads.retrieve(thread_id)\n        else:\n            thread = hub_client.beta.threads.create(\n                tool_resources=tool_resources,\n            )\n        thread_id = thread.id\n        print(f\"thread_id = {thread_id}\")\n\n        run = hub_client.beta.threads.runs.create(\n            thread_id=thread.id,\n            assistant_id=agent,\n            extra_body={\"delegate_execution\": True},\n        )\n\n        params = {\n            \"api_url\": CONFIG.api_url,\n            \"tool_resources\": run.tools,\n            \"data_source\": \"local_files\",\n            \"user_env_vars\": env_vars,\n            \"agent_env_vars\": {},\n            \"verbose\": verbose,\n        }\n        auth = CONFIG.auth\n        assert auth is not None\n        return start_with_environment(str(local_path), auth, thread_id, run.id, str(local_path), params)\n\n    def _run_local_session(\n        self,\n        task: str,\n        env_run: EnvironmentRun,\n        last_message_id: Optional[str] = None,\n    ) -&gt; Optional[str]:\n        env_run.run(task)\n\n        # List new messages\n        hub_client = get_hub_client()\n        messages = hub_client.beta.threads.messages.list(\n            thread_id=env_run.thread_id, after=last_message_id, order=\"asc\"\n        )\n        message_list = list(messages)\n        if message_list:\n            for msg in message_list:\n                if msg.metadata and msg.metadata.get(\"message_type\"):\n                    continue\n                if msg.role == \"assistant\":\n                    print(f\"Assistant: {msg.content[0].text.value}\")\n            last_message_id = message_list[-1].id\n        else:\n            print(\"No new messages\")\n\n        # Store the thread_id for potential use in interactive mode\n        self.last_thread_id = env_run.thread_id\n\n        return last_message_id\n\n    def _task(\n        self,\n        agent: str,\n        task: str,\n        thread_id: Optional[str] = None,\n        tool_resources: Optional[Dict[str, Any]] = None,\n        file_ids: Optional[List[str]] = None,\n        last_message_id: Optional[str] = None,\n        local_path: Optional[Path] = None,\n        verbose: bool = False,\n        env_vars: Optional[Dict[str, Any]] = None,\n        streaming: bool = True,\n    ) -&gt; Optional[str]:\n        \"\"\"Runs agent non-interactively with a single task.\"\"\"\n        assert_user_auth()\n\n        hub_client = get_hub_client()\n        if thread_id:\n            thread = hub_client.beta.threads.retrieve(thread_id)\n        else:\n            thread = hub_client.beta.threads.create(\n                tool_resources=tool_resources,\n            )\n\n        hub_client.beta.threads.messages.create(\n            thread_id=thread.id,\n            role=\"user\",\n            content=task,\n            attachments=[Attachment(file_id=file_id) for file_id in file_ids] if file_ids else None,\n        )\n\n        if not local_path:\n            hub_client.beta.threads.runs.create_and_poll(\n                thread_id=thread.id,\n                assistant_id=agent,\n            )\n        elif streaming:\n            run = hub_client.beta.threads.runs.create(\n                thread_id=thread.id,\n                assistant_id=agent,\n                stream=True,\n                extra_body={\"delegate_execution\": True},\n            )\n            params: dict = {\n                \"api_url\": CONFIG.api_url,\n                \"tool_resources\": [],  # run.tools, TODO this is not returned from the streaming run\n                \"data_source\": \"local_files\",\n                \"user_env_vars\": env_vars,\n                \"agent_env_vars\": {},\n                \"verbose\": verbose,\n            }\n            auth = CONFIG.auth\n            assert auth is not None\n            run_id = None\n            for event in run:\n                run_id = event.data.id\n                break\n\n            def run_async_loop():\n                loop = asyncio.new_event_loop()\n                asyncio.set_event_loop(loop)\n                try:\n                    loop.run_until_complete(self._print_stream_async(run))\n                finally:\n                    loop.close()\n\n            streaming_thread = threading.Thread(target=run_async_loop)\n            streaming_thread.start()\n\n            LocalRunner(str(local_path), agent, thread.id, run_id, auth, params)\n            streaming_thread.join()\n\n        else:\n            run = hub_client.beta.threads.runs.create(\n                thread_id=thread.id,\n                assistant_id=agent,\n                extra_body={\"delegate_execution\": True},\n            )\n            params = {\n                \"api_url\": CONFIG.api_url,\n                \"tool_resources\": run.tools,\n                \"data_source\": \"local_files\",\n                \"user_env_vars\": env_vars,\n                \"agent_env_vars\": {},\n                \"verbose\": verbose,\n            }\n            auth = CONFIG.auth\n            assert auth is not None\n            LocalRunner(str(local_path), agent, thread.id, run.id, auth, params)\n\n            # List new messages\n            messages = hub_client.beta.threads.messages.list(thread_id=thread.id, after=last_message_id, order=\"asc\")\n            message_list = list(messages)\n            if message_list:\n                for msg in message_list:\n                    if msg.metadata and msg.metadata.get(\"message_type\"):\n                        continue\n                    if msg.role == \"assistant\":\n                        print(f\"Assistant: {msg.content[0].text.value}\")\n                last_message_id = message_list[-1].id\n            else:\n                print(\"No new messages\")\n\n        # Store the thread_id for potential use in interactive mode\n        self.last_thread_id = thread.id\n\n        return last_message_id\n\n    async def _print_stream_async(self, run):\n        \"\"\"Asynchronously print the stream of messages from the run.\n\n        :param run: The stream to iterate over\n        :return:\n        \"\"\"\n        try:\n            for event in run:\n                if event and hasattr(event, \"event\") and event.event == \"thread.message.delta\":\n                    if hasattr(event.data, \"delta\") and hasattr(event.data.delta, \"content\"):\n                        for content in event.data.delta.content:\n                            value = content.text.value\n                            if value:\n                                print(content.text.value, end=\"\")\n                else:\n                    if event and hasattr(event, \"event\"):\n                        if event.event == \"thread.message.completed\":\n                            pass\n                        elif event.event == \"thread.message.error\":\n                            print(f\"Error: {event.data.error}\")\n                        elif event.event in [\n                            \"thread.run.completed\",\n                            \"thread.run.error\",\n                            \"thread.run.canceled\",\n                            \"thread.run.expired\",\n                            \"thread.run.requires_action\",\n                        ]:\n                            print(\"\")\n                            break\n                    await asyncio.sleep(0.01)\n        except Exception as e:\n            print(f\"Error in print_stream_async: {e}\")\n\n    def create(self, name: Optional[str] = None, description: Optional[str] = None) -&gt; None:\n        \"\"\"Create a new AI agent from scratch.\n\n        Args:\n          name (str) :\n            Name for the new agent (optional).\n          description (str) :\n            Description of the new agent (optional).\n\n        Examples:\n          # Create a new agent step-by-step with prompts\n          nearai agent create\n\n          # Create with specific name and description\n          nearai agent create --name my_agent --description \"My new agent\"\n\n        Documentation:\n          https://docs.near.ai/agents/quickstart\n\n        \"\"\"\n        # Check if the user is authenticated\n        if CONFIG.auth is None or CONFIG.auth.namespace is None:\n            print(\"Please login with `nearai login` before creating an agent.\")\n            return\n\n        namespace = CONFIG.auth.namespace\n\n        # Import the agent creator functions\n        from nearai.agent_creator import create_new_agent\n\n        # Create a new agent from scratch\n        create_new_agent(namespace, name, description)\n\n    def upload(\n        self,\n        local_path: str = \".\",\n        bump: bool = False,\n        minor_bump: bool = False,\n        major_bump: bool = False,\n        encrypt: bool = False,\n    ) -&gt; Optional[EntryLocation]:\n        \"\"\"Alias for 'nearai registry upload'.\"\"\"\n        assert_user_auth()\n        # Create an instance of RegistryCli and call its upload method\n        registry_cli = RegistryCli()\n        return registry_cli.upload(local_path, bump, minor_bump, major_bump, encrypt)\n\n    def __call__(self) -&gt; None:\n        \"\"\"Show help when 'nearai agent' is called without subcommands.\"\"\"\n        custom_args = [\"nearai\", \"agent\", \"--help\"]\n        handle_help_request(custom_args)\n</code></pre>"},{"location":"api/#nearai.cli.AgentCli.__call__","title":"__call__","text":"<pre><code>__call__() -&gt; None\n</code></pre> <p>Show help when 'nearai agent' is called without subcommands.</p> Source code in <code>nearai/cli.py</code> <pre><code>def __call__(self) -&gt; None:\n    \"\"\"Show help when 'nearai agent' is called without subcommands.\"\"\"\n    custom_args = [\"nearai\", \"agent\", \"--help\"]\n    handle_help_request(custom_args)\n</code></pre>"},{"location":"api/#nearai.cli.AgentCli._print_stream_async","title":"_print_stream_async  <code>async</code>","text":"<pre><code>_print_stream_async(run)\n</code></pre> <p>Asynchronously print the stream of messages from the run.</p> <p>:param run: The stream to iterate over :return:</p> Source code in <code>nearai/cli.py</code> <pre><code>async def _print_stream_async(self, run):\n    \"\"\"Asynchronously print the stream of messages from the run.\n\n    :param run: The stream to iterate over\n    :return:\n    \"\"\"\n    try:\n        for event in run:\n            if event and hasattr(event, \"event\") and event.event == \"thread.message.delta\":\n                if hasattr(event.data, \"delta\") and hasattr(event.data.delta, \"content\"):\n                    for content in event.data.delta.content:\n                        value = content.text.value\n                        if value:\n                            print(content.text.value, end=\"\")\n            else:\n                if event and hasattr(event, \"event\"):\n                    if event.event == \"thread.message.completed\":\n                        pass\n                    elif event.event == \"thread.message.error\":\n                        print(f\"Error: {event.data.error}\")\n                    elif event.event in [\n                        \"thread.run.completed\",\n                        \"thread.run.error\",\n                        \"thread.run.canceled\",\n                        \"thread.run.expired\",\n                        \"thread.run.requires_action\",\n                    ]:\n                        print(\"\")\n                        break\n                await asyncio.sleep(0.01)\n    except Exception as e:\n        print(f\"Error in print_stream_async: {e}\")\n</code></pre>"},{"location":"api/#nearai.cli.AgentCli._start_local_session","title":"_start_local_session","text":"<pre><code>_start_local_session(agent: str, thread_id: Optional[str] = None, tool_resources: Optional[Dict[str, Any]] = None, local_path: Optional[Path] = None, verbose: bool = False, env_vars: Optional[Dict[str, Any]] = None) -&gt; EnvironmentRun\n</code></pre> <p>Starts local session, no messages yet from user.</p> Source code in <code>nearai/cli.py</code> <pre><code>def _start_local_session(\n    self,\n    agent: str,\n    thread_id: Optional[str] = None,\n    tool_resources: Optional[Dict[str, Any]] = None,\n    local_path: Optional[Path] = None,\n    verbose: bool = False,\n    env_vars: Optional[Dict[str, Any]] = None,\n) -&gt; EnvironmentRun:\n    \"\"\"Starts local session, no messages yet from user.\"\"\"\n    assert_user_auth()\n\n    hub_client = get_hub_client()\n    if thread_id:\n        thread = hub_client.beta.threads.retrieve(thread_id)\n    else:\n        thread = hub_client.beta.threads.create(\n            tool_resources=tool_resources,\n        )\n    thread_id = thread.id\n    print(f\"thread_id = {thread_id}\")\n\n    run = hub_client.beta.threads.runs.create(\n        thread_id=thread.id,\n        assistant_id=agent,\n        extra_body={\"delegate_execution\": True},\n    )\n\n    params = {\n        \"api_url\": CONFIG.api_url,\n        \"tool_resources\": run.tools,\n        \"data_source\": \"local_files\",\n        \"user_env_vars\": env_vars,\n        \"agent_env_vars\": {},\n        \"verbose\": verbose,\n    }\n    auth = CONFIG.auth\n    assert auth is not None\n    return start_with_environment(str(local_path), auth, thread_id, run.id, str(local_path), params)\n</code></pre>"},{"location":"api/#nearai.cli.AgentCli._task","title":"_task","text":"<pre><code>_task(agent: str, task: str, thread_id: Optional[str] = None, tool_resources: Optional[Dict[str, Any]] = None, file_ids: Optional[List[str]] = None, last_message_id: Optional[str] = None, local_path: Optional[Path] = None, verbose: bool = False, env_vars: Optional[Dict[str, Any]] = None, streaming: bool = True) -&gt; Optional[str]\n</code></pre> <p>Runs agent non-interactively with a single task.</p> Source code in <code>nearai/cli.py</code> <pre><code>def _task(\n    self,\n    agent: str,\n    task: str,\n    thread_id: Optional[str] = None,\n    tool_resources: Optional[Dict[str, Any]] = None,\n    file_ids: Optional[List[str]] = None,\n    last_message_id: Optional[str] = None,\n    local_path: Optional[Path] = None,\n    verbose: bool = False,\n    env_vars: Optional[Dict[str, Any]] = None,\n    streaming: bool = True,\n) -&gt; Optional[str]:\n    \"\"\"Runs agent non-interactively with a single task.\"\"\"\n    assert_user_auth()\n\n    hub_client = get_hub_client()\n    if thread_id:\n        thread = hub_client.beta.threads.retrieve(thread_id)\n    else:\n        thread = hub_client.beta.threads.create(\n            tool_resources=tool_resources,\n        )\n\n    hub_client.beta.threads.messages.create(\n        thread_id=thread.id,\n        role=\"user\",\n        content=task,\n        attachments=[Attachment(file_id=file_id) for file_id in file_ids] if file_ids else None,\n    )\n\n    if not local_path:\n        hub_client.beta.threads.runs.create_and_poll(\n            thread_id=thread.id,\n            assistant_id=agent,\n        )\n    elif streaming:\n        run = hub_client.beta.threads.runs.create(\n            thread_id=thread.id,\n            assistant_id=agent,\n            stream=True,\n            extra_body={\"delegate_execution\": True},\n        )\n        params: dict = {\n            \"api_url\": CONFIG.api_url,\n            \"tool_resources\": [],  # run.tools, TODO this is not returned from the streaming run\n            \"data_source\": \"local_files\",\n            \"user_env_vars\": env_vars,\n            \"agent_env_vars\": {},\n            \"verbose\": verbose,\n        }\n        auth = CONFIG.auth\n        assert auth is not None\n        run_id = None\n        for event in run:\n            run_id = event.data.id\n            break\n\n        def run_async_loop():\n            loop = asyncio.new_event_loop()\n            asyncio.set_event_loop(loop)\n            try:\n                loop.run_until_complete(self._print_stream_async(run))\n            finally:\n                loop.close()\n\n        streaming_thread = threading.Thread(target=run_async_loop)\n        streaming_thread.start()\n\n        LocalRunner(str(local_path), agent, thread.id, run_id, auth, params)\n        streaming_thread.join()\n\n    else:\n        run = hub_client.beta.threads.runs.create(\n            thread_id=thread.id,\n            assistant_id=agent,\n            extra_body={\"delegate_execution\": True},\n        )\n        params = {\n            \"api_url\": CONFIG.api_url,\n            \"tool_resources\": run.tools,\n            \"data_source\": \"local_files\",\n            \"user_env_vars\": env_vars,\n            \"agent_env_vars\": {},\n            \"verbose\": verbose,\n        }\n        auth = CONFIG.auth\n        assert auth is not None\n        LocalRunner(str(local_path), agent, thread.id, run.id, auth, params)\n\n        # List new messages\n        messages = hub_client.beta.threads.messages.list(thread_id=thread.id, after=last_message_id, order=\"asc\")\n        message_list = list(messages)\n        if message_list:\n            for msg in message_list:\n                if msg.metadata and msg.metadata.get(\"message_type\"):\n                    continue\n                if msg.role == \"assistant\":\n                    print(f\"Assistant: {msg.content[0].text.value}\")\n            last_message_id = message_list[-1].id\n        else:\n            print(\"No new messages\")\n\n    # Store the thread_id for potential use in interactive mode\n    self.last_thread_id = thread.id\n\n    return last_message_id\n</code></pre>"},{"location":"api/#nearai.cli.AgentCli.create","title":"create","text":"<pre><code>create(name: Optional[str] = None, description: Optional[str] = None) -&gt; None\n</code></pre> <p>Create a new AI agent from scratch.</p> <p>Parameters:</p> Name Type Description Default <code>name (str) </code> <p>Name for the new agent (optional).</p> required <code>description (str) </code> <p>Description of the new agent (optional).</p> required <p>Examples:</p>"},{"location":"api/#nearai.cli.AgentCli.create--create-a-new-agent-step-by-step-with-prompts","title":"Create a new agent step-by-step with prompts","text":"<p>nearai agent create</p>"},{"location":"api/#nearai.cli.AgentCli.create--create-with-specific-name-and-description","title":"Create with specific name and description","text":"<p>nearai agent create --name my_agent --description \"My new agent\"</p> Documentation <p>https://docs.near.ai/agents/quickstart</p> Source code in <code>nearai/cli.py</code> <pre><code>def create(self, name: Optional[str] = None, description: Optional[str] = None) -&gt; None:\n    \"\"\"Create a new AI agent from scratch.\n\n    Args:\n      name (str) :\n        Name for the new agent (optional).\n      description (str) :\n        Description of the new agent (optional).\n\n    Examples:\n      # Create a new agent step-by-step with prompts\n      nearai agent create\n\n      # Create with specific name and description\n      nearai agent create --name my_agent --description \"My new agent\"\n\n    Documentation:\n      https://docs.near.ai/agents/quickstart\n\n    \"\"\"\n    # Check if the user is authenticated\n    if CONFIG.auth is None or CONFIG.auth.namespace is None:\n        print(\"Please login with `nearai login` before creating an agent.\")\n        return\n\n    namespace = CONFIG.auth.namespace\n\n    # Import the agent creator functions\n    from nearai.agent_creator import create_new_agent\n\n    # Create a new agent from scratch\n    create_new_agent(namespace, name, description)\n</code></pre>"},{"location":"api/#nearai.cli.AgentCli.dev","title":"dev","text":"<pre><code>dev() -&gt; int\n</code></pre> <p>Run a local development UI for agents that have their own UI.</p> <p>This launches a local server for testing and developing agent functionality in a browser-based environment.</p> <p>Examples:</p>"},{"location":"api/#nearai.cli.AgentCli.dev--start-the-local-development-server","title":"Start the local development server","text":"<p>nearai agent dev</p> Source code in <code>nearai/cli.py</code> <pre><code>def dev(self) -&gt; int:\n    \"\"\"Run a local development UI for agents that have their own UI.\n\n    This launches a local server for testing and developing agent functionality in a browser-based environment.\n\n    Examples:\n      # Start the local development server\n      nearai agent dev\n\n    \"\"\"\n    if not os.path.exists(\"hub/demo/.env\"):\n        shutil.copy(\"hub/demo/.env.example\", \"hub/demo/.env\")\n\n    ret_val = os.system(\"npm install --prefix hub/demo\")\n    if ret_val != 0:\n        print(\"Node.js is required to run the development server.\")\n        print(\"Please install Node.js from https://nodejs.org/\")\n    ret_val = os.system(\"npm run dev --prefix hub/demo\")\n    return ret_val\n</code></pre>"},{"location":"api/#nearai.cli.AgentCli.inspect","title":"inspect","text":"<pre><code>inspect(path: str) -&gt; None\n</code></pre> <p>Inspect the environment and contents of an agent at the specified path.</p> <p>This launches a Streamlit interface showing the agent's structure, code, and metadata.</p> <p>Parameters:</p> Name Type Description Default <code>path </code> <p>str Path to the agent directory to inspect (required)</p> required <p>Examples:</p>"},{"location":"api/#nearai.cli.AgentCli.inspect--inspect-a-local-agent","title":"Inspect a local agent","text":"<p>nearai agent inspect ./path/to/agent</p>"},{"location":"api/#nearai.cli.AgentCli.inspect--inspect-a-downloaded-registry-agent","title":"Inspect a downloaded registry agent","text":"<p>nearai agent inspect .near-registry/your-namespace/agent-name/0.1.0</p> Source code in <code>nearai/cli.py</code> <pre><code>def inspect(self, path: str) -&gt; None:\n    \"\"\"Inspect the environment and contents of an agent at the specified path.\n\n    This launches a Streamlit interface showing the agent's structure, code, and metadata.\n\n    Args:\n      path : str\n        Path to the agent directory to inspect (required)\n\n    Examples:\n      # Inspect a local agent\n      nearai agent inspect ./path/to/agent\n\n      # Inspect a downloaded registry agent\n      nearai agent inspect .near-registry/your-namespace/agent-name/0.1.0\n\n    \"\"\"\n    import subprocess\n\n    filename = Path(os.path.abspath(__file__)).parent / \"streamlit_inspect.py\"\n    subprocess.call([\"streamlit\", \"run\", filename, \"--\", path])\n</code></pre>"},{"location":"api/#nearai.cli.AgentCli.interactive","title":"interactive","text":"<pre><code>interactive(agent: Optional[str] = None, thread_id: Optional[str] = None, tool_resources: Optional[Dict[str, Any]] = None, local: bool = False, verbose: bool = False, env_vars: Optional[Dict[str, Any]] = None, stream: bool = False) -&gt; None\n</code></pre> <p>Run an agent interactively in a conversational interface.</p> <p>If no agent is specified, you'll be presented with a list of available agents to choose from. Specifying an agent will run the agent from NEAR AI Cloud. If you want to run a specific local agent, pass the path to the agent followed by the --local flag. (See examples below.)</p> <p>Parameters:</p> Name Type Description Default <code>agent (str) </code> <p>Path to the agent directory or agent ID (optional)</p> required <code>thread_id (str) </code> <p>Thread ID to continue an existing conversation</p> required <code>tool_resources (dict) </code> <p>Tool resources to pass to the agent (JSON format)</p> required <code>local (bool) </code> <p>Run the agent locally instead of in the cloud</p> required <code>verbose (bool) </code> <p>Show detailed debug information during execution</p> required <code>env_vars (dict) </code> <p>Environment variables to pass to the agent (JSON format)</p> required <code>stream (bool) </code> <p>Whether to stream the agent's output, only works with agents that stream completions</p> required <p>Examples:</p>"},{"location":"api/#nearai.cli.AgentCli.interactive--select-from-a-list-of-agents-to-run-that-you-created-or-downloaded","title":"Select from a list of agents to run that you created or downloaded","text":"<p>nearai agent interactive</p>"},{"location":"api/#nearai.cli.AgentCli.interactive--run-an-agent-hosted-on-near-ai-cloud","title":"Run an agent hosted on NEAR AI Cloud","text":"<p>nearai agent interactive example.near/agent-name/latest</p>"},{"location":"api/#nearai.cli.AgentCli.interactive--run-a-specific-agent-locally","title":"Run a specific agent locally","text":"<p>nearai agent interactive ./path/to/local/agent --local</p>"},{"location":"api/#nearai.cli.AgentCli.interactive--run-an-agent-locally-with-verbose-output","title":"Run an agent locally with verbose output","text":"<p>nearai agent interactive ./path/to/local/agent --local --verbose</p>"},{"location":"api/#nearai.cli.AgentCli.interactive--continue-an-existing-conversation","title":"Continue an existing conversation","text":"<p>nearai agent interactive example.near/agent-name/0.0.3 --thread-id abc123</p> Source code in <code>nearai/cli.py</code> <pre><code>def interactive(\n    self,\n    agent: Optional[str] = None,\n    thread_id: Optional[str] = None,\n    tool_resources: Optional[Dict[str, Any]] = None,\n    local: bool = False,\n    verbose: bool = False,\n    env_vars: Optional[Dict[str, Any]] = None,\n    stream: bool = False,\n) -&gt; None:\n    \"\"\"Run an agent interactively in a conversational interface.\n\n    If no agent is specified, you'll be presented with a list of available agents to choose from.\n    Specifying an agent will run the agent from NEAR AI Cloud. If you want to run a specific local agent,\n    pass the path to the agent followed by the --local flag. (See examples below.)\n\n    Args:\n      agent (str) :\n        Path to the agent directory or agent ID (optional)\n      thread_id (str) :\n        Thread ID to continue an existing conversation\n      tool_resources (dict) :\n        Tool resources to pass to the agent (JSON format)\n      local (bool) :\n        Run the agent locally instead of in the cloud\n      verbose (bool) :\n        Show detailed debug information during execution\n      env_vars (dict) :\n        Environment variables to pass to the agent (JSON format)\n      stream (bool) :\n        Whether to stream the agent's output, only works with agents that stream completions\n\n\n    Examples:\n      # Select from a list of agents to run that you created or downloaded\n      nearai agent interactive\n\n      # Run an agent hosted on NEAR AI Cloud\n      nearai agent interactive example.near/agent-name/latest\n\n      # Run a specific agent locally\n      nearai agent interactive ./path/to/local/agent --local\n\n      # Run an agent locally with verbose output\n      nearai agent interactive ./path/to/local/agent --local --verbose\n\n      # Continue an existing conversation\n      nearai agent interactive example.near/agent-name/0.0.3 --thread-id abc123\n\n    \"\"\"\n    assert_user_auth()\n\n    if agent is None:\n        local = True\n        # List available agents in the registry folder\n        registry_path = Path(get_registry_folder())\n        if not registry_path.exists():\n            print(\"Error: Registry folder not found. Please create an agent first.\")\n            return\n\n        agents = []\n        # Walk through registry to find agents\n        for namespace in registry_path.iterdir():\n            if namespace.is_dir():\n                for agent_name in namespace.iterdir():\n                    if agent_name.is_dir():\n                        for version in agent_name.iterdir():\n                            if version.is_dir():\n                                agents.append(version)\n\n        if not agents:\n            print(\"No agents found. Please create an agent first with 'nearai agent create'\")\n            return\n\n        # Sort agents by namespace then name\n        agents = sorted(agents, key=lambda x: (x.parts[-3], x.parts[-2]))\n        display_agents_in_columns(agents)\n\n        while True:\n            try:\n                choice = int(Prompt.ask(\"[blue bold]Select an agent (enter number)\")) - 1\n                if 0 &lt;= choice &lt; len(agents):\n                    agent = str(agents[choice])\n                    break\n                print(\"Invalid selection. Please try again.\")\n            except ValueError:\n                print(\"Please enter a valid number.\")\n            except KeyboardInterrupt:\n                print(\"\\nOperation cancelled.\")\n                return\n\n    # Convert agent path to Path object if it's a string\n    agent_path = Path(agent)\n    if local:\n        agent_path = resolve_local_path(agent_path)\n    else:\n        try:\n            parse_location(str(agent_path))\n        except Exception:\n            print(\n                f'Registry entry format is &lt;namespace&gt;/&lt;name&gt;/&lt;version&gt;, but \"{agent_path}\" was provided. Did you mean to run with a flag --local?'  # noqa: E501\n            )\n            exit(1)\n\n    agent_id = get_agent_id(agent_path, local)\n\n    if local:\n        if stream:\n            print(\"NOTE: streaming in interactive --local is not implemented.\")\n        env_run = self._start_local_session(\n            agent=agent_id,\n            thread_id=thread_id,\n            tool_resources=tool_resources,\n            local_path=agent_path,\n            verbose=verbose,\n            env_vars=env_vars,\n        )\n\n    last_message_id = None\n    print(f\"\\n=== Starting interactive session with agent: {agent_id} ===\")\n    print(\"\")\n    print(\"Type 'exit' to end the session\")\n    print(\"Type 'multiline' to enter multiline mode\")\n    print(\"\")\n\n    metadata = get_metadata(agent_path, local)\n    title = metadata.get(\"details\", {}).get(\"agent\", {}).get(\"welcome\", {}).get(\"title\")\n    if title:\n        print(title)\n    description = metadata.get(\"details\", {}).get(\"agent\", {}).get(\"welcome\", {}).get(\"description\")\n    if description:\n        print(description)\n\n    multiline = False\n\n    def print_multiline_prompt():\n        print(\"On Linux/macOS: To submit, press Ctrl+D at the beginning of a new line after your prompt\")\n        print(\"On Windows: Press Ctrl+Z followed by Enter\")\n\n    while True:\n        first_line = input(\"&gt; \")\n        if first_line.lower() == \"exit\":\n            break\n        if not multiline and first_line.lower() == \"multiline\":\n            multiline = True\n            print_multiline_prompt()\n            continue\n        lines = [first_line]\n\n        # NOTE: the code below tries to catch copy-paste by calling has_pending_input().\n        # This is OS-specific functionality and has been tested on Unix/Linux/Mac:\n        # 1. Works well with blocks of text of 3 lines and more.\n        # 2. Alas, does not trigger with text of 2 lines or less.\n        pending_input_on_this_line = has_pending_input()\n        if multiline or pending_input_on_this_line:\n            try:\n                pending_input_on_prev_line = pending_input_on_this_line\n                while True:\n                    pending_input_on_this_line = has_pending_input()\n                    if pending_input_on_prev_line or pending_input_on_this_line:\n                        line = input(\"\")\n                    else:\n                        if not multiline:\n                            multiline = True\n                            print_multiline_prompt()\n                        line = input(\"&gt; \")\n                    lines.append(line)\n                    pending_input_on_prev_line = pending_input_on_this_line\n            except EOFError:\n                print(\"\")\n\n        new_message = \"\\n\".join(lines)\n\n        if local:\n            last_message_id = self._run_local_session(\n                task=new_message, env_run=env_run, last_message_id=last_message_id\n            )\n        else:\n            last_message_id = self._task(\n                agent=agent_id,\n                task=new_message,\n                thread_id=thread_id,\n                tool_resources=tool_resources,\n                last_message_id=last_message_id,\n                local_path=agent_path if local else None,\n                verbose=verbose,\n                env_vars=env_vars,\n                streaming=stream,\n            )\n\n        # Update thread_id for the next iteration\n        if thread_id is None:\n            thread_id = self.last_thread_id\n</code></pre>"},{"location":"api/#nearai.cli.AgentCli.task","title":"task","text":"<pre><code>task(agent: str, task: str, thread_id: Optional[str] = None, tool_resources: Optional[Dict[str, Any]] = None, file_ids: Optional[List[str]] = None, local: bool = False, verbose: bool = False, env_vars: Optional[Dict[str, Any]] = None, stream: bool = False) -&gt; None\n</code></pre> <p>Run a single non-interactive task with an agent.</p> <p>The agent will process the task and return its response. This is useful for automation or when you don't need an ongoing conversation.</p> <p>Parameters:</p> Name Type Description Default <code>agent (str) </code> <p>Path to the agent directory or agent ID (required)</p> required <code>task (str) </code> <p>The task or question to send to the agent (required)</p> required <code>thread_id (str) </code> <p>Thread ID to continue an existing conversation</p> required <code>tool_resources (dict) </code> <p>Tool resources to pass to the agent (JSON format)</p> required <code>file_ids (list) </code> <p>File IDs to attach to the message</p> required <code>local (bool) </code> <p>Run the agent locally instead of in the cloud</p> required <code>verbose (bool) </code> <p>Show detailed debug information during execution</p> required <code>env_vars (dict) </code> <p>Environment variables to pass to the agent (JSON format)</p> required <code>stream (bool) </code> <p>Stream the agent's output, only works with agents that stream completions</p> required <p>Examples:</p>"},{"location":"api/#nearai.cli.AgentCli.task--send-a-simple-task-to-an-agent","title":"Send a simple task to an agent","text":"<p>nearai agent task --agent example.near/agent-name/0.0.3 --task \"Summarize this article: https://example.com/article\"</p>"},{"location":"api/#nearai.cli.AgentCli.task--run-a-local-agent-with-environment-variables","title":"Run a local agent with environment variables","text":"<p>nearai agent task --agent path/to/agent --task \"Generate a report\" --local --env-vars '{\"API_KEY\": \"secret\"}'</p>"},{"location":"api/#nearai.cli.AgentCli.task--continue-a-conversation-in-an-existing-thread","title":"Continue a conversation in an existing thread","text":"<p>nearai agent task --agent example.near/agent-name/0.0.3 --task \"Continue the analysis\" --thread-id abc123</p> Source code in <code>nearai/cli.py</code> <pre><code>def task(\n    self,\n    agent: str,\n    task: str,\n    thread_id: Optional[str] = None,\n    tool_resources: Optional[Dict[str, Any]] = None,\n    file_ids: Optional[List[str]] = None,\n    local: bool = False,\n    verbose: bool = False,\n    env_vars: Optional[Dict[str, Any]] = None,\n    stream: bool = False,\n) -&gt; None:\n    \"\"\"Run a single non-interactive task with an agent.\n\n    The agent will process the task and return its response.\n    This is useful for automation or when you don't need an ongoing conversation.\n\n    Args:\n      agent (str) :\n        Path to the agent directory or agent ID (required)\n      task (str) :\n        The task or question to send to the agent (required)\n      thread_id (str) :\n        Thread ID to continue an existing conversation\n      tool_resources (dict) :\n        Tool resources to pass to the agent (JSON format)\n      file_ids (list) :\n        File IDs to attach to the message\n      local (bool) :\n        Run the agent locally instead of in the cloud\n      verbose (bool) :\n        Show detailed debug information during execution\n      env_vars (dict) :\n        Environment variables to pass to the agent (JSON format)\n      stream (bool) :\n        Stream the agent's output, only works with agents that stream completions\n\n    Examples:\n      # Send a simple task to an agent\n      nearai agent task --agent example.near/agent-name/0.0.3 --task \"Summarize this article: https://example.com/article\"\n\n      # Run a local agent with environment variables\n      nearai agent task --agent path/to/agent --task \"Generate a report\" --local --env-vars '{\"API_KEY\": \"secret\"}'\n\n      # Continue a conversation in an existing thread\n      nearai agent task --agent example.near/agent-name/0.0.3 --task \"Continue the analysis\" --thread-id abc123\n\n    \"\"\"\n    last_message_id = self._task(\n        agent=agent,\n        task=task,\n        thread_id=thread_id,\n        tool_resources=tool_resources,\n        file_ids=file_ids,\n        local_path=resolve_local_path(Path(agent)) if local else None,\n        verbose=verbose,\n        env_vars=env_vars,\n        streaming=stream,\n    )\n    if last_message_id:\n        print(f\"Task completed. Thread ID: {self.last_thread_id}\")\n        print(f\"Last message ID: {last_message_id}\")\n</code></pre>"},{"location":"api/#nearai.cli.AgentCli.upload","title":"upload","text":"<pre><code>upload(local_path: str = '.', bump: bool = False, minor_bump: bool = False, major_bump: bool = False, encrypt: bool = False) -&gt; Optional[EntryLocation]\n</code></pre> <p>Alias for 'nearai registry upload'.</p> Source code in <code>nearai/cli.py</code> <pre><code>def upload(\n    self,\n    local_path: str = \".\",\n    bump: bool = False,\n    minor_bump: bool = False,\n    major_bump: bool = False,\n    encrypt: bool = False,\n) -&gt; Optional[EntryLocation]:\n    \"\"\"Alias for 'nearai registry upload'.\"\"\"\n    assert_user_auth()\n    # Create an instance of RegistryCli and call its upload method\n    registry_cli = RegistryCli()\n    return registry_cli.upload(local_path, bump, minor_bump, major_bump, encrypt)\n</code></pre>"},{"location":"api/#nearai.cli.BenchmarkCli","title":"BenchmarkCli","text":"<p>Commands for running and listing benchmarks on datasets with solver strategies.</p> Commands <p>nearai benchmark run : Run benchmark on a dataset with a solver strategy   (dataset*, solver_strategy*, --max-concurrent, --force, --subset,   --check-compatibility, --record, --num-inference-retries) nearai benchmark list : List all executed benchmarks   (--namespace, --benchmark, --solver, --args, --total, --offset)</p> Options <p>dataset* (str) :   Dataset to benchmark on solver_strategy* (str) :   Solver strategy to use --max-concurrent (int) :   Number of concurrent tasks to run --force (bool) :   Force re-run even if cached results exist --subset (str) :   Subset of the dataset to run on --check-compatibility (bool) :   Check if solver is compatible with dataset --record (bool) :   Record the benchmark results --num-inference-retries (int) :   Number of retries for inference --namespace (str) :   Filter benchmarks by namespace --benchmark (str) :   Filter benchmarks by benchmark name --solver (str) :   Filter benchmarks by solver name --args (str) :   Filter benchmarks by solver arguments --total (int) :   Total number of results to show --offset (int) :   Offset for pagination</p> <p>Examples:</p>"},{"location":"api/#nearai.cli.BenchmarkCli--run-a-benchmark-on-a-dataset-with-a-solver-strategy","title":"Run a benchmark on a dataset with a solver strategy","text":"<p>nearai benchmark run my-dataset my-solver-strategy --max-concurrent 4 --force</p>"},{"location":"api/#nearai.cli.BenchmarkCli--list-benchmark-results-filtered-by-namespace","title":"List benchmark results filtered by namespace","text":"<p>nearai benchmark list --namespace my-namespace --benchmark my-benchmark --total 50</p> Source code in <code>nearai/cli.py</code> <pre><code>class BenchmarkCli:\n    \"\"\"Commands for running and listing benchmarks on datasets with solver strategies.\n\n    Commands:\n      nearai benchmark run : Run benchmark on a dataset with a solver strategy\n        (dataset*, solver_strategy*, --max-concurrent, --force, --subset,\n        --check-compatibility, --record, --num-inference-retries)\n      nearai benchmark list : List all executed benchmarks\n        (--namespace, --benchmark, --solver, --args, --total, --offset)\n\n    Options:\n      dataset* (str) :\n        Dataset to benchmark on\n      solver_strategy* (str) :\n        Solver strategy to use\n      --max-concurrent (int) :\n        Number of concurrent tasks to run\n      --force (bool) :\n        Force re-run even if cached results exist\n      --subset (str) :\n        Subset of the dataset to run on\n      --check-compatibility (bool) :\n        Check if solver is compatible with dataset\n      --record (bool) :\n        Record the benchmark results\n      --num-inference-retries (int) :\n        Number of retries for inference\n      --namespace (str) :\n        Filter benchmarks by namespace\n      --benchmark (str) :\n        Filter benchmarks by benchmark name\n      --solver (str) :\n        Filter benchmarks by solver name\n      --args (str) :\n        Filter benchmarks by solver arguments\n      --total (int) :\n        Total number of results to show\n      --offset (int) :\n        Offset for pagination\n\n    Examples:\n      # Run a benchmark on a dataset with a solver strategy\n      nearai benchmark run my-dataset my-solver-strategy --max-concurrent 4 --force\n\n      # List benchmark results filtered by namespace\n      nearai benchmark list --namespace my-namespace --benchmark my-benchmark --total 50\n\n    \"\"\"\n\n    def __init__(self):\n        \"\"\"Initialize Benchmark API.\"\"\"\n        self.client = BenchmarkApi()\n\n    def _get_or_create_benchmark(self, benchmark_name: str, solver_name: str, args: Dict[str, Any], force: bool) -&gt; int:\n        if CONFIG.auth is None:\n            print(\"Please login with `nearai login`\")\n            exit(1)\n        namespace = CONFIG.auth.namespace\n\n        # Sort the args to have a consistent representation.\n        solver_args = json.dumps(OrderedDict(sorted(args.items())))\n\n        benchmark_id = self.client.get_benchmark_v1_benchmark_get_get(\n            namespace=namespace,\n            benchmark_name=benchmark_name,\n            solver_name=solver_name,\n            solver_args=solver_args,\n        )\n\n        if benchmark_id == -1 or force:\n            benchmark_id = self.client.create_benchmark_v1_benchmark_create_get(\n                benchmark_name=benchmark_name,\n                solver_name=solver_name,\n                solver_args=solver_args,\n            )\n\n        assert benchmark_id != -1\n        return benchmark_id\n\n    def run(\n        self,\n        dataset: str,\n        solver_strategy: str,\n        max_concurrent: int = 2,\n        force: bool = False,\n        subset: Optional[str] = None,\n        check_compatibility: bool = True,\n        record: bool = False,\n        num_inference_retries: int = 10,\n        **solver_args: Any,\n    ) -&gt; None:\n        \"\"\"Run benchmark on a dataset with a solver strategy.\n\n        This command executes a benchmark on a specified dataset using a given solver strategy.\n        Results are cached in the database for subsequent runs unless --force is used.\n\n        Args:\n          dataset (str) :\n            Name of the dataset to benchmark against\n          solver_strategy (str) :\n            Name of the solver strategy to use\n          max_concurrent (int) :\n            Maximum number of concurrent runs (-1 for CPU count)\n          force (bool) :\n            Force re-running the benchmark and update cache\n          subset (str) :\n            Optional subset of the dataset to use\n          check_compatibility (bool) :\n            Whether to check solver-dataset compatibility\n          record (bool) :\n            Whether to record detailed benchmark results\n          num_inference_retries (int) :\n            Number of retries for inference operations\n          **solver_args : (dict)\n            Additional arguments passed to the solver strategy\n\n        Examples:\n            # Run a benchmark with default settings\n            nearai benchmark run my-dataset my-solver-strategy\n\n            # Run with custom concurrency and force update\n            nearai benchmark run my-dataset my-solver-strategy --max-concurrent 4 --force\n\n            # Run on a subset with custom solver arguments\n            nearai benchmark run my-dataset my-solver-strategy --subset train --arg1 value1 --arg2 value2\n\n        Documentation:\n            https://docs.near.ai/models/benchmarks_and_evaluations/\n\n        \"\"\"\n        from nearai.benchmark import BenchmarkExecutor, DatasetInfo\n        from nearai.dataset import get_dataset, load_dataset\n        from nearai.solvers import SolverScoringMethod, SolverStrategy, SolverStrategyRegistry\n\n        CONFIG.num_inference_retries = num_inference_retries\n\n        args = dict(solver_args)\n        if subset is not None:\n            args[\"subset\"] = subset\n\n        benchmark_id = self._get_or_create_benchmark(\n            benchmark_name=dataset,\n            solver_name=solver_strategy,\n            args=args,\n            force=force,\n        )\n\n        solver_strategy_class: Union[SolverStrategy, None] = SolverStrategyRegistry.get(solver_strategy, None)\n        assert solver_strategy_class, (\n            f\"Solver strategy {solver_strategy} not found. Available strategies: {list(SolverStrategyRegistry.keys())}\"\n        )\n\n        name = dataset\n        if solver_strategy_class.scoring_method == SolverScoringMethod.Custom:\n            dataset = str(get_dataset(dataset))\n        else:\n            dataset = load_dataset(dataset)\n\n        solver_strategy_obj: SolverStrategy = solver_strategy_class(dataset_ref=dataset, **solver_args)  # type: ignore\n        if check_compatibility:\n            assert name in solver_strategy_obj.compatible_datasets() or any(\n                map(lambda n: n in name, solver_strategy_obj.compatible_datasets())\n            ), f\"Solver strategy {solver_strategy} is not compatible with dataset {name}\"\n\n        dest_path = get_registry_folder() / name\n        metadata_path = dest_path / \"metadata.json\"\n        with open(metadata_path, \"r\") as file:\n            metadata = json.load(file)\n\n        be = BenchmarkExecutor(\n            DatasetInfo(name, subset, dataset, metadata), solver_strategy_obj, benchmark_id=benchmark_id\n        )\n\n        cpu_count = os.cpu_count()\n        max_concurrent = (cpu_count if cpu_count is not None else 1) if max_concurrent &lt; 0 else max_concurrent\n        be.run(max_concurrent=max_concurrent, record=record)\n\n    def list(\n        self,\n        namespace: Optional[str] = None,\n        benchmark: Optional[str] = None,\n        solver: Optional[str] = None,\n        args: Optional[str] = None,\n        total: int = 32,\n        offset: int = 0,\n    ) -&gt; None:\n        \"\"\"List all executed benchmarks.\n\n        This command displays a table of all executed benchmarks, with options to filter\n        by namespace, benchmark name, solver name, and solver arguments. Results are\n        paginated using limit and offset parameters.\n\n        Args:\n          namespace (str) :\n            Filter results by namespace\n          benchmark (str) :\n            Filter results by benchmark name\n          solver (str) :\n            Filter results by solver name\n          args (str) :\n            Filter results by solver arguments (JSON string)\n          total (int) :\n            Maximum number of results to display\n          offset (int) :\n            Number of results to skip\n\n        Examples:\n            # List all benchmarks with default pagination\n            nearai benchmark list\n\n            # Filter by namespace and benchmark name\n            nearai benchmark list --namespace my-namespace --benchmark-name my-benchmark\n\n            # Filter by solver with custom pagination\n            nearai benchmark list --solver-name my-solver --limit 20 --offset 40\n\n            # Filter by solver arguments\n            nearai benchmark list --solver-args '{\"arg1\": \"value1\"}'\n\n        Documentation:\n            https://docs.near.ai/models/benchmarks_and_evaluations/\n\n        \"\"\"\n        result = self.client.list_benchmarks_v1_benchmark_list_get(\n            namespace=namespace,\n            benchmark_name=benchmark,\n            solver_name=solver,\n            solver_args=args,\n            total=total,\n            offset=offset,\n        )\n\n        header = [\"id\", \"namespace\", \"benchmark\", \"solver\", \"args\", \"score\", \"solved\", \"total\"]\n        table = []\n        for benchmark_output in result:\n            score = 100 * benchmark_output.solved / benchmark_output.total\n            table.append(\n                [\n                    fill(str(benchmark_output.id)),\n                    fill(benchmark_output.namespace),\n                    fill(benchmark_output.benchmark),\n                    fill(benchmark_output.solver),\n                    fill(benchmark_output.args),\n                    fill(f\"{score:.2f}%\"),\n                    fill(str(benchmark_output.solved)),\n                    fill(str(benchmark_output.total)),\n                ]\n            )\n\n        print(tabulate(table, headers=header, tablefmt=\"simple_grid\"))\n\n    def __call__(self) -&gt; None:\n        \"\"\"Show help when 'nearai benchmark' is called without subcommands.\"\"\"\n        custom_args = [\"nearai\", \"benchmark\", \"--help\"]\n        handle_help_request(custom_args)\n</code></pre>"},{"location":"api/#nearai.cli.BenchmarkCli.__call__","title":"__call__","text":"<pre><code>__call__() -&gt; None\n</code></pre> <p>Show help when 'nearai benchmark' is called without subcommands.</p> Source code in <code>nearai/cli.py</code> <pre><code>def __call__(self) -&gt; None:\n    \"\"\"Show help when 'nearai benchmark' is called without subcommands.\"\"\"\n    custom_args = [\"nearai\", \"benchmark\", \"--help\"]\n    handle_help_request(custom_args)\n</code></pre>"},{"location":"api/#nearai.cli.BenchmarkCli.__init__","title":"__init__","text":"<pre><code>__init__()\n</code></pre> <p>Initialize Benchmark API.</p> Source code in <code>nearai/cli.py</code> <pre><code>def __init__(self):\n    \"\"\"Initialize Benchmark API.\"\"\"\n    self.client = BenchmarkApi()\n</code></pre>"},{"location":"api/#nearai.cli.BenchmarkCli.list","title":"list","text":"<pre><code>list(namespace: Optional[str] = None, benchmark: Optional[str] = None, solver: Optional[str] = None, args: Optional[str] = None, total: int = 32, offset: int = 0) -&gt; None\n</code></pre> <p>List all executed benchmarks.</p> <p>This command displays a table of all executed benchmarks, with options to filter by namespace, benchmark name, solver name, and solver arguments. Results are paginated using limit and offset parameters.</p> <p>Parameters:</p> Name Type Description Default <code>namespace (str) </code> <p>Filter results by namespace</p> required <code>benchmark (str) </code> <p>Filter results by benchmark name</p> required <code>solver (str) </code> <p>Filter results by solver name</p> required <code>args (str) </code> <p>Filter results by solver arguments (JSON string)</p> required <code>total (int) </code> <p>Maximum number of results to display</p> required <code>offset (int) </code> <p>Number of results to skip</p> required <p>Examples:</p>"},{"location":"api/#nearai.cli.BenchmarkCli.list--list-all-benchmarks-with-default-pagination","title":"List all benchmarks with default pagination","text":"<p>nearai benchmark list</p>"},{"location":"api/#nearai.cli.BenchmarkCli.list--filter-by-namespace-and-benchmark-name","title":"Filter by namespace and benchmark name","text":"<p>nearai benchmark list --namespace my-namespace --benchmark-name my-benchmark</p>"},{"location":"api/#nearai.cli.BenchmarkCli.list--filter-by-solver-with-custom-pagination","title":"Filter by solver with custom pagination","text":"<p>nearai benchmark list --solver-name my-solver --limit 20 --offset 40</p>"},{"location":"api/#nearai.cli.BenchmarkCli.list--filter-by-solver-arguments","title":"Filter by solver arguments","text":"<p>nearai benchmark list --solver-args '{\"arg1\": \"value1\"}'</p> Documentation <p>https://docs.near.ai/models/benchmarks_and_evaluations/</p> Source code in <code>nearai/cli.py</code> <pre><code>def list(\n    self,\n    namespace: Optional[str] = None,\n    benchmark: Optional[str] = None,\n    solver: Optional[str] = None,\n    args: Optional[str] = None,\n    total: int = 32,\n    offset: int = 0,\n) -&gt; None:\n    \"\"\"List all executed benchmarks.\n\n    This command displays a table of all executed benchmarks, with options to filter\n    by namespace, benchmark name, solver name, and solver arguments. Results are\n    paginated using limit and offset parameters.\n\n    Args:\n      namespace (str) :\n        Filter results by namespace\n      benchmark (str) :\n        Filter results by benchmark name\n      solver (str) :\n        Filter results by solver name\n      args (str) :\n        Filter results by solver arguments (JSON string)\n      total (int) :\n        Maximum number of results to display\n      offset (int) :\n        Number of results to skip\n\n    Examples:\n        # List all benchmarks with default pagination\n        nearai benchmark list\n\n        # Filter by namespace and benchmark name\n        nearai benchmark list --namespace my-namespace --benchmark-name my-benchmark\n\n        # Filter by solver with custom pagination\n        nearai benchmark list --solver-name my-solver --limit 20 --offset 40\n\n        # Filter by solver arguments\n        nearai benchmark list --solver-args '{\"arg1\": \"value1\"}'\n\n    Documentation:\n        https://docs.near.ai/models/benchmarks_and_evaluations/\n\n    \"\"\"\n    result = self.client.list_benchmarks_v1_benchmark_list_get(\n        namespace=namespace,\n        benchmark_name=benchmark,\n        solver_name=solver,\n        solver_args=args,\n        total=total,\n        offset=offset,\n    )\n\n    header = [\"id\", \"namespace\", \"benchmark\", \"solver\", \"args\", \"score\", \"solved\", \"total\"]\n    table = []\n    for benchmark_output in result:\n        score = 100 * benchmark_output.solved / benchmark_output.total\n        table.append(\n            [\n                fill(str(benchmark_output.id)),\n                fill(benchmark_output.namespace),\n                fill(benchmark_output.benchmark),\n                fill(benchmark_output.solver),\n                fill(benchmark_output.args),\n                fill(f\"{score:.2f}%\"),\n                fill(str(benchmark_output.solved)),\n                fill(str(benchmark_output.total)),\n            ]\n        )\n\n    print(tabulate(table, headers=header, tablefmt=\"simple_grid\"))\n</code></pre>"},{"location":"api/#nearai.cli.BenchmarkCli.run","title":"run","text":"<pre><code>run(dataset: str, solver_strategy: str, max_concurrent: int = 2, force: bool = False, subset: Optional[str] = None, check_compatibility: bool = True, record: bool = False, num_inference_retries: int = 10, **solver_args: Any) -&gt; None\n</code></pre> <p>Run benchmark on a dataset with a solver strategy.</p> <p>This command executes a benchmark on a specified dataset using a given solver strategy. Results are cached in the database for subsequent runs unless --force is used.</p> <p>Parameters:</p> Name Type Description Default <code>dataset (str) </code> <p>Name of the dataset to benchmark against</p> required <code>solver_strategy (str) </code> <p>Name of the solver strategy to use</p> required <code>max_concurrent (int) </code> <p>Maximum number of concurrent runs (-1 for CPU count)</p> required <code>force (bool) </code> <p>Force re-running the benchmark and update cache</p> required <code>subset (str) </code> <p>Optional subset of the dataset to use</p> required <code>check_compatibility (bool) </code> <p>Whether to check solver-dataset compatibility</p> required <code>record (bool) </code> <p>Whether to record detailed benchmark results</p> required <code>num_inference_retries (int) </code> <p>Number of retries for inference operations</p> required <code>**solver_args </code> <p>(dict) Additional arguments passed to the solver strategy</p> required <p>Examples:</p>"},{"location":"api/#nearai.cli.BenchmarkCli.run--run-a-benchmark-with-default-settings","title":"Run a benchmark with default settings","text":"<p>nearai benchmark run my-dataset my-solver-strategy</p>"},{"location":"api/#nearai.cli.BenchmarkCli.run--run-with-custom-concurrency-and-force-update","title":"Run with custom concurrency and force update","text":"<p>nearai benchmark run my-dataset my-solver-strategy --max-concurrent 4 --force</p>"},{"location":"api/#nearai.cli.BenchmarkCli.run--run-on-a-subset-with-custom-solver-arguments","title":"Run on a subset with custom solver arguments","text":"<p>nearai benchmark run my-dataset my-solver-strategy --subset train --arg1 value1 --arg2 value2</p> Documentation <p>https://docs.near.ai/models/benchmarks_and_evaluations/</p> Source code in <code>nearai/cli.py</code> <pre><code>def run(\n    self,\n    dataset: str,\n    solver_strategy: str,\n    max_concurrent: int = 2,\n    force: bool = False,\n    subset: Optional[str] = None,\n    check_compatibility: bool = True,\n    record: bool = False,\n    num_inference_retries: int = 10,\n    **solver_args: Any,\n) -&gt; None:\n    \"\"\"Run benchmark on a dataset with a solver strategy.\n\n    This command executes a benchmark on a specified dataset using a given solver strategy.\n    Results are cached in the database for subsequent runs unless --force is used.\n\n    Args:\n      dataset (str) :\n        Name of the dataset to benchmark against\n      solver_strategy (str) :\n        Name of the solver strategy to use\n      max_concurrent (int) :\n        Maximum number of concurrent runs (-1 for CPU count)\n      force (bool) :\n        Force re-running the benchmark and update cache\n      subset (str) :\n        Optional subset of the dataset to use\n      check_compatibility (bool) :\n        Whether to check solver-dataset compatibility\n      record (bool) :\n        Whether to record detailed benchmark results\n      num_inference_retries (int) :\n        Number of retries for inference operations\n      **solver_args : (dict)\n        Additional arguments passed to the solver strategy\n\n    Examples:\n        # Run a benchmark with default settings\n        nearai benchmark run my-dataset my-solver-strategy\n\n        # Run with custom concurrency and force update\n        nearai benchmark run my-dataset my-solver-strategy --max-concurrent 4 --force\n\n        # Run on a subset with custom solver arguments\n        nearai benchmark run my-dataset my-solver-strategy --subset train --arg1 value1 --arg2 value2\n\n    Documentation:\n        https://docs.near.ai/models/benchmarks_and_evaluations/\n\n    \"\"\"\n    from nearai.benchmark import BenchmarkExecutor, DatasetInfo\n    from nearai.dataset import get_dataset, load_dataset\n    from nearai.solvers import SolverScoringMethod, SolverStrategy, SolverStrategyRegistry\n\n    CONFIG.num_inference_retries = num_inference_retries\n\n    args = dict(solver_args)\n    if subset is not None:\n        args[\"subset\"] = subset\n\n    benchmark_id = self._get_or_create_benchmark(\n        benchmark_name=dataset,\n        solver_name=solver_strategy,\n        args=args,\n        force=force,\n    )\n\n    solver_strategy_class: Union[SolverStrategy, None] = SolverStrategyRegistry.get(solver_strategy, None)\n    assert solver_strategy_class, (\n        f\"Solver strategy {solver_strategy} not found. Available strategies: {list(SolverStrategyRegistry.keys())}\"\n    )\n\n    name = dataset\n    if solver_strategy_class.scoring_method == SolverScoringMethod.Custom:\n        dataset = str(get_dataset(dataset))\n    else:\n        dataset = load_dataset(dataset)\n\n    solver_strategy_obj: SolverStrategy = solver_strategy_class(dataset_ref=dataset, **solver_args)  # type: ignore\n    if check_compatibility:\n        assert name in solver_strategy_obj.compatible_datasets() or any(\n            map(lambda n: n in name, solver_strategy_obj.compatible_datasets())\n        ), f\"Solver strategy {solver_strategy} is not compatible with dataset {name}\"\n\n    dest_path = get_registry_folder() / name\n    metadata_path = dest_path / \"metadata.json\"\n    with open(metadata_path, \"r\") as file:\n        metadata = json.load(file)\n\n    be = BenchmarkExecutor(\n        DatasetInfo(name, subset, dataset, metadata), solver_strategy_obj, benchmark_id=benchmark_id\n    )\n\n    cpu_count = os.cpu_count()\n    max_concurrent = (cpu_count if cpu_count is not None else 1) if max_concurrent &lt; 0 else max_concurrent\n    be.run(max_concurrent=max_concurrent, record=record)\n</code></pre>"},{"location":"api/#nearai.cli.CLI","title":"CLI","text":"<p>List of commands for NEAR AI CLI to be used in the help menu.</p> Getting Started <p>nearai              CLI MAIN MENU HELP nearai login        Authenticate with your NEAR account nearai logout       Clear your NEAR account authentication data nearai version      Display the current version of the CLI nearai location     Show the installation location of the CLI</p> Agent Development <p>nearai agent              AGENT HELP MENU nearai agent create       Create a new agent nearai agent upload       Upload an agent to the NEAR AI agent registry nearai agent interactive  Run an agent interactively nearai agent task         Run a single task with an agent nearai agent dev          Run local UI for development of agents nearai agent inspect      Inspect environment from given path</p> Registry Management <p>nearai registry                    REGISTRY HELP MENU nearai registry upload             Upload an item to the registry nearai registry download           Download an item from the registry nearai registry info               Show information about a registry item nearai registry list               List available items in the registry nearai registry update             Update the remote version in an agent's metadata.json file nearai registry metadata-template  Create a metadata template nearai permission                  PERMISSION HELP MENU (manage access control)</p> Model Operations <p>nearai benchmark run      Run benchmark on a dataset with a solver strategy nearai benchmark list     List all executed benchmarks nearai evaluation table   Print table of evaluations nearai finetune           Commands for fine-tuning modelsnear nearai tensorboard        Commands for TensorBoard integration nearai vllm run           Run VLLM server with OpenAI-compatible API nearai hub chat           Chat with model from NEAR AI hub</p> Configuration <p>nearai config             CONFIG HELP MENU nearai config set         Set a configuration value nearai config get         Get a configuration value nearai config show        Show all configuration values</p> Source code in <code>nearai/cli.py</code> <pre><code>class CLI:\n    # TODO: Dynamically generate help menu based on available commands\n    \"\"\"List of commands for NEAR AI CLI to be used in the help menu.\n\n    Getting Started:\n      nearai              CLI MAIN MENU HELP\n      nearai login        Authenticate with your NEAR account\n      nearai logout       Clear your NEAR account authentication data\n      nearai version      Display the current version of the CLI\n      nearai location     Show the installation location of the CLI\n\n    Agent Development:\n      nearai agent              AGENT HELP MENU\n      nearai agent create       Create a new agent\n      nearai agent upload       Upload an agent to the NEAR AI agent registry\n      nearai agent interactive  Run an agent interactively\n      nearai agent task         Run a single task with an agent\n      nearai agent dev          Run local UI for development of agents\n      nearai agent inspect      Inspect environment from given path\n\n    Registry Management:\n      nearai registry                    REGISTRY HELP MENU\n      nearai registry upload             Upload an item to the registry\n      nearai registry download           Download an item from the registry\n      nearai registry info               Show information about a registry item\n      nearai registry list               List available items in the registry\n      nearai registry update             Update the remote version in an agent's metadata.json file\n      nearai registry metadata-template  Create a metadata template\n      nearai permission                  PERMISSION HELP MENU (manage access control)\n\n    Model Operations:\n      nearai benchmark run      Run benchmark on a dataset with a solver strategy\n      nearai benchmark list     List all executed benchmarks\n      nearai evaluation table   Print table of evaluations\n      nearai finetune           Commands for fine-tuning modelsnear\n      nearai tensorboard        Commands for TensorBoard integration\n      nearai vllm run           Run VLLM server with OpenAI-compatible API\n      nearai hub chat           Chat with model from NEAR AI hub\n\n    Configuration:\n      nearai config             CONFIG HELP MENU\n      nearai config set         Set a configuration value\n      nearai config get         Get a configuration value\n      nearai config show        Show all configuration values\n\n    \"\"\"  # noqa: D400 D415 D210\n\n    def __init__(self) -&gt; None:  # noqa: D107\n        self.registry = RegistryCli()\n        self.login = LoginCLI()\n        self.logout = LogoutCLI()\n        self.hub = HubCLI()\n        self.log = LogCLI()\n\n        self.config = ConfigCli()\n        self.benchmark = BenchmarkCli()\n        self.evaluation = EvaluationCli()\n        self.agent = AgentCli()\n        self.tensorboard = TensorboardCli()\n        self.vllm = VllmCli()\n        self.permission = PermissionCli()\n\n    def submit(self, path: Optional[str] = None, worker_kind: str = WorkerKind.GPU_8_A100.value):\n        \"\"\"Submit a task to be executed by a worker.\"\"\"\n        if path is None:\n            path = os.getcwd()\n\n        worker_kind_t = WorkerKind(worker_kind)\n\n        location = self.registry.upload(path)\n\n        if location is None:\n            print(\"Error: Failed to upload entry\")\n            return\n\n        delegation_api = DelegationApi()\n        delegation_api.delegate_v1_delegation_delegate_post(\n            delegate_account_id=CONFIG.scheduler_account_id,\n            expires_at=datetime.now() + timedelta(days=1),\n        )\n\n        try:\n            client = JobsApi()\n            client.add_job_v1_jobs_add_job_post(\n                worker_kind_t,\n                BodyAddJobV1JobsAddJobPost(entry_location=location),\n            )\n        except Exception as e:\n            print(\"Error: \", e)\n            delegation_api.revoke_delegation_v1_delegation_revoke_delegation_post(\n                delegate_account_id=CONFIG.scheduler_account_id,\n            )\n\n    def location(self) -&gt; None:  # noqa: D102\n        \"\"\"Show location where nearai is installed.\"\"\"\n        from nearai import cli_path\n\n        print(cli_path())\n\n    def version(self):\n        \"\"\"Show nearai version.\"\"\"\n        print(importlib.metadata.version(\"nearai\"))\n\n    def task(self, *args, **kwargs):\n        \"\"\"CLI command for running a single task.\"\"\"\n        self.agent.task_cli(*args, **kwargs)\n\n    def help(self) -&gt; None:\n        \"\"\"Display help information about the NEAR AI CLI.\"\"\"\n        custom_args = [\"nearai\", \"--help\"]\n        handle_help_request(custom_args)\n</code></pre>"},{"location":"api/#nearai.cli.CLI.help","title":"help","text":"<pre><code>help() -&gt; None\n</code></pre> <p>Display help information about the NEAR AI CLI.</p> Source code in <code>nearai/cli.py</code> <pre><code>def help(self) -&gt; None:\n    \"\"\"Display help information about the NEAR AI CLI.\"\"\"\n    custom_args = [\"nearai\", \"--help\"]\n    handle_help_request(custom_args)\n</code></pre>"},{"location":"api/#nearai.cli.CLI.location","title":"location","text":"<pre><code>location() -&gt; None\n</code></pre> <p>Show location where nearai is installed.</p> Source code in <code>nearai/cli.py</code> <pre><code>def location(self) -&gt; None:  # noqa: D102\n    \"\"\"Show location where nearai is installed.\"\"\"\n    from nearai import cli_path\n\n    print(cli_path())\n</code></pre>"},{"location":"api/#nearai.cli.CLI.submit","title":"submit","text":"<pre><code>submit(path: Optional[str] = None, worker_kind: str = GPU_8_A100.value)\n</code></pre> <p>Submit a task to be executed by a worker.</p> Source code in <code>nearai/cli.py</code> <pre><code>def submit(self, path: Optional[str] = None, worker_kind: str = WorkerKind.GPU_8_A100.value):\n    \"\"\"Submit a task to be executed by a worker.\"\"\"\n    if path is None:\n        path = os.getcwd()\n\n    worker_kind_t = WorkerKind(worker_kind)\n\n    location = self.registry.upload(path)\n\n    if location is None:\n        print(\"Error: Failed to upload entry\")\n        return\n\n    delegation_api = DelegationApi()\n    delegation_api.delegate_v1_delegation_delegate_post(\n        delegate_account_id=CONFIG.scheduler_account_id,\n        expires_at=datetime.now() + timedelta(days=1),\n    )\n\n    try:\n        client = JobsApi()\n        client.add_job_v1_jobs_add_job_post(\n            worker_kind_t,\n            BodyAddJobV1JobsAddJobPost(entry_location=location),\n        )\n    except Exception as e:\n        print(\"Error: \", e)\n        delegation_api.revoke_delegation_v1_delegation_revoke_delegation_post(\n            delegate_account_id=CONFIG.scheduler_account_id,\n        )\n</code></pre>"},{"location":"api/#nearai.cli.CLI.task","title":"task","text":"<pre><code>task(*args, **kwargs)\n</code></pre> <p>CLI command for running a single task.</p> Source code in <code>nearai/cli.py</code> <pre><code>def task(self, *args, **kwargs):\n    \"\"\"CLI command for running a single task.\"\"\"\n    self.agent.task_cli(*args, **kwargs)\n</code></pre>"},{"location":"api/#nearai.cli.CLI.version","title":"version","text":"<pre><code>version()\n</code></pre> <p>Show nearai version.</p> Source code in <code>nearai/cli.py</code> <pre><code>def version(self):\n    \"\"\"Show nearai version.\"\"\"\n    print(importlib.metadata.version(\"nearai\"))\n</code></pre>"},{"location":"api/#nearai.cli.ConfigCli","title":"ConfigCli","text":"<p>Configuration commands help you manage your NEAR AI CLI settings.</p> <p>You can view, set, and modify various configuration values that control how the CLI behaves.</p> Commands <p>nearai config set : Add or update a configuration value   (key*, value*, --local) nearai config get : Retrieve a configuration value   (key*) nearai config show : Display all configuration values</p> Options <p>key* (str) :   The configuration key to set or get value* (str) :   The value to assign to the configuration key --local (bool) :   Store the configuration value in the local config file</p> <p>Examples:</p>"},{"location":"api/#nearai.cli.ConfigCli--view-all-configuration-values","title":"View all configuration values","text":"<p>nearai config show</p>"},{"location":"api/#nearai.cli.ConfigCli--get-a-specific-configuration-value","title":"Get a specific configuration value","text":"<p>nearai config get api_url</p>"},{"location":"api/#nearai.cli.ConfigCli--set-a-configuration-value-globally","title":"Set a configuration value (globally)","text":"<p>nearai config set model claude-3-opus-20240229</p>"},{"location":"api/#nearai.cli.ConfigCli--set-a-configuration-value-locally-for-current-project","title":"Set a configuration value (locally for current project)","text":"<p>nearai config set model claude-3-opus-20240229 --local</p>"},{"location":"api/#nearai.cli.ConfigCli--change-the-api-url","title":"Change the API URL","text":"<p>nearai config set api_url https://custom-api.example.com</p> Source code in <code>nearai/cli.py</code> <pre><code>class ConfigCli:\n    \"\"\"Configuration commands help you manage your NEAR AI CLI settings.\n\n    You can view, set, and modify various configuration values that control how the CLI behaves.\n\n    Commands:\n      nearai config set : Add or update a configuration value\n        (key*, value*, --local)\n      nearai config get : Retrieve a configuration value\n        (key*)\n      nearai config show : Display all configuration values\n\n    Options:\n      key* (str) :\n        The configuration key to set or get\n      value* (str) :\n        The value to assign to the configuration key\n      --local (bool) :\n        Store the configuration value in the local config file\n\n    Examples:\n      # View all configuration values\n      nearai config show\n\n      # Get a specific configuration value\n      nearai config get api_url\n\n      # Set a configuration value (globally)\n      nearai config set model claude-3-opus-20240229\n\n      # Set a configuration value (locally for current project)\n      nearai config set model claude-3-opus-20240229 --local\n\n      # Change the API URL\n      nearai config set api_url https://custom-api.example.com\n\n    \"\"\"\n\n    def set(self, key: str, value: str, local: bool = False) -&gt; None:\n        \"\"\"Add key-value pair to the config file.\"\"\"\n        update_config(key, value, local)\n\n    def get(self, key: str) -&gt; None:\n        \"\"\"Get value of a key in the config file.\"\"\"\n        print(CONFIG.get(key))\n\n    def show(self) -&gt; None:  # noqa: D102\n        for key, value in asdict(CONFIG).items():\n            print(f\"{key}: {value}\")\n\n    def __call__(self) -&gt; None:\n        \"\"\"Show help when 'nearai config' is called without subcommands.\"\"\"\n        custom_args = [\"nearai\", \"config\", \"--help\"]\n        handle_help_request(custom_args)\n</code></pre>"},{"location":"api/#nearai.cli.ConfigCli.__call__","title":"__call__","text":"<pre><code>__call__() -&gt; None\n</code></pre> <p>Show help when 'nearai config' is called without subcommands.</p> Source code in <code>nearai/cli.py</code> <pre><code>def __call__(self) -&gt; None:\n    \"\"\"Show help when 'nearai config' is called without subcommands.\"\"\"\n    custom_args = [\"nearai\", \"config\", \"--help\"]\n    handle_help_request(custom_args)\n</code></pre>"},{"location":"api/#nearai.cli.ConfigCli.get","title":"get","text":"<pre><code>get(key: str) -&gt; None\n</code></pre> <p>Get value of a key in the config file.</p> Source code in <code>nearai/cli.py</code> <pre><code>def get(self, key: str) -&gt; None:\n    \"\"\"Get value of a key in the config file.\"\"\"\n    print(CONFIG.get(key))\n</code></pre>"},{"location":"api/#nearai.cli.ConfigCli.set","title":"set","text":"<pre><code>set(key: str, value: str, local: bool = False) -&gt; None\n</code></pre> <p>Add key-value pair to the config file.</p> Source code in <code>nearai/cli.py</code> <pre><code>def set(self, key: str, value: str, local: bool = False) -&gt; None:\n    \"\"\"Add key-value pair to the config file.\"\"\"\n    update_config(key, value, local)\n</code></pre>"},{"location":"api/#nearai.cli.EvaluationCli","title":"EvaluationCli","text":"<p>Commands for evaluating and analyzing model performance on benchmark datasets.</p> Commands <p>nearai evaluation table : Print table of evaluations   (--all-key-columns, --all-metrics, --num-columns, --metric-name-max-length) nearai evaluation read_solutions : Read solutions.json from evaluation entry   (entry*, --status, --verbose)</p> Options <p>entry* (str) :   Evaluation entry to read solutions from (format: namespace/name/version) --all-key-columns (bool) :   Show all key columns in the table --all-metrics (bool) :   Show all metrics in the table --num-columns (int) :   Maximum number of columns to display --metric-name-max-length (int) :   Maximum length for metric names in display --status (bool) :   Filter solutions by status (true/false) --verbose (bool) :   Show verbose information including detailed logs</p> <p>Examples:</p>"},{"location":"api/#nearai.cli.EvaluationCli--display-evaluation-table-with-default-settings","title":"Display evaluation table with default settings","text":"<p>nearai evaluation table</p>"},{"location":"api/#nearai.cli.EvaluationCli--display-evaluation-table-with-all-metrics-and-columns","title":"Display evaluation table with all metrics and columns","text":"<p>nearai evaluation table --all-key-columns --all-metrics --num-columns 10</p>"},{"location":"api/#nearai.cli.EvaluationCli--read-solutions-from-an-evaluation-entry","title":"Read solutions from an evaluation entry","text":"<p>nearai evaluation read_solutions example.near/benchmark-result/0.1.0</p>"},{"location":"api/#nearai.cli.EvaluationCli--read-only-successful-solutions-with-verbose-output","title":"Read only successful solutions with verbose output","text":"<p>nearai evaluation read_solutions example.near/benchmark-result/0.1.0 --status true --verbose</p> <p>Documentation:   https://docs.near.ai/models/benchmarks_and_evaluations/</p> Source code in <code>nearai/cli.py</code> <pre><code>class EvaluationCli:\n    \"\"\"Commands for evaluating and analyzing model performance on benchmark datasets.\n\n    Commands:\n      nearai evaluation table : Print table of evaluations\n        (--all-key-columns, --all-metrics, --num-columns, --metric-name-max-length)\n      nearai evaluation read_solutions : Read solutions.json from evaluation entry\n        (entry*, --status, --verbose)\n\n    Options:\n      entry* (str) :\n        Evaluation entry to read solutions from (format: namespace/name/version)\n      --all-key-columns (bool) :\n        Show all key columns in the table\n      --all-metrics (bool) :\n        Show all metrics in the table\n      --num-columns (int) :\n        Maximum number of columns to display\n      --metric-name-max-length (int) :\n        Maximum length for metric names in display\n      --status (bool) :\n        Filter solutions by status (true/false)\n      --verbose (bool) :\n        Show verbose information including detailed logs\n\n    Examples:\n      # Display evaluation table with default settings\n      nearai evaluation table\n\n      # Display evaluation table with all metrics and columns\n      nearai evaluation table --all-key-columns --all-metrics --num-columns 10\n\n      # Read solutions from an evaluation entry\n      nearai evaluation read_solutions example.near/benchmark-result/0.1.0\n\n      # Read only successful solutions with verbose output\n      nearai evaluation read_solutions example.near/benchmark-result/0.1.0 --status true --verbose\n\n      Documentation:\n        https://docs.near.ai/models/benchmarks_and_evaluations/\n\n    \"\"\"\n\n    def table(\n        self,\n        all_key_columns: bool = False,\n        all_metrics: bool = False,\n        num_columns: int = 6,\n        metric_name_max_length: int = 30,\n    ) -&gt; None:\n        \"\"\"Displays a table of all evaluation results, with options to customize the display of columns and metrics.\n\n        The table can be configured to show all key columns and metrics, or a limited subset for better readability.\n\n        Args:\n        ----\n          all_key_columns (bool) :\n            Show all key columns in the table instead of just the important ones. Default is False.\n          all_metrics (bool) :\n            Show all available metrics instead of just the default subset. Default is False.\n          num_columns (int) :\n            Maximum number of columns to display in the table.\n          metric_name_max_length (int) :\n            Maximum length for metric names in the display.\n\n        Examples:\n        --------\n            # Display evaluation table with default settings\n            nearai evaluation table\n\n            # Show all available columns and metrics\n            nearai evaluation table --all-key-columns --all-metrics\n\n            # Customize table display\n            nearai evaluation table --num-columns 8 --metric-name-max-length 40\n\n        Documentation:\n            https://docs.near.ai/models/benchmarks_and_evaluations/\n\n        \"\"\"\n        from nearai.evaluation import print_evaluation_table\n\n        api = EvaluationApi()\n        table = api.table_v1_evaluation_table_get()\n\n        print_evaluation_table(\n            table.rows,\n            table.columns,\n            table.important_columns,\n            all_key_columns,\n            all_metrics,\n            num_columns,\n            metric_name_max_length,\n        )\n\n    def read_solutions(self, entry: str, status: Optional[bool] = None, verbose: bool = False) -&gt; None:\n        \"\"\"Reads and displays the solutions.json file from a specified evaluation entry.\n\n          It can filter solutions by status and show either concise or verbose output for each solution.\n\n        Args:\n        ----\n          entry (str) :\n            Evaluation entry to read solutions from (format: namespace/name/version)\n          status (Optional[bool]) :\n            Filter solutions by status (true/false)\n          verbose (bool) :\n            Show verbose information including detailed logs\n\n        Examples:\n        --------\n            # Read all solutions from an evaluation entry\n            nearai evaluation read_solutions example.near/benchmark-result/0.1.0\n\n            # Read only successful solutions\n            nearai evaluation read_solutions example.near/benchmark-result/0.1.0 --status true\n\n            # Read solutions with verbose output\n            nearai evaluation read_solutions example.near/benchmark-result/0.1.0 --verbose\n\n        Documentation:\n            https://docs.near.ai/models/benchmarks_and_evaluations/\n\n        \"\"\"\n        entry_path = registry.download(entry)\n        solutions_file = entry_path / \"solutions.json\"\n\n        if not solutions_file.exists():\n            print(f\"No solutions file found for entry: {entry}\")\n            return\n\n        try:\n            with open(solutions_file) as f:\n                solutions = json.load(f)\n        except json.JSONDecodeError:\n            print(f\"Error reading solutions file for entry: {entry}\")\n            return\n\n        # Filter solutions if status is specified\n        if status is not None:\n            solutions = [s for s in solutions if s.get(\"status\") == status]\n        if not solutions:\n            print(\"No solutions found matching criteria\")\n            return\n        print(f\"\\nFound {len(solutions)} solutions{' with status=' + str(status) if status is not None else ''}\")\n\n        for i, solution in enumerate(solutions, 1):\n            print(\"-\" * 80)\n            print(f\"\\nSolution {i}/{len(solutions)}:\")\n            datum = solution.get(\"datum\")\n            print(f\"datum: {json.dumps(datum, indent=2, ensure_ascii=False)}\")\n            status = solution.get(\"status\")\n            print(f\"status: {status}\")\n            info: dict = solution.get(\"info\", {})\n            if not verbose and isinstance(info, dict):\n                info.pop(\"verbose\", {})\n            print(f\"info: {json.dumps(info, indent=2, ensure_ascii=False)}\")\n            if i == 1:\n                print(\"Enter to continue, type 'exit' to quit.\")\n            new_message = input(\"&gt; \")\n            if new_message.lower() == \"exit\":\n                break\n\n    def __call__(self) -&gt; None:\n        \"\"\"Show help when 'nearai evaluation' is called without subcommands.\"\"\"\n        custom_args = [\"nearai\", \"evaluation\", \"--help\"]\n        handle_help_request(custom_args)\n</code></pre>"},{"location":"api/#nearai.cli.EvaluationCli.__call__","title":"__call__","text":"<pre><code>__call__() -&gt; None\n</code></pre> <p>Show help when 'nearai evaluation' is called without subcommands.</p> Source code in <code>nearai/cli.py</code> <pre><code>def __call__(self) -&gt; None:\n    \"\"\"Show help when 'nearai evaluation' is called without subcommands.\"\"\"\n    custom_args = [\"nearai\", \"evaluation\", \"--help\"]\n    handle_help_request(custom_args)\n</code></pre>"},{"location":"api/#nearai.cli.EvaluationCli.read_solutions","title":"read_solutions","text":"<pre><code>read_solutions(entry: str, status: Optional[bool] = None, verbose: bool = False) -&gt; None\n</code></pre> <p>Reads and displays the solutions.json file from a specified evaluation entry.</p> <p>It can filter solutions by status and show either concise or verbose output for each solution.</p> <p>entry (str) :     Evaluation entry to read solutions from (format: namespace/name/version)   status (Optional[bool]) :     Filter solutions by status (true/false)   verbose (bool) :     Show verbose information including detailed logs</p> <p>Examples:</p> <pre><code># Read all solutions from an evaluation entry\nnearai evaluation read_solutions example.near/benchmark-result/0.1.0\n\n# Read only successful solutions\nnearai evaluation read_solutions example.near/benchmark-result/0.1.0 --status true\n\n# Read solutions with verbose output\nnearai evaluation read_solutions example.near/benchmark-result/0.1.0 --verbose\n</code></pre> Documentation <p>https://docs.near.ai/models/benchmarks_and_evaluations/</p> Source code in <code>nearai/cli.py</code> <pre><code>def read_solutions(self, entry: str, status: Optional[bool] = None, verbose: bool = False) -&gt; None:\n    \"\"\"Reads and displays the solutions.json file from a specified evaluation entry.\n\n      It can filter solutions by status and show either concise or verbose output for each solution.\n\n    Args:\n    ----\n      entry (str) :\n        Evaluation entry to read solutions from (format: namespace/name/version)\n      status (Optional[bool]) :\n        Filter solutions by status (true/false)\n      verbose (bool) :\n        Show verbose information including detailed logs\n\n    Examples:\n    --------\n        # Read all solutions from an evaluation entry\n        nearai evaluation read_solutions example.near/benchmark-result/0.1.0\n\n        # Read only successful solutions\n        nearai evaluation read_solutions example.near/benchmark-result/0.1.0 --status true\n\n        # Read solutions with verbose output\n        nearai evaluation read_solutions example.near/benchmark-result/0.1.0 --verbose\n\n    Documentation:\n        https://docs.near.ai/models/benchmarks_and_evaluations/\n\n    \"\"\"\n    entry_path = registry.download(entry)\n    solutions_file = entry_path / \"solutions.json\"\n\n    if not solutions_file.exists():\n        print(f\"No solutions file found for entry: {entry}\")\n        return\n\n    try:\n        with open(solutions_file) as f:\n            solutions = json.load(f)\n    except json.JSONDecodeError:\n        print(f\"Error reading solutions file for entry: {entry}\")\n        return\n\n    # Filter solutions if status is specified\n    if status is not None:\n        solutions = [s for s in solutions if s.get(\"status\") == status]\n    if not solutions:\n        print(\"No solutions found matching criteria\")\n        return\n    print(f\"\\nFound {len(solutions)} solutions{' with status=' + str(status) if status is not None else ''}\")\n\n    for i, solution in enumerate(solutions, 1):\n        print(\"-\" * 80)\n        print(f\"\\nSolution {i}/{len(solutions)}:\")\n        datum = solution.get(\"datum\")\n        print(f\"datum: {json.dumps(datum, indent=2, ensure_ascii=False)}\")\n        status = solution.get(\"status\")\n        print(f\"status: {status}\")\n        info: dict = solution.get(\"info\", {})\n        if not verbose and isinstance(info, dict):\n            info.pop(\"verbose\", {})\n        print(f\"info: {json.dumps(info, indent=2, ensure_ascii=False)}\")\n        if i == 1:\n            print(\"Enter to continue, type 'exit' to quit.\")\n        new_message = input(\"&gt; \")\n        if new_message.lower() == \"exit\":\n            break\n</code></pre>"},{"location":"api/#nearai.cli.EvaluationCli.table","title":"table","text":"<pre><code>table(all_key_columns: bool = False, all_metrics: bool = False, num_columns: int = 6, metric_name_max_length: int = 30) -&gt; None\n</code></pre> <p>Displays a table of all evaluation results, with options to customize the display of columns and metrics.</p> <p>The table can be configured to show all key columns and metrics, or a limited subset for better readability.</p> <p>all_key_columns (bool) :     Show all key columns in the table instead of just the important ones. Default is False.   all_metrics (bool) :     Show all available metrics instead of just the default subset. Default is False.   num_columns (int) :     Maximum number of columns to display in the table.   metric_name_max_length (int) :     Maximum length for metric names in the display.</p> <p>Examples:</p> <pre><code># Display evaluation table with default settings\nnearai evaluation table\n\n# Show all available columns and metrics\nnearai evaluation table --all-key-columns --all-metrics\n\n# Customize table display\nnearai evaluation table --num-columns 8 --metric-name-max-length 40\n</code></pre> Documentation <p>https://docs.near.ai/models/benchmarks_and_evaluations/</p> Source code in <code>nearai/cli.py</code> <pre><code>def table(\n    self,\n    all_key_columns: bool = False,\n    all_metrics: bool = False,\n    num_columns: int = 6,\n    metric_name_max_length: int = 30,\n) -&gt; None:\n    \"\"\"Displays a table of all evaluation results, with options to customize the display of columns and metrics.\n\n    The table can be configured to show all key columns and metrics, or a limited subset for better readability.\n\n    Args:\n    ----\n      all_key_columns (bool) :\n        Show all key columns in the table instead of just the important ones. Default is False.\n      all_metrics (bool) :\n        Show all available metrics instead of just the default subset. Default is False.\n      num_columns (int) :\n        Maximum number of columns to display in the table.\n      metric_name_max_length (int) :\n        Maximum length for metric names in the display.\n\n    Examples:\n    --------\n        # Display evaluation table with default settings\n        nearai evaluation table\n\n        # Show all available columns and metrics\n        nearai evaluation table --all-key-columns --all-metrics\n\n        # Customize table display\n        nearai evaluation table --num-columns 8 --metric-name-max-length 40\n\n    Documentation:\n        https://docs.near.ai/models/benchmarks_and_evaluations/\n\n    \"\"\"\n    from nearai.evaluation import print_evaluation_table\n\n    api = EvaluationApi()\n    table = api.table_v1_evaluation_table_get()\n\n    print_evaluation_table(\n        table.rows,\n        table.columns,\n        table.important_columns,\n        all_key_columns,\n        all_metrics,\n        num_columns,\n        metric_name_max_length,\n    )\n</code></pre>"},{"location":"api/#nearai.cli.HubCLI","title":"HubCLI","text":"<p>Interact with models hosted on the NEAR AI hub.</p> Commands <p>nearai hub chat : Chat with model from NEAR AI hub   (--query, --model, --provider, --endpoint, --info)</p> Options <p>--query (str) :   User's query to send to the model --model (str) :   Name of the model to use --provider (str) :   Name of the provider (e.g., \"fireworks\", \"hyperbolic\") --endpoint (str) :   NEAR AI Hub's URL to connect to --info (bool) :   Display system information about the request</p> <p>Examples:</p>"},{"location":"api/#nearai.cli.HubCLI--chat-with-the-default-model","title":"Chat with the default model","text":"<p>nearai hub chat --query \"Explain quantum computing in simple terms\"</p>"},{"location":"api/#nearai.cli.HubCLI--chat-with-a-specific-model-from-a-provider","title":"Chat with a specific model from a provider","text":"<p>nearai hub chat --query \"Write a limerick about AI\" --model claude-3-opus-20240229 --provider fireworks</p>"},{"location":"api/#nearai.cli.HubCLI--display-system-information-about-the-request","title":"Display system information about the request","text":"<p>nearai hub chat --query \"Tell me a joke\" --info</p> Source code in <code>nearai/cli.py</code> <pre><code>class HubCLI:\n    \"\"\"Interact with models hosted on the NEAR AI hub.\n\n    Commands:\n      nearai hub chat : Chat with model from NEAR AI hub\n        (--query, --model, --provider, --endpoint, --info)\n\n    Options:\n      --query (str) :\n        User's query to send to the model\n      --model (str) :\n        Name of the model to use\n      --provider (str) :\n        Name of the provider (e.g., \"fireworks\", \"hyperbolic\")\n      --endpoint (str) :\n        NEAR AI Hub's URL to connect to\n      --info (bool) :\n        Display system information about the request\n\n    Examples:\n      # Chat with the default model\n      nearai hub chat --query \"Explain quantum computing in simple terms\"\n\n      # Chat with a specific model from a provider\n      nearai hub chat --query \"Write a limerick about AI\" --model claude-3-opus-20240229 --provider fireworks\n\n      # Display system information about the request\n      nearai hub chat --query \"Tell me a joke\" --info\n\n    \"\"\"\n\n    def chat(self, **kwargs):\n        \"\"\"Chat with a model from the NEAR AI hub.\n\n        This command allows you to interact with language models hosted on the NEAR AI hub.\n        You can specify which model to use, which provider to use, and customize the chat\n        experience with various parameters.\n\n        Args:\n            **kwargs : (dict)\n                Keyword arguments that can include:\n                query (str):\n                  User's query to send to the model\n                model (str):\n                  Name of the model to use\n                provider (str):\n                  Name of the provider (e.g., \"fireworks\", \"hyperbolic\")\n                endpoint (str):\n                  NEAR AI Hub's URL to connect to\n                info (bool):\n                  Display system information about the request\n                Additional parameters passed to the model\n\n        Examples:\n            # Chat with the default model\n            nearai hub chat --query \"Explain quantum computing in simple terms\"\n\n            # Chat with a specific model from a provider\n            nearai hub chat --query \"Write a limerick about AI\" --model claude-3-opus-20240229 --provider hyperbolic\n\n            # Display system information about the request\n            nearai hub chat --query \"Tell me a joke\" --info\n\n            # Chat with a model using a custom endpoint\n            nearai hub chat --query \"Summarize this text\" --endpoint https://custom-hub.example.com\n\n        \"\"\"\n        from nearai.hub import Hub\n\n        hub = Hub(CONFIG)\n        hub.chat(kwargs)\n\n    def __call__(self) -&gt; None:\n        \"\"\"Show help when 'nearai hub' is called without subcommands.\"\"\"\n        custom_args = [\"nearai\", \"hub\", \"--help\"]\n        handle_help_request(custom_args)\n</code></pre>"},{"location":"api/#nearai.cli.HubCLI.__call__","title":"__call__","text":"<pre><code>__call__() -&gt; None\n</code></pre> <p>Show help when 'nearai hub' is called without subcommands.</p> Source code in <code>nearai/cli.py</code> <pre><code>def __call__(self) -&gt; None:\n    \"\"\"Show help when 'nearai hub' is called without subcommands.\"\"\"\n    custom_args = [\"nearai\", \"hub\", \"--help\"]\n    handle_help_request(custom_args)\n</code></pre>"},{"location":"api/#nearai.cli.HubCLI.chat","title":"chat","text":"<pre><code>chat(**kwargs)\n</code></pre> <p>Chat with a model from the NEAR AI hub.</p> <p>This command allows you to interact with language models hosted on the NEAR AI hub. You can specify which model to use, which provider to use, and customize the chat experience with various parameters.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs </code> <p>(dict) Keyword arguments that can include: query (str):   User's query to send to the model model (str):   Name of the model to use provider (str):   Name of the provider (e.g., \"fireworks\", \"hyperbolic\") endpoint (str):   NEAR AI Hub's URL to connect to info (bool):   Display system information about the request Additional parameters passed to the model</p> required <p>Examples:</p>"},{"location":"api/#nearai.cli.HubCLI.chat--chat-with-the-default-model","title":"Chat with the default model","text":"<p>nearai hub chat --query \"Explain quantum computing in simple terms\"</p>"},{"location":"api/#nearai.cli.HubCLI.chat--chat-with-a-specific-model-from-a-provider","title":"Chat with a specific model from a provider","text":"<p>nearai hub chat --query \"Write a limerick about AI\" --model claude-3-opus-20240229 --provider hyperbolic</p>"},{"location":"api/#nearai.cli.HubCLI.chat--display-system-information-about-the-request","title":"Display system information about the request","text":"<p>nearai hub chat --query \"Tell me a joke\" --info</p>"},{"location":"api/#nearai.cli.HubCLI.chat--chat-with-a-model-using-a-custom-endpoint","title":"Chat with a model using a custom endpoint","text":"<p>nearai hub chat --query \"Summarize this text\" --endpoint https://custom-hub.example.com</p> Source code in <code>nearai/cli.py</code> <pre><code>def chat(self, **kwargs):\n    \"\"\"Chat with a model from the NEAR AI hub.\n\n    This command allows you to interact with language models hosted on the NEAR AI hub.\n    You can specify which model to use, which provider to use, and customize the chat\n    experience with various parameters.\n\n    Args:\n        **kwargs : (dict)\n            Keyword arguments that can include:\n            query (str):\n              User's query to send to the model\n            model (str):\n              Name of the model to use\n            provider (str):\n              Name of the provider (e.g., \"fireworks\", \"hyperbolic\")\n            endpoint (str):\n              NEAR AI Hub's URL to connect to\n            info (bool):\n              Display system information about the request\n            Additional parameters passed to the model\n\n    Examples:\n        # Chat with the default model\n        nearai hub chat --query \"Explain quantum computing in simple terms\"\n\n        # Chat with a specific model from a provider\n        nearai hub chat --query \"Write a limerick about AI\" --model claude-3-opus-20240229 --provider hyperbolic\n\n        # Display system information about the request\n        nearai hub chat --query \"Tell me a joke\" --info\n\n        # Chat with a model using a custom endpoint\n        nearai hub chat --query \"Summarize this text\" --endpoint https://custom-hub.example.com\n\n    \"\"\"\n    from nearai.hub import Hub\n\n    hub = Hub(CONFIG)\n    hub.chat(kwargs)\n</code></pre>"},{"location":"api/#nearai.cli.LoginCLI","title":"LoginCLI","text":"<p>Commands for authenticating with your NEAR account for accessing NEAR AI services.</p> Commands <p>nearai login : Login with NEAR Mainnet account   (--remote, --auth_url, --accountId, --privateKey) nearai login status : Display login status and auth details nearai login save : Save NEAR account auth data   (--accountId, --signature, --publicKey, --callbackUrl, --nonce)</p> Options <p>--remote (bool) :   Enable remote login to sign message with NEAR account on another machine --auth_url (str) :   URL to the authentication portal --accountId (str) :   NEAR account ID in .near-credentials folder to sign message --privateKey (str) :   Private key to sign a message directly --signature (str) :   Signature for manual authentication --publicKey (str) :   Public key used to sign the message --callbackUrl (str) :   Callback URL for the authentication flow --nonce (str) :   Nonce value for authentication security</p> <p>Examples:</p>"},{"location":"api/#nearai.cli.LoginCLI--login-using-web-based-flow","title":"Login using web-based flow","text":"<p>nearai login</p>"},{"location":"api/#nearai.cli.LoginCLI--login-using-credentials-from-near-credentials","title":"Login using credentials from .near-credentials","text":"<p>nearai login --accountId your-account.near</p>"},{"location":"api/#nearai.cli.LoginCLI--login-with-direct-key-less-secure-use-with-caution","title":"Login with direct key (less secure, use with caution)","text":"<p>nearai login --accountId your-account.near --privateKey ed25519:YOUR_PRIVATE_KEY</p>"},{"location":"api/#nearai.cli.LoginCLI--check-current-login-status","title":"Check current login status","text":"<p>nearai login status</p> Source code in <code>nearai/cli.py</code> <pre><code>class LoginCLI:\n    \"\"\"Commands for authenticating with your NEAR account for accessing NEAR AI services.\n\n    Commands:\n      nearai login : Login with NEAR Mainnet account\n        (--remote, --auth_url, --accountId, --privateKey)\n      nearai login status : Display login status and auth details\n      nearai login save : Save NEAR account auth data\n        (--accountId, --signature, --publicKey, --callbackUrl, --nonce)\n\n    Options:\n      --remote (bool) :\n        Enable remote login to sign message with NEAR account on another machine\n      --auth_url (str) :\n        URL to the authentication portal\n      --accountId (str) :\n        NEAR account ID in .near-credentials folder to sign message\n      --privateKey (str) :\n        Private key to sign a message directly\n      --signature (str) :\n        Signature for manual authentication\n      --publicKey (str) :\n        Public key used to sign the message\n      --callbackUrl (str) :\n        Callback URL for the authentication flow\n      --nonce (str) :\n        Nonce value for authentication security\n\n    Examples:\n      # Login using web-based flow\n      nearai login\n\n      # Login using credentials from .near-credentials\n      nearai login --accountId your-account.near\n\n      # Login with direct key (less secure, use with caution)\n      nearai login --accountId your-account.near --privateKey ed25519:YOUR_PRIVATE_KEY\n\n      # Check current login status\n      nearai login status\n\n    \"\"\"\n\n    def __call__(self, **kwargs):\n        \"\"\"Login with NEAR Mainnet account.\"\"\"\n        from nearai.login import generate_and_save_signature, login_with_file_credentials, login_with_near_auth\n\n        remote = kwargs.get(\"remote\", False)\n        account_id = kwargs.get(\"accountId\", None)\n        private_key = kwargs.get(\"privateKey\", None)\n\n        if not remote and account_id and private_key:\n            generate_and_save_signature(account_id, private_key)\n        elif not remote and account_id:\n            login_with_file_credentials(account_id)\n        else:\n            auth_url = kwargs.get(\"auth_url\", \"https://auth.near.ai\")\n            login_with_near_auth(remote, auth_url)\n\n    def status(self):\n        \"\"\"Load NEAR account authorization data.\"\"\"\n        from nearai.login import print_login_status\n\n        print_login_status()\n\n    def save(self, **kwargs):\n        \"\"\"Save NEAR account authorization data.\n\n        Args:\n        ----\n            kwargs (dict) :\n                Keyword arguments that can include:\n                accountId (str) :\n                    Near Account ID\n                signature (str) :\n                    Signature\n                publicKey (str) :\n                    Public Key used to sign the message\n                callbackUrl (str) :\n                    Callback Url\n                nonce (str) :\n                    Nonce\n\n        \"\"\"\n        from nearai.login import update_auth_config\n\n        account_id = kwargs.get(\"accountId\")\n        signature = kwargs.get(\"signature\")\n        public_key = kwargs.get(\"publicKey\")\n        callback_url = kwargs.get(\"callbackUrl\")\n        nonce = kwargs.get(\"nonce\")\n\n        if account_id and signature and public_key and callback_url and nonce:\n            update_auth_config(account_id, signature, public_key, callback_url, nonce)\n        else:\n            print(\"Missing data\")\n</code></pre>"},{"location":"api/#nearai.cli.LoginCLI.__call__","title":"__call__","text":"<pre><code>__call__(**kwargs)\n</code></pre> <p>Login with NEAR Mainnet account.</p> Source code in <code>nearai/cli.py</code> <pre><code>def __call__(self, **kwargs):\n    \"\"\"Login with NEAR Mainnet account.\"\"\"\n    from nearai.login import generate_and_save_signature, login_with_file_credentials, login_with_near_auth\n\n    remote = kwargs.get(\"remote\", False)\n    account_id = kwargs.get(\"accountId\", None)\n    private_key = kwargs.get(\"privateKey\", None)\n\n    if not remote and account_id and private_key:\n        generate_and_save_signature(account_id, private_key)\n    elif not remote and account_id:\n        login_with_file_credentials(account_id)\n    else:\n        auth_url = kwargs.get(\"auth_url\", \"https://auth.near.ai\")\n        login_with_near_auth(remote, auth_url)\n</code></pre>"},{"location":"api/#nearai.cli.LoginCLI.save","title":"save","text":"<pre><code>save(**kwargs)\n</code></pre> <p>Save NEAR account authorization data.</p> <pre><code>kwargs (dict) :\n    Keyword arguments that can include:\n    accountId (str) :\n        Near Account ID\n    signature (str) :\n        Signature\n    publicKey (str) :\n        Public Key used to sign the message\n    callbackUrl (str) :\n        Callback Url\n    nonce (str) :\n        Nonce\n</code></pre> Source code in <code>nearai/cli.py</code> <pre><code>def save(self, **kwargs):\n    \"\"\"Save NEAR account authorization data.\n\n    Args:\n    ----\n        kwargs (dict) :\n            Keyword arguments that can include:\n            accountId (str) :\n                Near Account ID\n            signature (str) :\n                Signature\n            publicKey (str) :\n                Public Key used to sign the message\n            callbackUrl (str) :\n                Callback Url\n            nonce (str) :\n                Nonce\n\n    \"\"\"\n    from nearai.login import update_auth_config\n\n    account_id = kwargs.get(\"accountId\")\n    signature = kwargs.get(\"signature\")\n    public_key = kwargs.get(\"publicKey\")\n    callback_url = kwargs.get(\"callbackUrl\")\n    nonce = kwargs.get(\"nonce\")\n\n    if account_id and signature and public_key and callback_url and nonce:\n        update_auth_config(account_id, signature, public_key, callback_url, nonce)\n    else:\n        print(\"Missing data\")\n</code></pre>"},{"location":"api/#nearai.cli.LoginCLI.status","title":"status","text":"<pre><code>status()\n</code></pre> <p>Load NEAR account authorization data.</p> Source code in <code>nearai/cli.py</code> <pre><code>def status(self):\n    \"\"\"Load NEAR account authorization data.\"\"\"\n    from nearai.login import print_login_status\n\n    print_login_status()\n</code></pre>"},{"location":"api/#nearai.cli.LogoutCLI","title":"LogoutCLI","text":"<p>Clear your NEAR account authentication data from the local configuration.</p> Commands <p>nearai logout : Logout and remove authentication data</p> <p>Examples:</p>"},{"location":"api/#nearai.cli.LogoutCLI--remove-authentication-data","title":"Remove authentication data","text":"<p>nearai logout</p> Source code in <code>nearai/cli.py</code> <pre><code>class LogoutCLI:\n    \"\"\"Clear your NEAR account authentication data from the local configuration.\n\n    Commands:\n      nearai logout : Logout and remove authentication data\n\n    Examples:\n      # Remove authentication data\n      nearai logout\n\n    \"\"\"\n\n    def __call__(self, **kwargs):\n        \"\"\"Clear NEAR account auth data.\"\"\"\n        from nearai.config import load_config_file, save_config_file\n\n        config = load_config_file()\n        if not config.get(\"auth\") or not config[\"auth\"].get(\"account_id\"):\n            print(\"Auth data does not exist.\")\n        else:\n            config.pop(\"auth\", None)\n            save_config_file(config)\n            print(\"Auth data removed\")\n</code></pre>"},{"location":"api/#nearai.cli.LogoutCLI.__call__","title":"__call__","text":"<pre><code>__call__(**kwargs)\n</code></pre> <p>Clear NEAR account auth data.</p> Source code in <code>nearai/cli.py</code> <pre><code>def __call__(self, **kwargs):\n    \"\"\"Clear NEAR account auth data.\"\"\"\n    from nearai.config import load_config_file, save_config_file\n\n    config = load_config_file()\n    if not config.get(\"auth\") or not config[\"auth\"].get(\"account_id\"):\n        print(\"Auth data does not exist.\")\n    else:\n        config.pop(\"auth\", None)\n        save_config_file(config)\n        print(\"Auth data removed\")\n</code></pre>"},{"location":"api/#nearai.cli.PermissionCli","title":"PermissionCli","text":"<p>Commands for managing permissions and access control for NEAR AI resources.</p> Commands <p>nearai permission grant : Grant permission to an account   (account_id*, permission*) nearai permission revoke : Revoke permission from an account   (account_id*, --permission)</p> Options <p>account_id (str) :   The NEAR account ID to grant or revoke permissions for permission (str) :   The permission to grant or revoke (leave empty on revoke to remove all permissions)</p> <p>Examples:</p>"},{"location":"api/#nearai.cli.PermissionCli--grant-model-access-permission-to-an-account","title":"Grant model access permission to an account","text":"<p>nearai permission grant alice.near model_access</p>"},{"location":"api/#nearai.cli.PermissionCli--grant-multiple-permissions-run-multiple-commands","title":"Grant multiple permissions (run multiple commands)","text":"<p>nearai permission grant bob.near agent_creation nearai permission grant bob.near model_access</p>"},{"location":"api/#nearai.cli.PermissionCli--revoke-a-specific-permission","title":"Revoke a specific permission","text":"<p>nearai permission revoke charlie.near model_access</p>"},{"location":"api/#nearai.cli.PermissionCli--revoke-all-permissions-from-an-account","title":"Revoke all permissions from an account","text":"<p>nearai permission revoke dave.near</p> Source code in <code>nearai/cli.py</code> <pre><code>class PermissionCli:\n    \"\"\"Commands for managing permissions and access control for NEAR AI resources.\n\n    Commands:\n      nearai permission grant : Grant permission to an account\n        (account_id*, permission*)\n      nearai permission revoke : Revoke permission from an account\n        (account_id*, --permission)\n\n    Options:\n      account_id (str) :\n        The NEAR account ID to grant or revoke permissions for\n      permission (str) :\n        The permission to grant or revoke (leave empty on revoke to remove all permissions)\n\n    Examples:\n      # Grant model access permission to an account\n      nearai permission grant alice.near model_access\n\n      # Grant multiple permissions (run multiple commands)\n      nearai permission grant bob.near agent_creation\n      nearai permission grant bob.near model_access\n\n      # Revoke a specific permission\n      nearai permission revoke charlie.near model_access\n\n      # Revoke all permissions from an account\n      nearai permission revoke dave.near\n\n    \"\"\"\n\n    def __init__(self) -&gt; None:  # noqa: D107\n        self.client = PermissionsApi()\n\n    def grant(self, account_id: str, permission: str):\n        \"\"\"Grant a specific permission to a NEAR account.\n\n        This command allows you to grant a specific permission to a NEAR account, enabling\n        them to access certain NEAR AI resources or perform specific actions.\n\n        Args:\n        ----\n           account_id : (str)\n               The NEAR account ID to grant the permission to\n           permission : (str)\n               The permission to grant (e.g., 'model_access', 'agent_creation')\n\n        Examples:\n        --------\n            # Grant model access permission to an account\n            nearai permission grant alice.near model_access\n\n            # Grant agent creation permission\n            nearai permission grant bob.near agent_creation\n\n            # Grant evaluation access permission\n            nearai permission grant charlie.near evaluation_access\n\n        \"\"\"\n        self.client.grant_permission_v1_permissions_grant_permission_post(account_id, permission)\n\n    def revoke(self, account_id: str, permission: str = \"\"):\n        \"\"\"Revoke permissions from a NEAR account.\n\n        If no permission is specified, all permissions will be revoked from the account.\n\n        Args:\n        ----\n            account_id : (str)\n                The NEAR account ID to revoke the permission from\n            permission : (str)\n                The permission to revoke (optional, if empty all permissions are revoked)\n\n        Examples:\n        --------\n            # Revoke a specific permission\n            nearai permission revoke alice.near model_access\n\n            # Revoke all permissions from an account\n            nearai permission revoke bob.near\n\n            # Revoke agent creation permission\n            nearai permission revoke charlie.near agent_creation\n\n        \"\"\"\n        self.client.revoke_permission_v1_permissions_revoke_permission_post(account_id, permission)\n\n    def __call__(self) -&gt; None:\n        \"\"\"Show help when 'nearai permission' is called without subcommands.\"\"\"\n        custom_args = [\"nearai\", \"permission\", \"--help\"]\n        handle_help_request(custom_args)\n</code></pre>"},{"location":"api/#nearai.cli.PermissionCli.__call__","title":"__call__","text":"<pre><code>__call__() -&gt; None\n</code></pre> <p>Show help when 'nearai permission' is called without subcommands.</p> Source code in <code>nearai/cli.py</code> <pre><code>def __call__(self) -&gt; None:\n    \"\"\"Show help when 'nearai permission' is called without subcommands.\"\"\"\n    custom_args = [\"nearai\", \"permission\", \"--help\"]\n    handle_help_request(custom_args)\n</code></pre>"},{"location":"api/#nearai.cli.PermissionCli.grant","title":"grant","text":"<pre><code>grant(account_id: str, permission: str)\n</code></pre> <p>Grant a specific permission to a NEAR account.</p> <p>This command allows you to grant a specific permission to a NEAR account, enabling them to access certain NEAR AI resources or perform specific actions.</p> <p>account_id : (str)        The NEAR account ID to grant the permission to    permission : (str)        The permission to grant (e.g., 'model_access', 'agent_creation')</p> <p>Examples:</p> <pre><code># Grant model access permission to an account\nnearai permission grant alice.near model_access\n\n# Grant agent creation permission\nnearai permission grant bob.near agent_creation\n\n# Grant evaluation access permission\nnearai permission grant charlie.near evaluation_access\n</code></pre> Source code in <code>nearai/cli.py</code> <pre><code>def grant(self, account_id: str, permission: str):\n    \"\"\"Grant a specific permission to a NEAR account.\n\n    This command allows you to grant a specific permission to a NEAR account, enabling\n    them to access certain NEAR AI resources or perform specific actions.\n\n    Args:\n    ----\n       account_id : (str)\n           The NEAR account ID to grant the permission to\n       permission : (str)\n           The permission to grant (e.g., 'model_access', 'agent_creation')\n\n    Examples:\n    --------\n        # Grant model access permission to an account\n        nearai permission grant alice.near model_access\n\n        # Grant agent creation permission\n        nearai permission grant bob.near agent_creation\n\n        # Grant evaluation access permission\n        nearai permission grant charlie.near evaluation_access\n\n    \"\"\"\n    self.client.grant_permission_v1_permissions_grant_permission_post(account_id, permission)\n</code></pre>"},{"location":"api/#nearai.cli.PermissionCli.revoke","title":"revoke","text":"<pre><code>revoke(account_id: str, permission: str = '')\n</code></pre> <p>Revoke permissions from a NEAR account.</p> <p>If no permission is specified, all permissions will be revoked from the account.</p> <pre><code>account_id : (str)\n    The NEAR account ID to revoke the permission from\npermission : (str)\n    The permission to revoke (optional, if empty all permissions are revoked)\n</code></pre> <p>Examples:</p> <pre><code># Revoke a specific permission\nnearai permission revoke alice.near model_access\n\n# Revoke all permissions from an account\nnearai permission revoke bob.near\n\n# Revoke agent creation permission\nnearai permission revoke charlie.near agent_creation\n</code></pre> Source code in <code>nearai/cli.py</code> <pre><code>def revoke(self, account_id: str, permission: str = \"\"):\n    \"\"\"Revoke permissions from a NEAR account.\n\n    If no permission is specified, all permissions will be revoked from the account.\n\n    Args:\n    ----\n        account_id : (str)\n            The NEAR account ID to revoke the permission from\n        permission : (str)\n            The permission to revoke (optional, if empty all permissions are revoked)\n\n    Examples:\n    --------\n        # Revoke a specific permission\n        nearai permission revoke alice.near model_access\n\n        # Revoke all permissions from an account\n        nearai permission revoke bob.near\n\n        # Revoke agent creation permission\n        nearai permission revoke charlie.near agent_creation\n\n    \"\"\"\n    self.client.revoke_permission_v1_permissions_revoke_permission_post(account_id, permission)\n</code></pre>"},{"location":"api/#nearai.cli.RegistryCli","title":"RegistryCli","text":"<p>Manage items in the NEAR AI Registry including agents, models, datasets, and evaluations.</p> <p>These commands allow you to upload, download, update, and list available items in the NEAR AI Registry.</p> Commands <p>nearai registry upload : Upload an item to the registry   () nearai registry download : Download an item from the registry   (, --force) nearai registry info : Show information about a registry item   () nearai registry list : List available items in the registry   (--namespace, --category, --tags, --total, --offset, --show-all, --show-latest-version, --star) nearai registry metadata-template : Create a metadata template   (--local-path, --category, --description) nearai registry update : Update metadata of a registry item   () Options <p>* (str) :   Path to the directory containing the agent to upload * (str) :   Entry location of the item to download (format: namespace/name/version) --force (bool) :   Force download even if the item exists locally --namespace (str) :   Filter items by namespace --category (str) :   Filter items by category (e.g., 'agent', 'model') --tags (str) :   Filter items by tags (comma-separated) --total (int) :   Maximum number of items to show --offset (int) :   Offset for pagination --show-all (bool) :   Show all versions of items --show-latest-version (bool) :   Show only the latest version of each item --star (str) :   Show items starred by a specific user <p>Examples:</p>"},{"location":"api/#nearai.cli.RegistryCli--upload-an-agent-to-the-registry","title":"Upload an agent to the registry","text":"<p>nearai registry upload ./path/to/agent</p>"},{"location":"api/#nearai.cli.RegistryCli--download-an-agent-from-the-registry","title":"Download an agent from the registry","text":"<p>nearai registry download example.near/agent-name/0.0.3</p>"},{"location":"api/#nearai.cli.RegistryCli--show-information-about-a-registry-item","title":"Show information about a registry item","text":"<p>nearai registry info example.near/agent-name/0.0.3</p>"},{"location":"api/#nearai.cli.RegistryCli--list-items-by-category","title":"List items by category","text":"<p>nearai registry list --category evaluation</p>"},{"location":"api/#nearai.cli.RegistryCli--list-items-with-specific-tags","title":"List items with specific tags","text":"<p>nearai registry list --tags \"vector-store\"</p> Documentation <p>https://docs.near.ai/agents/registry</p> Source code in <code>nearai/cli.py</code> <pre><code>class RegistryCli:\n    \"\"\"Manage items in the NEAR AI Registry including agents, models, datasets, and evaluations.\n\n    These commands allow you to upload, download, update, and list available items in the NEAR AI Registry.\n\n    Commands:\n      nearai registry upload : Upload an item to the registry\n        (&lt;path&gt;*)\n      nearai registry download : Download an item from the registry\n        (&lt;entry-location&gt;*, --force)\n      nearai registry info : Show information about a registry item\n        (&lt;entry-location&gt;*)\n      nearai registry list : List available items in the registry\n        (--namespace, --category, --tags, --total, --offset, --show-all, --show-latest-version, --star)\n      nearai registry metadata-template : Create a metadata template\n        (--local-path, --category, --description)\n      nearai registry update : Update metadata of a registry item\n        (&lt;path&gt;*)\n\n    Options:\n      &lt;path&gt;* (str) :\n        Path to the directory containing the agent to upload\n      &lt;entry-location&gt;* (str) :\n        Entry location of the item to download (format: namespace/name/version)\n      --force (bool) :\n        Force download even if the item exists locally\n      --namespace (str) :\n        Filter items by namespace\n      --category (str) :\n        Filter items by category (e.g., 'agent', 'model')\n      --tags (str) :\n        Filter items by tags (comma-separated)\n      --total (int) :\n        Maximum number of items to show\n      --offset (int) :\n        Offset for pagination\n      --show-all (bool) :\n        Show all versions of items\n      --show-latest-version (bool) :\n        Show only the latest version of each item\n      --star (str) :\n        Show items starred by a specific user\n\n    Examples:\n      # Upload an agent to the registry\n      nearai registry upload ./path/to/agent\n\n      # Download an agent from the registry\n      nearai registry download example.near/agent-name/0.0.3\n\n      # Show information about a registry item\n      nearai registry info example.near/agent-name/0.0.3\n\n      # List items by category\n      nearai registry list --category evaluation\n\n      # List items with specific tags\n      nearai registry list --tags \"vector-store\"\n\n    Documentation:\n        https://docs.near.ai/agents/registry\n\n    \"\"\"\n\n    def info(self, entry: str) -&gt; None:\n        \"\"\"Display detailed information about a registry item.\n\n        Includes metadata and available provider matches for models.\n\n        Args:\n          entry (str) :\n            Entry location of the item to display information for (format: namespace/name/version)\n\n        Examples:\n          # Show information about a specific registry item\n          nearai registry info example.near/agent-name/0.0.3\n\n          # Show information about a model\n          nearai registry info example.near/model-name/1.0.0\n\n        Documentation:\n          https://docs.near.ai/agents/registry\n\n        \"\"\"\n        entry_location = parse_location(entry)\n        metadata = registry.info(entry_location)\n\n        if metadata is None:\n            print(f\"Entry {entry} not found.\")\n            return\n\n        print(metadata.model_dump_json(indent=2))\n        if metadata.category == \"model\":\n            available_provider_matches = ProviderModels(CONFIG.get_client_config()).available_provider_matches(\n                NamespacedName(name=metadata.name, namespace=entry_location.namespace)\n            )\n            if len(available_provider_matches) &gt; 0:\n                header = [\"provider\", \"name\"]\n\n                table = []\n                for provider, name in available_provider_matches.items():\n                    table.append(\n                        [\n                            fill(provider),\n                            fill(name),\n                        ]\n                    )\n                print(tabulate(table, headers=header, tablefmt=\"simple_grid\"))\n\n    def metadata_template(self, local_path: str = \".\", category: str = \"\", description: str = \"\"):\n        \"\"\"Create a metadata template file for a registry item.\n\n        This generates a properly formatted metadata.json file with default values\n        that can be customized for your agent or model.\n\n        Args:\n          local_path : str\n            Path to the directory where the metadata template will be created\n          category : str\n            Category of the item (e.g., 'agent', 'model', 'dataset', 'evaluation')\n          description : str\n            Description of the item\n\n        Examples:\n          # Create a metadata template in the current directory\n          nearai registry metadata-template\n\n          # Create a metadata template for an agent with description\n          nearai registry metadata-template --category agent --description \"My helpful assistant\"\n\n          # Create a metadata template in a specific directory\n          nearai registry metadata-template path/to/directory --category model\n\n        \"\"\"\n        path = resolve_local_path(Path(local_path))\n\n        metadata_path = path / \"metadata.json\"\n\n        version = path.name\n        # Validate version format\n        is_valid, error = validate_version(version)\n        if not is_valid:\n            print(error)\n            return\n\n        name = path.parent.name\n        assert not re.match(PATTERN, name), f\"Invalid agent name: {name}\"\n        assert \" \" not in name\n\n        with open(metadata_path, \"w\") as f:\n            metadata: Dict[str, Any] = {\n                \"name\": name,\n                \"version\": version,\n                \"description\": description,\n                \"category\": category,\n                \"tags\": [],\n                \"details\": {},\n                \"show_entry\": True,\n            }\n\n            if category == \"agent\":\n                metadata[\"details\"][\"agent\"] = {}\n                metadata[\"details\"][\"agent\"][\"welcome\"] = {\n                    \"title\": name,\n                    \"description\": description,\n                }\n                metadata[\"details\"][\"agent\"][\"defaults\"] = {\n                    \"model\": DEFAULT_MODEL,\n                    \"model_provider\": DEFAULT_PROVIDER,\n                    \"model_temperature\": DEFAULT_MODEL_TEMPERATURE,\n                    \"model_max_tokens\": DEFAULT_MODEL_MAX_TOKENS,\n                }\n                metadata[\"details\"][\"agent\"][\"framework\"] = \"minimal\"\n\n            json.dump(metadata, f, indent=2)\n\n    def list(\n        self,\n        namespace: str = \"\",\n        category: str = \"\",\n        tags: str = \"\",\n        total: int = 32,\n        offset: int = 0,\n        show_all: bool = False,\n        show_latest_version: bool = True,\n        star: str = \"\",\n    ) -&gt; None:\n        \"\"\"List available items in the NEAR AI registry.\n\n        You can filter the results by namespace, category, tags, and other criteria to find specific items.\n\n        Args:\n          namespace (str) :\n            Filter items by namespace/user account (example.near)\n          category (str) :\n            Filter items by category ('agent', 'model', 'evaluation')\n          tags (str) :\n            Filter items by tags (comma-separated)\n          total (int) :\n            Maximum number of items to show\n          offset (int) :\n            Offset for pagination\n          show_all (bool) :\n            Show all versions of items\n          show_latest_version (bool) :\n            Show only the latest version of each item\n          star (str) :\n            Show items starred by a specific user\n\n        Examples:\n          # List all items in the registry\n          nearai registry list\n\n          # List agents in the registry\n          nearai registry list --category agent\n\n          # List items with specific tags\n          nearai registry list --tags \"summarization,text\"\n\n          # List items from a specific namespace\n          nearai registry list --namespace example.near\n\n          # Show all versions of items\n          nearai registry list --show-all\n\n        \"\"\"\n        # Check if we're listing models specifically and provide alternative options\n        if category.lower() == \"model\":\n            print(\"\\n\u26a0\ufe0f  The category=model entries in the registry are deprecated.\")\n            print(\"We recommend using Fireworks, OpenAI, Anthropic, Crynux, or other provider models instead.\")\n            print(\"\\nEnter one of the following options:\")\n            print(\"- registry        : See the registry models anyway\")\n            print(\"- nearai          : List models from NEAR AI endpoint (Fireworks models, free inference)\")\n            print(\"- openai          : List available OpenAI models. Requires OPENAI_API_KEY.\")\n            print(\"- anthropic       : List available Anthropic models. Requires ANTHROPIC_API_KEY.\")\n            print(\n                \"- crynux          : List available Crynux models (framework support, public API key, low rate limit)\"\n            )\n            print(\"(May require api keys or correct moon phase)\")\n            print(\"\")\n\n            choice = input(\"\\nYour choice: \").strip().lower()\n\n            if choice != \"registry\":\n                models = fetch_models_from_provider(choice)\n                for model in models:\n                    print(f\"{model}\")\n                return\n\n        # Make sure tags is a comma-separated list of tags\n        tags_l = parse_tags(tags)\n        tags = \",\".join(tags_l)\n\n        entries = registry.list(\n            namespace=namespace,\n            category=category,\n            tags=tags,\n            total=total + 1,\n            offset=offset,\n            show_all=show_all,\n            show_latest_version=show_latest_version,\n            starred_by=star,\n        )\n\n        more_rows = len(entries) &gt; total\n        entries = entries[:total]\n\n        header = [\"entry\", \"category\", \"description\", \"tags\"]\n\n        table = []\n        for entry in entries:\n            table.append(\n                [\n                    fill(f\"{entry.namespace}/{entry.name}/{entry.version}\"),\n                    fill(entry.category, 20),\n                    fill(entry.description, 50),\n                    fill(\", \".join(entry.tags), 20),\n                ]\n            )\n\n        if more_rows:\n            table.append([\"...\", \"...\", \"...\", \"...\"])\n\n        print(tabulate(table, headers=header, tablefmt=\"simple_grid\"))\n\n        if category == \"model\" and len(entries) &lt; total and namespace == \"\" and tags == \"\" and star == \"\":\n            unregistered_common_provider_models = ProviderModels(\n                CONFIG.get_client_config()\n            ).get_unregistered_common_provider_models(registry.dict_models())\n            if len(unregistered_common_provider_models):\n                print(\n                    f\"There are unregistered common provider models: {unregistered_common_provider_models}. Run 'nearai registry upload-unregistered-common-provider-models' to update registry.\"  # noqa: E501\n                )\n\n    def update(self, local_path: str = \".\") -&gt; None:\n        \"\"\"Update the remote metadata of an item in the NEAR AI Registry.\n\n        Looks for a metadata.json file in the given directory and updates the remote metadata with the new values.\n\n        Args:\n          local_path (str) :\n            Path to the directory containing the item to update\n\n        Examples:\n          # Update metadata for the item in the current directory\n          nearai registry update\n\n          # Update metadata for a specific item\n          nearai registry update path/to/item\n\n        \"\"\"\n        path = resolve_local_path(Path(local_path))\n\n        if CONFIG.auth is None:\n            print(\"Please login with `nearai login`\")\n            exit(1)\n\n        metadata_path = path / \"metadata.json\"\n        check_metadata_present(metadata_path)\n\n        with open(metadata_path) as f:\n            metadata: Dict[str, Any] = json.load(f)\n\n        namespace = get_namespace(path)\n\n        entry_location = EntryLocation.model_validate(\n            dict(\n                namespace=namespace,\n                name=metadata.pop(\"name\"),\n                version=metadata.pop(\"version\"),\n            )\n        )\n        assert \" \" not in entry_location.name\n\n        entry_metadata = EntryMetadataInput.model_validate(metadata)\n        result = registry.update(entry_location, entry_metadata)\n        print(json.dumps(result, indent=2))\n\n    def upload_unregistered_common_provider_models(self, dry_run: bool = True) -&gt; None:\n        \"\"\"Create new registry items for unregistered common provider models.\n\n        This command helps keep the registry up-to-date with the latest models from various providers.\n\n        Args:\n          dry_run (bool) :\n            Perform a dry run without actually uploading\n\n        Examples:\n          # Perform a dry run to see what would be uploaded\n          nearai registry upload-unregistered-common-provider-models\n\n          # Actually upload the unregistered models\n          nearai registry upload-unregistered-common-provider-models --dry-run=False\n\n        \"\"\"\n        provider_matches_list = ProviderModels(CONFIG.get_client_config()).get_unregistered_common_provider_models(\n            registry.dict_models()\n        )\n        if len(provider_matches_list) == 0:\n            print(\"No new models to upload.\")\n            return\n\n        print(\"Going to create new registry items:\")\n        header = [\"entry\", \"description\"]\n        table = []\n        paths = []\n        for provider_matches in provider_matches_list:\n            provider_model = provider_matches.get(DEFAULT_PROVIDER) or next(iter(provider_matches.values()))\n            _, model = get_provider_namespaced_model(provider_model)\n            assert model.namespace == \"\"\n            model.name = create_registry_name(model.name)\n            model.namespace = DEFAULT_NAMESPACE\n            version = \"1.0.0\"\n            description = \" &amp; \".join(provider_matches.values())\n            table.append(\n                [\n                    fill(f\"{model.namespace}/{model.name}/{version}\"),\n                    fill(description, 50),\n                ]\n            )\n\n            path = get_registry_folder() / model.namespace / model.name / version\n            path.mkdir(parents=True, exist_ok=True)\n            paths.append(path)\n            metadata_path = path / \"metadata.json\"\n            with open(metadata_path, \"w\") as f:\n                metadata: Dict[str, Any] = {\n                    \"name\": model.name,\n                    \"version\": version,\n                    \"description\": description,\n                    \"category\": \"model\",\n                    \"tags\": [],\n                    \"details\": {},\n                    \"show_entry\": True,\n                }\n                json.dump(metadata, f, indent=2)\n\n        print(tabulate(table, headers=header, tablefmt=\"simple_grid\"))\n        if dry_run:\n            print(\"Please verify, then repeat the command with --dry_run=False\")\n        else:\n            for path in paths:\n                self.upload(str(path))\n\n    def upload(\n        self,\n        local_path: str,\n        bump: bool = False,\n        minor_bump: bool = False,\n        major_bump: bool = False,\n        encrypt: bool = False,\n    ) -&gt; Optional[EntryLocation]:\n        \"\"\"Upload an item to the NEAR AI registry for public use.\n\n        Args:\n          local_path (str) :\n            Path to the agent directory\n          bump (bool) :\n            Automatically increment patch version if it already exists\n          minor_bump (bool) :\n            Bump with minor version increment (0.1.0 \u2192 0.2.0)\n          major_bump (bool) :\n            Bump with major version increment (1.5.2 \u2192 2.0.0)\n          encrypt (bool) :\n            Encrypt all uploaded files using a generated encryption key\n\n        Examples:\n          # Upload an item in the current directory\n          nearai registry upload .\n\n          # Upload a specific agent directory\n          nearai registry upload ./path/to/item\n\n          # Upload with automatic version bumping\n          nearai registry upload ./path/to/item --bump\n\n          # Upload with minor version bump\n          nearai registry upload ./path/to/item --minor-bump\n\n          # Upload with encryption enabled (private entries)\n          nearai registry upload ./path/to/item --encrypt\n\n        \"\"\"\n        console = Console()\n        path = resolve_local_path(Path(local_path))\n        metadata_path = path / \"metadata.json\"\n\n        # Load and validate metadata\n        metadata, error = load_and_validate_metadata(metadata_path)\n        if error:\n            console.print(\n                Panel(Text(error, style=\"bold red\"), title=\"Metadata Error\", border_style=\"red\", padding=(1, 2))\n            )\n            return None\n\n        # At this point, metadata is guaranteed to be not None\n        assert metadata is not None, \"Metadata should not be None if error is None\"\n\n        name = metadata[\"name\"]\n        version = metadata[\"version\"]\n\n        # Handle encryption key generation if --encrypt flag is used\n        if encrypt:\n            # Initialize details if not present\n            if \"details\" not in metadata:\n                metadata[\"details\"] = {}\n\n            # Generate encryption key if not present\n            if \"encryption_key\" not in metadata[\"details\"]:\n                encryption_key = FileEncryption.generate_encryption_key()\n                metadata[\"details\"][\"encryption_key\"] = encryption_key\n\n                # Update metadata.json file with the new encryption key\n                with open(metadata_path, \"w\") as f:\n                    json.dump(metadata, f, indent=2)\n\n                console.print(\n                    Panel(\n                        Text.assemble(\n                            (\"\ud83d\udd10 Encryption enabled\\n\\n\", \"bold green\"),\n                            (f\"Encryption key {encryption_key} generated and stored in metadata.json\\n\", \"dim\"),\n                        ),\n                        title=\"Encryption\",\n                        border_style=\"green\",\n                        padding=(1, 2),\n                    )\n                )\n\n        # Get namespace using the function from registry.py\n        try:\n            namespace = get_namespace(path)\n        except ValueError:\n            console.print(\n                Panel(\n                    Text(\"Please login with `nearai login` before uploading\", style=\"bold red\"),\n                    title=\"Authentication Error\",\n                    border_style=\"red\",\n                    padding=(1, 2),\n                )\n            )\n            return None\n\n        # Check if this version already exists\n        exists, error = check_version_exists(namespace, name, version)\n\n        if error:\n            console.print(\n                Panel(Text(error, style=\"bold red\"), title=\"Registry Error\", border_style=\"red\", padding=(1, 2))\n            )\n            return None\n\n        # Display the version check result\n        display_version_check(namespace, name, version, exists)\n\n        bump_requested = bump or minor_bump or major_bump\n\n        if exists and bump_requested:\n            # Handle version bump\n            old_version = version\n\n            # Determine increment type based on flags\n            if major_bump:\n                increment_type = \"major\"\n            elif minor_bump:\n                increment_type = \"minor\"\n            else:\n                increment_type = \"patch\"  # Default for bump\n\n            version = increment_version_by_type(version, increment_type)\n\n            # Enhanced version update message\n            update_panel = Panel(\n                Text.assemble(\n                    (\"Updating Version...\\n\\n\", \"bold\"),\n                    (\"Previous version: \", \"dim\"),\n                    (f\"{old_version}\\n\", \"yellow\"),\n                    (\"New version:     \", \"dim\"),\n                    (f\"{version}\", \"green bold\"),\n                    (\"\\n\\nIncrement type: \", \"dim\"),\n                    (f\"{increment_type}\", \"cyan\"),\n                ),\n                title=\"Bump\",\n                border_style=\"green\",\n                padding=(1, 2),\n            )\n            console.print(update_panel)\n\n            # Update metadata.json with new version\n            metadata[\"version\"] = version\n            with open(metadata_path, \"w\") as f:\n                json.dump(metadata, f, indent=2)\n\n            console.print(f\"\\n\u2705 Updated [bold]{metadata_path}[/bold] with new version\\n\")\n            console.print(Rule(style=\"dim\"))\n\n        elif exists and not bump_requested:\n            # Show error panel for version conflict\n            error_panel = Panel(\n                Text.assemble(\n                    (\"To upload a new version:\\n\", \"yellow\"),\n                    (f\"1. Edit {metadata_path}\\n\", \"dim\"),\n                    ('2. Update the \"version\" field (e.g., increment from \"0.0.1\" to \"0.0.2\")\\n', \"dim\"),\n                    (\"3. Try uploading again\\n\\n\", \"dim\"),\n                    (\"Or use the following flags:\\n\", \"yellow\"),\n                    (\"  --bump          # Patch update (0.0.1 \u2192 0.0.2)\\n\", \"green\"),\n                    (\"  --minor-bump    # Minor update (0.0.1 \u2192 0.1.0)\\n\", \"green\"),\n                    (\"  --major-bump    # Major update (0.1.0 \u2192 1.0.0)\\n\", \"green\"),\n                ),\n                title=\"Version Conflict\",\n                border_style=\"red\",\n            )\n            console.print(error_panel)\n            return None\n\n        # Version doesn't exist or has been bumped, proceed with upload\n        console.print(\n            f\"\\n\ud83d\udcc2 [bold]Uploading[/bold] version [green bold]{version}[/green bold] of [blue bold]{name}[/blue bold] to [cyan bold]{namespace}[/cyan bold]...\\n\"  # noqa: E501\n        )\n\n        try:\n            result = registry.upload(path, show_progress=True, encrypt=encrypt)\n\n            if result:\n                success_panel = Panel(\n                    Text.assemble(\n                        (\"Upload completed successfully! \ud83d\ude80 \\n\\n\", \"bold green\"),\n                        (\"Name:      \", \"dim\"),\n                        (f\"{result.name}\\n\", \"cyan\"),\n                        (\"Version:   \", \"dim\"),\n                        (f\"{result.version}\\n\", \"cyan\"),\n                        (\"Namespace: \", \"dim\"),\n                        (f\"{result.namespace}\", \"cyan\"),\n                    ),\n                    title=\"Success\",\n                    border_style=\"green\",\n                    padding=(1, 2),\n                )\n                console.print(success_panel)\n                return result\n            else:\n                console.print(\n                    Panel(\n                        Text(\"Upload failed for unknown reasons\", style=\"bold red\"),\n                        title=\"Upload Error\",\n                        border_style=\"red\",\n                        padding=(1, 2),\n                    )\n                )\n                return None\n\n        except Exception as e:\n            console.print(\n                Panel(\n                    Text(f\"Error during upload: {str(e)}\", style=\"bold red\"),\n                    title=\"Upload Error\",\n                    border_style=\"red\",\n                    padding=(1, 2),\n                )\n            )\n            return None\n\n    def download(self, entry_location: str, force: bool = False, encryption_key: Optional[str] = None) -&gt; None:\n        \"\"\"Download an item from the NEAR AI registry to your local machine.\n\n        This allows you to use or inspect agents, models, datasets, etc. that have been published by others.\n\n        Args:\n          entry_location (str) :\n            Entry location of the item to download (format: namespace/name/version)\n          force (bool) :\n            Force download even if the item already exists locally\n          encryption_key (str) :\n            Decrypt files with this encryption key\n\n        Examples:\n          # Download a specific registry item\n          nearai registry download example.near/agent-name/0.0.3\n\n          # Force download an item that already exists locally\n          nearai registry download example.near/model-name/1.0.0 --force\n\n        \"\"\"\n        registry.download(entry_location, force=force, show_progress=True, encryption_key=encryption_key)\n\n    def __call__(self):\n        \"\"\"Show help when 'nearai registry' is called without subcommands.\"\"\"\n        custom_args = [\"nearai\", \"registry\", \"--help\"]\n        handle_help_request(custom_args)\n</code></pre>"},{"location":"api/#nearai.cli.RegistryCli.__call__","title":"__call__","text":"<pre><code>__call__()\n</code></pre> <p>Show help when 'nearai registry' is called without subcommands.</p> Source code in <code>nearai/cli.py</code> <pre><code>def __call__(self):\n    \"\"\"Show help when 'nearai registry' is called without subcommands.\"\"\"\n    custom_args = [\"nearai\", \"registry\", \"--help\"]\n    handle_help_request(custom_args)\n</code></pre>"},{"location":"api/#nearai.cli.RegistryCli.download","title":"download","text":"<pre><code>download(entry_location: str, force: bool = False, encryption_key: Optional[str] = None) -&gt; None\n</code></pre> <p>Download an item from the NEAR AI registry to your local machine.</p> <p>This allows you to use or inspect agents, models, datasets, etc. that have been published by others.</p> <p>Parameters:</p> Name Type Description Default <code>entry_location (str) </code> <p>Entry location of the item to download (format: namespace/name/version)</p> required <code>force (bool) </code> <p>Force download even if the item already exists locally</p> required <code>encryption_key (str) </code> <p>Decrypt files with this encryption key</p> required <p>Examples:</p>"},{"location":"api/#nearai.cli.RegistryCli.download--download-a-specific-registry-item","title":"Download a specific registry item","text":"<p>nearai registry download example.near/agent-name/0.0.3</p>"},{"location":"api/#nearai.cli.RegistryCli.download--force-download-an-item-that-already-exists-locally","title":"Force download an item that already exists locally","text":"<p>nearai registry download example.near/model-name/1.0.0 --force</p> Source code in <code>nearai/cli.py</code> <pre><code>def download(self, entry_location: str, force: bool = False, encryption_key: Optional[str] = None) -&gt; None:\n    \"\"\"Download an item from the NEAR AI registry to your local machine.\n\n    This allows you to use or inspect agents, models, datasets, etc. that have been published by others.\n\n    Args:\n      entry_location (str) :\n        Entry location of the item to download (format: namespace/name/version)\n      force (bool) :\n        Force download even if the item already exists locally\n      encryption_key (str) :\n        Decrypt files with this encryption key\n\n    Examples:\n      # Download a specific registry item\n      nearai registry download example.near/agent-name/0.0.3\n\n      # Force download an item that already exists locally\n      nearai registry download example.near/model-name/1.0.0 --force\n\n    \"\"\"\n    registry.download(entry_location, force=force, show_progress=True, encryption_key=encryption_key)\n</code></pre>"},{"location":"api/#nearai.cli.RegistryCli.info","title":"info","text":"<pre><code>info(entry: str) -&gt; None\n</code></pre> <p>Display detailed information about a registry item.</p> <p>Includes metadata and available provider matches for models.</p> <p>Parameters:</p> Name Type Description Default <code>entry (str) </code> <p>Entry location of the item to display information for (format: namespace/name/version)</p> required <p>Examples:</p>"},{"location":"api/#nearai.cli.RegistryCli.info--show-information-about-a-specific-registry-item","title":"Show information about a specific registry item","text":"<p>nearai registry info example.near/agent-name/0.0.3</p>"},{"location":"api/#nearai.cli.RegistryCli.info--show-information-about-a-model","title":"Show information about a model","text":"<p>nearai registry info example.near/model-name/1.0.0</p> Documentation <p>https://docs.near.ai/agents/registry</p> Source code in <code>nearai/cli.py</code> <pre><code>def info(self, entry: str) -&gt; None:\n    \"\"\"Display detailed information about a registry item.\n\n    Includes metadata and available provider matches for models.\n\n    Args:\n      entry (str) :\n        Entry location of the item to display information for (format: namespace/name/version)\n\n    Examples:\n      # Show information about a specific registry item\n      nearai registry info example.near/agent-name/0.0.3\n\n      # Show information about a model\n      nearai registry info example.near/model-name/1.0.0\n\n    Documentation:\n      https://docs.near.ai/agents/registry\n\n    \"\"\"\n    entry_location = parse_location(entry)\n    metadata = registry.info(entry_location)\n\n    if metadata is None:\n        print(f\"Entry {entry} not found.\")\n        return\n\n    print(metadata.model_dump_json(indent=2))\n    if metadata.category == \"model\":\n        available_provider_matches = ProviderModels(CONFIG.get_client_config()).available_provider_matches(\n            NamespacedName(name=metadata.name, namespace=entry_location.namespace)\n        )\n        if len(available_provider_matches) &gt; 0:\n            header = [\"provider\", \"name\"]\n\n            table = []\n            for provider, name in available_provider_matches.items():\n                table.append(\n                    [\n                        fill(provider),\n                        fill(name),\n                    ]\n                )\n            print(tabulate(table, headers=header, tablefmt=\"simple_grid\"))\n</code></pre>"},{"location":"api/#nearai.cli.RegistryCli.list","title":"list","text":"<pre><code>list(namespace: str = '', category: str = '', tags: str = '', total: int = 32, offset: int = 0, show_all: bool = False, show_latest_version: bool = True, star: str = '') -&gt; None\n</code></pre> <p>List available items in the NEAR AI registry.</p> <p>You can filter the results by namespace, category, tags, and other criteria to find specific items.</p> <p>Parameters:</p> Name Type Description Default <code>namespace (str) </code> <p>Filter items by namespace/user account (example.near)</p> required <code>category (str) </code> <p>Filter items by category ('agent', 'model', 'evaluation')</p> required <code>tags (str) </code> <p>Filter items by tags (comma-separated)</p> required <code>total (int) </code> <p>Maximum number of items to show</p> required <code>offset (int) </code> <p>Offset for pagination</p> required <code>show_all (bool) </code> <p>Show all versions of items</p> required <code>show_latest_version (bool) </code> <p>Show only the latest version of each item</p> required <code>star (str) </code> <p>Show items starred by a specific user</p> required <p>Examples:</p>"},{"location":"api/#nearai.cli.RegistryCli.list--list-all-items-in-the-registry","title":"List all items in the registry","text":"<p>nearai registry list</p>"},{"location":"api/#nearai.cli.RegistryCli.list--list-agents-in-the-registry","title":"List agents in the registry","text":"<p>nearai registry list --category agent</p>"},{"location":"api/#nearai.cli.RegistryCli.list--list-items-with-specific-tags","title":"List items with specific tags","text":"<p>nearai registry list --tags \"summarization,text\"</p>"},{"location":"api/#nearai.cli.RegistryCli.list--list-items-from-a-specific-namespace","title":"List items from a specific namespace","text":"<p>nearai registry list --namespace example.near</p>"},{"location":"api/#nearai.cli.RegistryCli.list--show-all-versions-of-items","title":"Show all versions of items","text":"<p>nearai registry list --show-all</p> Source code in <code>nearai/cli.py</code> <pre><code>def list(\n    self,\n    namespace: str = \"\",\n    category: str = \"\",\n    tags: str = \"\",\n    total: int = 32,\n    offset: int = 0,\n    show_all: bool = False,\n    show_latest_version: bool = True,\n    star: str = \"\",\n) -&gt; None:\n    \"\"\"List available items in the NEAR AI registry.\n\n    You can filter the results by namespace, category, tags, and other criteria to find specific items.\n\n    Args:\n      namespace (str) :\n        Filter items by namespace/user account (example.near)\n      category (str) :\n        Filter items by category ('agent', 'model', 'evaluation')\n      tags (str) :\n        Filter items by tags (comma-separated)\n      total (int) :\n        Maximum number of items to show\n      offset (int) :\n        Offset for pagination\n      show_all (bool) :\n        Show all versions of items\n      show_latest_version (bool) :\n        Show only the latest version of each item\n      star (str) :\n        Show items starred by a specific user\n\n    Examples:\n      # List all items in the registry\n      nearai registry list\n\n      # List agents in the registry\n      nearai registry list --category agent\n\n      # List items with specific tags\n      nearai registry list --tags \"summarization,text\"\n\n      # List items from a specific namespace\n      nearai registry list --namespace example.near\n\n      # Show all versions of items\n      nearai registry list --show-all\n\n    \"\"\"\n    # Check if we're listing models specifically and provide alternative options\n    if category.lower() == \"model\":\n        print(\"\\n\u26a0\ufe0f  The category=model entries in the registry are deprecated.\")\n        print(\"We recommend using Fireworks, OpenAI, Anthropic, Crynux, or other provider models instead.\")\n        print(\"\\nEnter one of the following options:\")\n        print(\"- registry        : See the registry models anyway\")\n        print(\"- nearai          : List models from NEAR AI endpoint (Fireworks models, free inference)\")\n        print(\"- openai          : List available OpenAI models. Requires OPENAI_API_KEY.\")\n        print(\"- anthropic       : List available Anthropic models. Requires ANTHROPIC_API_KEY.\")\n        print(\n            \"- crynux          : List available Crynux models (framework support, public API key, low rate limit)\"\n        )\n        print(\"(May require api keys or correct moon phase)\")\n        print(\"\")\n\n        choice = input(\"\\nYour choice: \").strip().lower()\n\n        if choice != \"registry\":\n            models = fetch_models_from_provider(choice)\n            for model in models:\n                print(f\"{model}\")\n            return\n\n    # Make sure tags is a comma-separated list of tags\n    tags_l = parse_tags(tags)\n    tags = \",\".join(tags_l)\n\n    entries = registry.list(\n        namespace=namespace,\n        category=category,\n        tags=tags,\n        total=total + 1,\n        offset=offset,\n        show_all=show_all,\n        show_latest_version=show_latest_version,\n        starred_by=star,\n    )\n\n    more_rows = len(entries) &gt; total\n    entries = entries[:total]\n\n    header = [\"entry\", \"category\", \"description\", \"tags\"]\n\n    table = []\n    for entry in entries:\n        table.append(\n            [\n                fill(f\"{entry.namespace}/{entry.name}/{entry.version}\"),\n                fill(entry.category, 20),\n                fill(entry.description, 50),\n                fill(\", \".join(entry.tags), 20),\n            ]\n        )\n\n    if more_rows:\n        table.append([\"...\", \"...\", \"...\", \"...\"])\n\n    print(tabulate(table, headers=header, tablefmt=\"simple_grid\"))\n\n    if category == \"model\" and len(entries) &lt; total and namespace == \"\" and tags == \"\" and star == \"\":\n        unregistered_common_provider_models = ProviderModels(\n            CONFIG.get_client_config()\n        ).get_unregistered_common_provider_models(registry.dict_models())\n        if len(unregistered_common_provider_models):\n            print(\n                f\"There are unregistered common provider models: {unregistered_common_provider_models}. Run 'nearai registry upload-unregistered-common-provider-models' to update registry.\"  # noqa: E501\n            )\n</code></pre>"},{"location":"api/#nearai.cli.RegistryCli.metadata_template","title":"metadata_template","text":"<pre><code>metadata_template(local_path: str = '.', category: str = '', description: str = '')\n</code></pre> <p>Create a metadata template file for a registry item.</p> <p>This generates a properly formatted metadata.json file with default values that can be customized for your agent or model.</p> <p>Parameters:</p> Name Type Description Default <code>local_path </code> <p>str Path to the directory where the metadata template will be created</p> required <code>category </code> <p>str Category of the item (e.g., 'agent', 'model', 'dataset', 'evaluation')</p> required <code>description </code> <p>str Description of the item</p> required <p>Examples:</p>"},{"location":"api/#nearai.cli.RegistryCli.metadata_template--create-a-metadata-template-in-the-current-directory","title":"Create a metadata template in the current directory","text":"<p>nearai registry metadata-template</p>"},{"location":"api/#nearai.cli.RegistryCli.metadata_template--create-a-metadata-template-for-an-agent-with-description","title":"Create a metadata template for an agent with description","text":"<p>nearai registry metadata-template --category agent --description \"My helpful assistant\"</p>"},{"location":"api/#nearai.cli.RegistryCli.metadata_template--create-a-metadata-template-in-a-specific-directory","title":"Create a metadata template in a specific directory","text":"<p>nearai registry metadata-template path/to/directory --category model</p> Source code in <code>nearai/cli.py</code> <pre><code>def metadata_template(self, local_path: str = \".\", category: str = \"\", description: str = \"\"):\n    \"\"\"Create a metadata template file for a registry item.\n\n    This generates a properly formatted metadata.json file with default values\n    that can be customized for your agent or model.\n\n    Args:\n      local_path : str\n        Path to the directory where the metadata template will be created\n      category : str\n        Category of the item (e.g., 'agent', 'model', 'dataset', 'evaluation')\n      description : str\n        Description of the item\n\n    Examples:\n      # Create a metadata template in the current directory\n      nearai registry metadata-template\n\n      # Create a metadata template for an agent with description\n      nearai registry metadata-template --category agent --description \"My helpful assistant\"\n\n      # Create a metadata template in a specific directory\n      nearai registry metadata-template path/to/directory --category model\n\n    \"\"\"\n    path = resolve_local_path(Path(local_path))\n\n    metadata_path = path / \"metadata.json\"\n\n    version = path.name\n    # Validate version format\n    is_valid, error = validate_version(version)\n    if not is_valid:\n        print(error)\n        return\n\n    name = path.parent.name\n    assert not re.match(PATTERN, name), f\"Invalid agent name: {name}\"\n    assert \" \" not in name\n\n    with open(metadata_path, \"w\") as f:\n        metadata: Dict[str, Any] = {\n            \"name\": name,\n            \"version\": version,\n            \"description\": description,\n            \"category\": category,\n            \"tags\": [],\n            \"details\": {},\n            \"show_entry\": True,\n        }\n\n        if category == \"agent\":\n            metadata[\"details\"][\"agent\"] = {}\n            metadata[\"details\"][\"agent\"][\"welcome\"] = {\n                \"title\": name,\n                \"description\": description,\n            }\n            metadata[\"details\"][\"agent\"][\"defaults\"] = {\n                \"model\": DEFAULT_MODEL,\n                \"model_provider\": DEFAULT_PROVIDER,\n                \"model_temperature\": DEFAULT_MODEL_TEMPERATURE,\n                \"model_max_tokens\": DEFAULT_MODEL_MAX_TOKENS,\n            }\n            metadata[\"details\"][\"agent\"][\"framework\"] = \"minimal\"\n\n        json.dump(metadata, f, indent=2)\n</code></pre>"},{"location":"api/#nearai.cli.RegistryCli.update","title":"update","text":"<pre><code>update(local_path: str = '.') -&gt; None\n</code></pre> <p>Update the remote metadata of an item in the NEAR AI Registry.</p> <p>Looks for a metadata.json file in the given directory and updates the remote metadata with the new values.</p> <p>Parameters:</p> Name Type Description Default <code>local_path (str) </code> <p>Path to the directory containing the item to update</p> required <p>Examples:</p>"},{"location":"api/#nearai.cli.RegistryCli.update--update-metadata-for-the-item-in-the-current-directory","title":"Update metadata for the item in the current directory","text":"<p>nearai registry update</p>"},{"location":"api/#nearai.cli.RegistryCli.update--update-metadata-for-a-specific-item","title":"Update metadata for a specific item","text":"<p>nearai registry update path/to/item</p> Source code in <code>nearai/cli.py</code> <pre><code>def update(self, local_path: str = \".\") -&gt; None:\n    \"\"\"Update the remote metadata of an item in the NEAR AI Registry.\n\n    Looks for a metadata.json file in the given directory and updates the remote metadata with the new values.\n\n    Args:\n      local_path (str) :\n        Path to the directory containing the item to update\n\n    Examples:\n      # Update metadata for the item in the current directory\n      nearai registry update\n\n      # Update metadata for a specific item\n      nearai registry update path/to/item\n\n    \"\"\"\n    path = resolve_local_path(Path(local_path))\n\n    if CONFIG.auth is None:\n        print(\"Please login with `nearai login`\")\n        exit(1)\n\n    metadata_path = path / \"metadata.json\"\n    check_metadata_present(metadata_path)\n\n    with open(metadata_path) as f:\n        metadata: Dict[str, Any] = json.load(f)\n\n    namespace = get_namespace(path)\n\n    entry_location = EntryLocation.model_validate(\n        dict(\n            namespace=namespace,\n            name=metadata.pop(\"name\"),\n            version=metadata.pop(\"version\"),\n        )\n    )\n    assert \" \" not in entry_location.name\n\n    entry_metadata = EntryMetadataInput.model_validate(metadata)\n    result = registry.update(entry_location, entry_metadata)\n    print(json.dumps(result, indent=2))\n</code></pre>"},{"location":"api/#nearai.cli.RegistryCli.upload","title":"upload","text":"<pre><code>upload(local_path: str, bump: bool = False, minor_bump: bool = False, major_bump: bool = False, encrypt: bool = False) -&gt; Optional[EntryLocation]\n</code></pre> <p>Upload an item to the NEAR AI registry for public use.</p> <p>Parameters:</p> Name Type Description Default <code>local_path (str) </code> <p>Path to the agent directory</p> required <code>bump (bool) </code> <p>Automatically increment patch version if it already exists</p> required <code>minor_bump (bool) </code> <p>Bump with minor version increment (0.1.0 \u2192 0.2.0)</p> required <code>major_bump (bool) </code> <p>Bump with major version increment (1.5.2 \u2192 2.0.0)</p> required <code>encrypt (bool) </code> <p>Encrypt all uploaded files using a generated encryption key</p> required <p>Examples:</p>"},{"location":"api/#nearai.cli.RegistryCli.upload--upload-an-item-in-the-current-directory","title":"Upload an item in the current directory","text":"<p>nearai registry upload .</p>"},{"location":"api/#nearai.cli.RegistryCli.upload--upload-a-specific-agent-directory","title":"Upload a specific agent directory","text":"<p>nearai registry upload ./path/to/item</p>"},{"location":"api/#nearai.cli.RegistryCli.upload--upload-with-automatic-version-bumping","title":"Upload with automatic version bumping","text":"<p>nearai registry upload ./path/to/item --bump</p>"},{"location":"api/#nearai.cli.RegistryCli.upload--upload-with-minor-version-bump","title":"Upload with minor version bump","text":"<p>nearai registry upload ./path/to/item --minor-bump</p>"},{"location":"api/#nearai.cli.RegistryCli.upload--upload-with-encryption-enabled-private-entries","title":"Upload with encryption enabled (private entries)","text":"<p>nearai registry upload ./path/to/item --encrypt</p> Source code in <code>nearai/cli.py</code> <pre><code>def upload(\n    self,\n    local_path: str,\n    bump: bool = False,\n    minor_bump: bool = False,\n    major_bump: bool = False,\n    encrypt: bool = False,\n) -&gt; Optional[EntryLocation]:\n    \"\"\"Upload an item to the NEAR AI registry for public use.\n\n    Args:\n      local_path (str) :\n        Path to the agent directory\n      bump (bool) :\n        Automatically increment patch version if it already exists\n      minor_bump (bool) :\n        Bump with minor version increment (0.1.0 \u2192 0.2.0)\n      major_bump (bool) :\n        Bump with major version increment (1.5.2 \u2192 2.0.0)\n      encrypt (bool) :\n        Encrypt all uploaded files using a generated encryption key\n\n    Examples:\n      # Upload an item in the current directory\n      nearai registry upload .\n\n      # Upload a specific agent directory\n      nearai registry upload ./path/to/item\n\n      # Upload with automatic version bumping\n      nearai registry upload ./path/to/item --bump\n\n      # Upload with minor version bump\n      nearai registry upload ./path/to/item --minor-bump\n\n      # Upload with encryption enabled (private entries)\n      nearai registry upload ./path/to/item --encrypt\n\n    \"\"\"\n    console = Console()\n    path = resolve_local_path(Path(local_path))\n    metadata_path = path / \"metadata.json\"\n\n    # Load and validate metadata\n    metadata, error = load_and_validate_metadata(metadata_path)\n    if error:\n        console.print(\n            Panel(Text(error, style=\"bold red\"), title=\"Metadata Error\", border_style=\"red\", padding=(1, 2))\n        )\n        return None\n\n    # At this point, metadata is guaranteed to be not None\n    assert metadata is not None, \"Metadata should not be None if error is None\"\n\n    name = metadata[\"name\"]\n    version = metadata[\"version\"]\n\n    # Handle encryption key generation if --encrypt flag is used\n    if encrypt:\n        # Initialize details if not present\n        if \"details\" not in metadata:\n            metadata[\"details\"] = {}\n\n        # Generate encryption key if not present\n        if \"encryption_key\" not in metadata[\"details\"]:\n            encryption_key = FileEncryption.generate_encryption_key()\n            metadata[\"details\"][\"encryption_key\"] = encryption_key\n\n            # Update metadata.json file with the new encryption key\n            with open(metadata_path, \"w\") as f:\n                json.dump(metadata, f, indent=2)\n\n            console.print(\n                Panel(\n                    Text.assemble(\n                        (\"\ud83d\udd10 Encryption enabled\\n\\n\", \"bold green\"),\n                        (f\"Encryption key {encryption_key} generated and stored in metadata.json\\n\", \"dim\"),\n                    ),\n                    title=\"Encryption\",\n                    border_style=\"green\",\n                    padding=(1, 2),\n                )\n            )\n\n    # Get namespace using the function from registry.py\n    try:\n        namespace = get_namespace(path)\n    except ValueError:\n        console.print(\n            Panel(\n                Text(\"Please login with `nearai login` before uploading\", style=\"bold red\"),\n                title=\"Authentication Error\",\n                border_style=\"red\",\n                padding=(1, 2),\n            )\n        )\n        return None\n\n    # Check if this version already exists\n    exists, error = check_version_exists(namespace, name, version)\n\n    if error:\n        console.print(\n            Panel(Text(error, style=\"bold red\"), title=\"Registry Error\", border_style=\"red\", padding=(1, 2))\n        )\n        return None\n\n    # Display the version check result\n    display_version_check(namespace, name, version, exists)\n\n    bump_requested = bump or minor_bump or major_bump\n\n    if exists and bump_requested:\n        # Handle version bump\n        old_version = version\n\n        # Determine increment type based on flags\n        if major_bump:\n            increment_type = \"major\"\n        elif minor_bump:\n            increment_type = \"minor\"\n        else:\n            increment_type = \"patch\"  # Default for bump\n\n        version = increment_version_by_type(version, increment_type)\n\n        # Enhanced version update message\n        update_panel = Panel(\n            Text.assemble(\n                (\"Updating Version...\\n\\n\", \"bold\"),\n                (\"Previous version: \", \"dim\"),\n                (f\"{old_version}\\n\", \"yellow\"),\n                (\"New version:     \", \"dim\"),\n                (f\"{version}\", \"green bold\"),\n                (\"\\n\\nIncrement type: \", \"dim\"),\n                (f\"{increment_type}\", \"cyan\"),\n            ),\n            title=\"Bump\",\n            border_style=\"green\",\n            padding=(1, 2),\n        )\n        console.print(update_panel)\n\n        # Update metadata.json with new version\n        metadata[\"version\"] = version\n        with open(metadata_path, \"w\") as f:\n            json.dump(metadata, f, indent=2)\n\n        console.print(f\"\\n\u2705 Updated [bold]{metadata_path}[/bold] with new version\\n\")\n        console.print(Rule(style=\"dim\"))\n\n    elif exists and not bump_requested:\n        # Show error panel for version conflict\n        error_panel = Panel(\n            Text.assemble(\n                (\"To upload a new version:\\n\", \"yellow\"),\n                (f\"1. Edit {metadata_path}\\n\", \"dim\"),\n                ('2. Update the \"version\" field (e.g., increment from \"0.0.1\" to \"0.0.2\")\\n', \"dim\"),\n                (\"3. Try uploading again\\n\\n\", \"dim\"),\n                (\"Or use the following flags:\\n\", \"yellow\"),\n                (\"  --bump          # Patch update (0.0.1 \u2192 0.0.2)\\n\", \"green\"),\n                (\"  --minor-bump    # Minor update (0.0.1 \u2192 0.1.0)\\n\", \"green\"),\n                (\"  --major-bump    # Major update (0.1.0 \u2192 1.0.0)\\n\", \"green\"),\n            ),\n            title=\"Version Conflict\",\n            border_style=\"red\",\n        )\n        console.print(error_panel)\n        return None\n\n    # Version doesn't exist or has been bumped, proceed with upload\n    console.print(\n        f\"\\n\ud83d\udcc2 [bold]Uploading[/bold] version [green bold]{version}[/green bold] of [blue bold]{name}[/blue bold] to [cyan bold]{namespace}[/cyan bold]...\\n\"  # noqa: E501\n    )\n\n    try:\n        result = registry.upload(path, show_progress=True, encrypt=encrypt)\n\n        if result:\n            success_panel = Panel(\n                Text.assemble(\n                    (\"Upload completed successfully! \ud83d\ude80 \\n\\n\", \"bold green\"),\n                    (\"Name:      \", \"dim\"),\n                    (f\"{result.name}\\n\", \"cyan\"),\n                    (\"Version:   \", \"dim\"),\n                    (f\"{result.version}\\n\", \"cyan\"),\n                    (\"Namespace: \", \"dim\"),\n                    (f\"{result.namespace}\", \"cyan\"),\n                ),\n                title=\"Success\",\n                border_style=\"green\",\n                padding=(1, 2),\n            )\n            console.print(success_panel)\n            return result\n        else:\n            console.print(\n                Panel(\n                    Text(\"Upload failed for unknown reasons\", style=\"bold red\"),\n                    title=\"Upload Error\",\n                    border_style=\"red\",\n                    padding=(1, 2),\n                )\n            )\n            return None\n\n    except Exception as e:\n        console.print(\n            Panel(\n                Text(f\"Error during upload: {str(e)}\", style=\"bold red\"),\n                title=\"Upload Error\",\n                border_style=\"red\",\n                padding=(1, 2),\n            )\n        )\n        return None\n</code></pre>"},{"location":"api/#nearai.cli.RegistryCli.upload_unregistered_common_provider_models","title":"upload_unregistered_common_provider_models","text":"<pre><code>upload_unregistered_common_provider_models(dry_run: bool = True) -&gt; None\n</code></pre> <p>Create new registry items for unregistered common provider models.</p> <p>This command helps keep the registry up-to-date with the latest models from various providers.</p> <p>Parameters:</p> Name Type Description Default <code>dry_run (bool) </code> <p>Perform a dry run without actually uploading</p> required <p>Examples:</p>"},{"location":"api/#nearai.cli.RegistryCli.upload_unregistered_common_provider_models--perform-a-dry-run-to-see-what-would-be-uploaded","title":"Perform a dry run to see what would be uploaded","text":"<p>nearai registry upload-unregistered-common-provider-models</p>"},{"location":"api/#nearai.cli.RegistryCli.upload_unregistered_common_provider_models--actually-upload-the-unregistered-models","title":"Actually upload the unregistered models","text":"<p>nearai registry upload-unregistered-common-provider-models --dry-run=False</p> Source code in <code>nearai/cli.py</code> <pre><code>def upload_unregistered_common_provider_models(self, dry_run: bool = True) -&gt; None:\n    \"\"\"Create new registry items for unregistered common provider models.\n\n    This command helps keep the registry up-to-date with the latest models from various providers.\n\n    Args:\n      dry_run (bool) :\n        Perform a dry run without actually uploading\n\n    Examples:\n      # Perform a dry run to see what would be uploaded\n      nearai registry upload-unregistered-common-provider-models\n\n      # Actually upload the unregistered models\n      nearai registry upload-unregistered-common-provider-models --dry-run=False\n\n    \"\"\"\n    provider_matches_list = ProviderModels(CONFIG.get_client_config()).get_unregistered_common_provider_models(\n        registry.dict_models()\n    )\n    if len(provider_matches_list) == 0:\n        print(\"No new models to upload.\")\n        return\n\n    print(\"Going to create new registry items:\")\n    header = [\"entry\", \"description\"]\n    table = []\n    paths = []\n    for provider_matches in provider_matches_list:\n        provider_model = provider_matches.get(DEFAULT_PROVIDER) or next(iter(provider_matches.values()))\n        _, model = get_provider_namespaced_model(provider_model)\n        assert model.namespace == \"\"\n        model.name = create_registry_name(model.name)\n        model.namespace = DEFAULT_NAMESPACE\n        version = \"1.0.0\"\n        description = \" &amp; \".join(provider_matches.values())\n        table.append(\n            [\n                fill(f\"{model.namespace}/{model.name}/{version}\"),\n                fill(description, 50),\n            ]\n        )\n\n        path = get_registry_folder() / model.namespace / model.name / version\n        path.mkdir(parents=True, exist_ok=True)\n        paths.append(path)\n        metadata_path = path / \"metadata.json\"\n        with open(metadata_path, \"w\") as f:\n            metadata: Dict[str, Any] = {\n                \"name\": model.name,\n                \"version\": version,\n                \"description\": description,\n                \"category\": \"model\",\n                \"tags\": [],\n                \"details\": {},\n                \"show_entry\": True,\n            }\n            json.dump(metadata, f, indent=2)\n\n    print(tabulate(table, headers=header, tablefmt=\"simple_grid\"))\n    if dry_run:\n        print(\"Please verify, then repeat the command with --dry_run=False\")\n    else:\n        for path in paths:\n            self.upload(str(path))\n</code></pre>"},{"location":"api/#nearai.cli.VllmCli","title":"VllmCli","text":"<p>Commands for running VLLM server with OpenAI-compatible API for local inference.</p> Commands <p>nearai vllm run : Run VLLM server with OpenAI-compatible API   (--model, --host, --port, --tensor-parallel-size, --gpu-memory-utilization)</p> Options <p>--model (str) :   Path to the model or model name from Hugging Face --host (str) :   Host to bind the server to --port (int) :   Port to bind the server to --tensor-parallel-size (int) :   Number of GPUs to use for tensor parallelism --gpu-memory-utilization (float) :   Fraction of GPU memory to use</p> <p>Examples:</p>"},{"location":"api/#nearai.cli.VllmCli--run-vllm-server-with-default-settings","title":"Run VLLM server with default settings","text":"<p>nearai vllm run --model mistralai/Mistral-7B-Instruct-v0.1</p>"},{"location":"api/#nearai.cli.VllmCli--run-vllm-server-with-custom-host-and-port","title":"Run VLLM server with custom host and port","text":"<p>nearai vllm run --model meta-llama/Llama-2-7b-chat-hf --host 0.0.0.0 --port 8080</p>"},{"location":"api/#nearai.cli.VllmCli--run-with-multiple-gpus-and-specific-memory-utilization","title":"Run with multiple GPUs and specific memory utilization","text":"<p>nearai vllm run --model meta-llama/Llama-2-13b-chat-hf --tensor-parallel-size 2 --gpu-memory-utilization 0.8</p> Source code in <code>nearai/cli.py</code> <pre><code>class VllmCli:\n    \"\"\"Commands for running VLLM server with OpenAI-compatible API for local inference.\n\n    Commands:\n      nearai vllm run : Run VLLM server with OpenAI-compatible API\n        (--model, --host, --port, --tensor-parallel-size, --gpu-memory-utilization)\n\n    Options:\n      --model (str) :\n        Path to the model or model name from Hugging Face\n      --host (str) :\n        Host to bind the server to\n      --port (int) :\n        Port to bind the server to\n      --tensor-parallel-size (int) :\n        Number of GPUs to use for tensor parallelism\n      --gpu-memory-utilization (float) :\n        Fraction of GPU memory to use\n\n    Examples:\n      # Run VLLM server with default settings\n      nearai vllm run --model mistralai/Mistral-7B-Instruct-v0.1\n\n      # Run VLLM server with custom host and port\n      nearai vllm run --model meta-llama/Llama-2-7b-chat-hf --host 0.0.0.0 --port 8080\n\n      # Run with multiple GPUs and specific memory utilization\n      nearai vllm run --model meta-llama/Llama-2-13b-chat-hf --tensor-parallel-size 2 --gpu-memory-utilization 0.8\n\n    \"\"\"\n\n    def run(self, *args: Any, **kwargs: Any) -&gt; None:  # noqa: D102 D417\n        \"\"\"Run a VLLM server with an OpenAI-compatible API for local inference.\n\n        This command starts a VLLM server that provides an OpenAI-compatible API for running\n        language models locally. The server supports various configuration options for\n        optimizing performance and resource utilization.\n\n        Args:\n          **kwargs : (dict)\n            Keyword arguments that can include:\n            model (str):\n              Path to the model or model name from Hugging Face\n            host (str):\n              Host to bind the server to\n            port (int):\n              Port to bind the server to\n            tensor_parallel_size (int):\n              Number of GPUs to use for tensor parallelism\n            gpu_memory_utilization (float):\n              Fraction of GPU memory to use\n\n        Examples:\n            # Run VLLM server with default settings\n            nearai vllm run --model mistralai/Mistral-7B-Instruct-v0.1\n\n            # Run VLLM server with custom host and port\n            nearai vllm run --model meta-llama/Llama-2-7b-chat-hf --host 0.0.0.0 --port 8080\n\n            # Run with multiple GPUs and specific memory utilization\n            nearai vllm run --model meta-llama/Llama-2-13b-chat-hf --tensor-parallel-size 2 --gpu-memory-utilization 0.8\n\n            # Run with additional VLLM configuration\n            nearai vllm run --model mistralai/Mistral-7B-Instruct-v0.1 --max-model-len 4096 --dtype float16\n\n        \"\"\"\n        original_argv = sys.argv.copy()\n        sys.argv = [\n            sys.argv[0],\n        ]\n        for key, value in kwargs.items():\n            sys.argv.extend([f\"--{key.replace('_', '-')}\", str(value)])\n        print(sys.argv)\n\n        try:\n            runpy.run_module(\"vllm.entrypoints.openai.api_server\", run_name=\"__main__\", alter_sys=True)\n        finally:\n            sys.argv = original_argv\n\n    def __call__(self) -&gt; None:\n        \"\"\"Show help when 'nearai vllm' is called without subcommands.\"\"\"\n        custom_args = [\"nearai\", \"vllm\", \"--help\"]\n        handle_help_request(custom_args)\n</code></pre>"},{"location":"api/#nearai.cli.VllmCli.__call__","title":"__call__","text":"<pre><code>__call__() -&gt; None\n</code></pre> <p>Show help when 'nearai vllm' is called without subcommands.</p> Source code in <code>nearai/cli.py</code> <pre><code>def __call__(self) -&gt; None:\n    \"\"\"Show help when 'nearai vllm' is called without subcommands.\"\"\"\n    custom_args = [\"nearai\", \"vllm\", \"--help\"]\n    handle_help_request(custom_args)\n</code></pre>"},{"location":"api/#nearai.cli.VllmCli.run","title":"run","text":"<pre><code>run(*args: Any, **kwargs: Any) -&gt; None\n</code></pre> <p>Run a VLLM server with an OpenAI-compatible API for local inference.</p> <p>This command starts a VLLM server that provides an OpenAI-compatible API for running language models locally. The server supports various configuration options for optimizing performance and resource utilization.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs </code> <p>(dict) Keyword arguments that can include: model (str):   Path to the model or model name from Hugging Face host (str):   Host to bind the server to port (int):   Port to bind the server to tensor_parallel_size (int):   Number of GPUs to use for tensor parallelism gpu_memory_utilization (float):   Fraction of GPU memory to use</p> required <p>Examples:</p>"},{"location":"api/#nearai.cli.VllmCli.run--run-vllm-server-with-default-settings","title":"Run VLLM server with default settings","text":"<p>nearai vllm run --model mistralai/Mistral-7B-Instruct-v0.1</p>"},{"location":"api/#nearai.cli.VllmCli.run--run-vllm-server-with-custom-host-and-port","title":"Run VLLM server with custom host and port","text":"<p>nearai vllm run --model meta-llama/Llama-2-7b-chat-hf --host 0.0.0.0 --port 8080</p>"},{"location":"api/#nearai.cli.VllmCli.run--run-with-multiple-gpus-and-specific-memory-utilization","title":"Run with multiple GPUs and specific memory utilization","text":"<p>nearai vllm run --model meta-llama/Llama-2-13b-chat-hf --tensor-parallel-size 2 --gpu-memory-utilization 0.8</p>"},{"location":"api/#nearai.cli.VllmCli.run--run-with-additional-vllm-configuration","title":"Run with additional VLLM configuration","text":"<p>nearai vllm run --model mistralai/Mistral-7B-Instruct-v0.1 --max-model-len 4096 --dtype float16</p> Source code in <code>nearai/cli.py</code> <pre><code>def run(self, *args: Any, **kwargs: Any) -&gt; None:  # noqa: D102 D417\n    \"\"\"Run a VLLM server with an OpenAI-compatible API for local inference.\n\n    This command starts a VLLM server that provides an OpenAI-compatible API for running\n    language models locally. The server supports various configuration options for\n    optimizing performance and resource utilization.\n\n    Args:\n      **kwargs : (dict)\n        Keyword arguments that can include:\n        model (str):\n          Path to the model or model name from Hugging Face\n        host (str):\n          Host to bind the server to\n        port (int):\n          Port to bind the server to\n        tensor_parallel_size (int):\n          Number of GPUs to use for tensor parallelism\n        gpu_memory_utilization (float):\n          Fraction of GPU memory to use\n\n    Examples:\n        # Run VLLM server with default settings\n        nearai vllm run --model mistralai/Mistral-7B-Instruct-v0.1\n\n        # Run VLLM server with custom host and port\n        nearai vllm run --model meta-llama/Llama-2-7b-chat-hf --host 0.0.0.0 --port 8080\n\n        # Run with multiple GPUs and specific memory utilization\n        nearai vllm run --model meta-llama/Llama-2-13b-chat-hf --tensor-parallel-size 2 --gpu-memory-utilization 0.8\n\n        # Run with additional VLLM configuration\n        nearai vllm run --model mistralai/Mistral-7B-Instruct-v0.1 --max-model-len 4096 --dtype float16\n\n    \"\"\"\n    original_argv = sys.argv.copy()\n    sys.argv = [\n        sys.argv[0],\n    ]\n    for key, value in kwargs.items():\n        sys.argv.extend([f\"--{key.replace('_', '-')}\", str(value)])\n    print(sys.argv)\n\n    try:\n        runpy.run_module(\"vllm.entrypoints.openai.api_server\", run_name=\"__main__\", alter_sys=True)\n    finally:\n        sys.argv = original_argv\n</code></pre>"},{"location":"api/#nearai.cli.check_update","title":"check_update","text":"<pre><code>check_update()\n</code></pre> <p>Check if there is a new version of nearai CLI available.</p> Source code in <code>nearai/cli.py</code> <pre><code>def check_update():\n    \"\"\"Check if there is a new version of nearai CLI available.\"\"\"\n    try:\n        api = DefaultApi()\n        latest = api.version_v1_version_get()\n        current = importlib.metadata.version(\"nearai\")\n\n        if latest != current:\n            print(f\"New version of nearai CLI available: {latest}. Current version: {current}\")\n            print(\"Run `pip install --upgrade nearai` to update.\")\n\n    except Exception as _:\n        pass\n</code></pre>"},{"location":"api/#nearai.cli.main","title":"main","text":"<pre><code>main() -&gt; None\n</code></pre> <p>Main entry point for the NEAR AI CLI.</p> Source code in <code>nearai/cli.py</code> <pre><code>def main() -&gt; None:\n    \"\"\"Main entry point for the NEAR AI CLI.\"\"\"\n    check_update()\n\n    # Check if help is requested\n    if \"--help\" in sys.argv or len(sys.argv) == 1:\n        if handle_help_request():\n            return\n\n    # Otherwise, proceed with normal command processing\n    fire.Fire(CLI)\n</code></pre>"},{"location":"api/#nearai.cli_help","title":"cli_help","text":""},{"location":"api/#nearai.cli_help._display_commands_section","title":"_display_commands_section","text":"<pre><code>_display_commands_section(console: Console, sections: Dict[str, List[str]]) -&gt; None\n</code></pre> <p>Display the commands section in a table.</p> <pre><code>console: Rich console for display\nsections: Parsed sections dictionary\n</code></pre> Source code in <code>nearai/cli_help.py</code> <pre><code>def _display_commands_section(console: Console, sections: Dict[str, List[str]]) -&gt; None:\n    \"\"\"Display the commands section in a table.\n\n    Args:\n    ----\n        console: Rich console for display\n        sections: Parsed sections dictionary\n\n    \"\"\"\n    commands_table = Table(box=ROUNDED, expand=False, width=120, style=\"dim\")\n    commands_table.add_column(\"Command\", style=\"cyan bold\", no_wrap=True)\n    commands_table.add_column(\"Description\", style=\"white\")\n    commands_table.add_column(\"Options\", style=\"dim\")\n\n    lines = sections[\"commands\"]\n    i = 0\n\n    while i &lt; len(lines):\n        line = lines[i].strip()\n        if not line:\n            i += 1\n            continue\n\n        # Command format is \"command : description\"\n        cmd_parts = line.split(\" : \", 1)\n        if len(cmd_parts) == 2:\n            cmd, desc = cmd_parts[0].strip(), cmd_parts[1].strip()\n            options_str, next_index = _parse_command_options(lines, i)\n            i = next_index  # Update index based on options parsing\n            commands_table.add_row(cmd, desc, options_str)\n        i += 1\n\n    console.print(commands_table)\n\n    # Check if any commands have required parameters (marked with *)\n    if any(\"*\" in line for line in sections[\"commands\"]):\n        console.print(\"* Required parameter\")\n</code></pre>"},{"location":"api/#nearai.cli_help._display_description_section","title":"_display_description_section","text":"<pre><code>_display_description_section(console: Console, sections: Dict[str, List[str]]) -&gt; None\n</code></pre> <p>Display the description section in a panel.</p> <pre><code>console: Rich console for display\nsections: Parsed sections dictionary\n</code></pre> Source code in <code>nearai/cli_help.py</code> <pre><code>def _display_description_section(console: Console, sections: Dict[str, List[str]]) -&gt; None:\n    \"\"\"Display the description section in a panel.\n\n    Args:\n    ----\n        console: Rich console for display\n        sections: Parsed sections dictionary\n\n    \"\"\"\n    if \"description\" in sections:\n        description = \" \".join(sections[\"description\"])\n        if description:\n            console.print(Panel(description, title=\"Info\", expand=False, border_style=\"blue\", width=120))\n</code></pre>"},{"location":"api/#nearai.cli_help._display_documentation_section","title":"_display_documentation_section","text":"<pre><code>_display_documentation_section(console: Console, sections: Dict[str, List[str]]) -&gt; None\n</code></pre> <p>Display documentation section.</p> <pre><code>console: Rich console for display\nsections: Parsed sections dictionary\n</code></pre> Source code in <code>nearai/cli_help.py</code> <pre><code>def _display_documentation_section(console: Console, sections: Dict[str, List[str]]) -&gt; None:\n    \"\"\"Display documentation section.\n\n    Args:\n    ----\n        console: Rich console for display\n        sections: Parsed sections dictionary\n\n    \"\"\"\n    doc_content = \" \".join(sections[\"documentation\"])\n    console.print(f\"For more information see: [bold blue]{doc_content}[/bold blue]\\n\")\n</code></pre>"},{"location":"api/#nearai.cli_help._display_examples_section","title":"_display_examples_section","text":"<pre><code>_display_examples_section(console: Console, sections: Dict[str, List[str]]) -&gt; None\n</code></pre> <p>Display examples section in a panel.</p> <pre><code>console: Rich console for display\nsections: Parsed sections dictionary\n</code></pre> Source code in <code>nearai/cli_help.py</code> <pre><code>def _display_examples_section(console: Console, sections: Dict[str, List[str]]) -&gt; None:\n    \"\"\"Display examples section in a panel.\n\n    Args:\n    ----\n        console: Rich console for display\n        sections: Parsed sections dictionary\n\n    \"\"\"\n    examples_text = []\n    current_example: List[str] = []\n\n    for line in sections[\"examples\"]:\n        line_stripped = line.strip()\n\n        if not line_stripped:\n            # Empty line separates examples\n            if current_example:\n                examples_text.append(\"\\n\".join(current_example))\n                examples_text.append(\"\")  # Add spacing\n                current_example = []\n            continue\n\n        if line_stripped.startswith(\"#\"):\n            # This is a comment/description for an example\n            current_example.append(f\"[dim]{line_stripped}[/dim]\")\n        else:\n            # This is a command\n            current_example.append(f\"[cyan]{line_stripped}[/cyan]\")\n\n    # Add the last example\n    if current_example:\n        examples_text.append(\"\\n\".join(current_example))\n\n    # Display examples in a panel\n    if examples_text:\n        console.print(\n            Panel(\"\\n\".join(examples_text), title=\"Examples\", border_style=\"cyan\", expand=False, padding=(1, 2))\n        )\n        console.print()\n</code></pre>"},{"location":"api/#nearai.cli_help._display_param_section","title":"_display_param_section","text":"<pre><code>_display_param_section(console: Console, sections: Dict[str, List[str]], section_name: str, section_title: str, param_pattern: str, obj: Any = None, method_name: str = '__class__') -&gt; None\n</code></pre> <p>Display a parameter section (Args or Options) in a table.</p> <pre><code>console: Rich console for display\nsections: Parsed sections dictionary\nsection_name: Name of the section in the sections dict\nsection_title: Title to display for the section\nparam_pattern: Regex pattern to match parameter definitions\nobj: The object containing the method to inspect\nmethod_name: The name of the method to inspect, or \"__class__\" for class docstring\n</code></pre> Source code in <code>nearai/cli_help.py</code> <pre><code>def _display_param_section(\n    console: Console,\n    sections: Dict[str, List[str]],\n    section_name: str,\n    section_title: str,\n    param_pattern: str,\n    obj: Any = None,\n    method_name: str = \"__class__\",\n) -&gt; None:\n    \"\"\"Display a parameter section (Args or Options) in a table.\n\n    Args:\n    ----\n        console: Rich console for display\n        sections: Parsed sections dictionary\n        section_name: Name of the section in the sections dict\n        section_title: Title to display for the section\n        param_pattern: Regex pattern to match parameter definitions\n        obj: The object containing the method to inspect\n        method_name: The name of the method to inspect, or \"__class__\" for class docstring\n\n    \"\"\"\n    section_lines = sections.get(section_name, [])\n    if not section_lines:\n        return\n\n    # Create table for parameters\n    table = Table(title=section_title, show_header=True, header_style=\"bold magenta\")\n    table.add_column(\"Parameter\", style=\"yellow\")\n    table.add_column(\"Type\", style=\"cyan\", justify=\"center\")\n    table.add_column(\"Description\", style=\"white\")\n    table.add_column(\"Default\", style=\"green\", justify=\"center\")\n\n    # Get the method object for default value lookup\n    method = None\n    if obj and method_name:\n        if method_name != \"__class__\":\n            method = getattr(obj, method_name, None)\n        else:\n            method = obj\n\n    i = 0\n    while i &lt; len(section_lines):\n        line = section_lines[i].rstrip()\n\n        # Skip empty lines\n        if not line:\n            i += 1\n            continue\n\n        # Check if this is a parameter line\n        match = re.match(param_pattern, line)\n        if match:\n            param_name = match.group(1)\n            param_type = match.group(2)\n\n            # Parse parameter description\n            description, next_index = _parse_param_description(section_lines, i, param_pattern)\n            i = next_index  # Update index based on description parsing\n\n            # Get default value from function signature if available\n            default_value = \"-\"\n            if method:\n                try:\n                    sig = inspect.signature(method)\n                    if param_name in sig.parameters:\n                        param = sig.parameters[param_name]\n                        if param.default != inspect.Parameter.empty:\n                            default_value = str(param.default)\n                except (ValueError, TypeError):\n                    pass\n\n            table.add_row(param_name, param_type, description, default_value)\n        else:\n            i += 1\n\n    if table.row_count &gt; 0:\n        console.print(table)\n        console.print()\n</code></pre>"},{"location":"api/#nearai.cli_help._extract_basic_metadata","title":"_extract_basic_metadata","text":"<pre><code>_extract_basic_metadata(obj: Any, method_name: str, console: Console) -&gt; Tuple[bool, Optional[str], str]\n</code></pre> <p>Extract basic metadata from the object or method.</p> <pre><code>obj: The object containing the docstring\nmethod_name: Name of the method or \"__class__\" for class docstring\nconsole: Console for error printing\n</code></pre> <pre><code>Tuple of (is_class, docstring, display_name)\n</code></pre> Source code in <code>nearai/cli_help.py</code> <pre><code>def _extract_basic_metadata(obj: Any, method_name: str, console: Console) -&gt; Tuple[bool, Optional[str], str]:\n    \"\"\"Extract basic metadata from the object or method.\n\n    Args:\n    ----\n        obj: The object containing the docstring\n        method_name: Name of the method or \"__class__\" for class docstring\n        console: Console for error printing\n\n    Returns:\n    -------\n        Tuple of (is_class, docstring, display_name)\n\n    \"\"\"\n    if method_name == \"__class__\":\n        docstring = inspect.getdoc(obj)\n        class_name = obj.__class__.__name__\n        display_name = class_name.replace(\"Cli\", \"\").replace(\"CLI\", \"\")\n        is_class = True\n    else:\n        method = getattr(obj, method_name, None)\n        if not method or not method.__doc__:\n            console.print(f\"[bold red]No documentation available for {method_name}[/bold red]\")\n            return False, None, \"\"\n        docstring = inspect.getdoc(method)\n        class_name = obj.__class__.__name__\n        display_name = class_name.replace(\"Cli\", \"\").replace(\"CLI\", \"\")\n        is_class = False\n\n    if not docstring:\n        console.print(f\"[bold red]No documentation available for {obj.__class__.__name__}[/bold red]\")\n        return is_class, None, display_name\n\n    return is_class, docstring, display_name\n</code></pre>"},{"location":"api/#nearai.cli_help._extract_description_section","title":"_extract_description_section","text":"<pre><code>_extract_description_section(lines: List[str]) -&gt; List[str]\n</code></pre> <p>Extract the description section from docstring lines.</p> <p>The description is the first part of the docstring until the first section header or double blank line.</p> <pre><code>lines: Lines of the docstring\n</code></pre> <pre><code>List of description lines\n</code></pre> Source code in <code>nearai/cli_help.py</code> <pre><code>def _extract_description_section(lines: List[str]) -&gt; List[str]:\n    \"\"\"Extract the description section from docstring lines.\n\n    The description is the first part of the docstring until the first\n    section header or double blank line.\n\n    Args:\n    ----\n        lines: Lines of the docstring\n\n    Returns:\n    -------\n        List of description lines\n\n    \"\"\"\n    description: List[str] = []\n    if not lines:\n        return description\n\n    # First line is always part of the description\n    description.append(lines[0].strip())\n\n    # Look for continuation of description\n    i = 1\n    while i &lt; len(lines):\n        line = lines[i].strip()\n\n        # If we find an empty line\n        if not line:\n            # Skip the blank line\n            i += 1\n            # Continue until we hit a second blank line or section header\n            while i &lt; len(lines):\n                line = lines[i].strip()\n                if not line:  # Found second blank line\n                    break\n                if re.match(r\"^([A-Za-z][A-Za-z\\s]+):$\", line):  # Found section header\n                    break\n                description.append(line)\n                i += 1\n            break\n        i += 1\n\n    return description\n</code></pre>"},{"location":"api/#nearai.cli_help._parse_command_options","title":"_parse_command_options","text":"<pre><code>_parse_command_options(lines: List[str], current_index: int) -&gt; Tuple[str, int]\n</code></pre> <p>Parse options for a command from the following lines.</p> <pre><code>lines: All command lines\ncurrent_index: Current position in the lines\n</code></pre> <pre><code>Tuple of (options_string, next_index)\n</code></pre> Source code in <code>nearai/cli_help.py</code> <pre><code>def _parse_command_options(lines: List[str], current_index: int) -&gt; Tuple[str, int]:\n    \"\"\"Parse options for a command from the following lines.\n\n    Args:\n    ----\n        lines: All command lines\n        current_index: Current position in the lines\n\n    Returns:\n    -------\n        Tuple of (options_string, next_index)\n\n    \"\"\"\n    options = []\n    j = current_index + 1\n    in_options = False\n    next_index = current_index\n\n    # Check if next line starts with options\n    if j &lt; len(lines) and lines[j].strip().startswith(\"(\"):\n        in_options = True\n        # Process option lines until we find the closing parenthesis\n        while j &lt; len(lines) and in_options:\n            opt_line = lines[j].strip()\n            options.append(opt_line)\n\n            if opt_line.endswith(\")\"):\n                in_options = False\n            j += 1\n\n        next_index = j - 1  # Skip the processed option lines\n\n    # Join and format options\n    options_str = \"\"\n    if options:\n        # Remove parentheses and join with spaces\n        options_str = \" \".join(options)\n        options_str = options_str.strip(\"() \")\n\n    return options_str, next_index\n</code></pre>"},{"location":"api/#nearai.cli_help._parse_docstring_sections","title":"_parse_docstring_sections","text":"<pre><code>_parse_docstring_sections(docstring: str) -&gt; Dict[str, List[str]]\n</code></pre> <p>Parse a docstring into sections.</p> <pre><code>docstring: The docstring to parse\n</code></pre> <pre><code>Dictionary mapping section names to content lines\n</code></pre> Source code in <code>nearai/cli_help.py</code> <pre><code>def _parse_docstring_sections(docstring: str) -&gt; Dict[str, List[str]]:\n    \"\"\"Parse a docstring into sections.\n\n    Args:\n    ----\n        docstring: The docstring to parse\n\n    Returns:\n    -------\n        Dictionary mapping section names to content lines\n\n    \"\"\"\n    sections = {}\n    lines = docstring.split(\"\\n\")\n\n    # First, extract the description section (special handling)\n    sections[\"description\"] = _extract_description_section(lines)\n\n    # Then extract all other sections\n    section_pattern = r\"^([A-Za-z][A-Za-z\\s]+):$\"\n    current_section = None\n    section_content: List[str] = []\n\n    i = 0\n    while i &lt; len(lines):\n        line = lines[i].strip()\n\n        # Skip empty lines\n        if not line:\n            i += 1\n            continue\n\n        # Check if this is a section header\n        section_match = re.match(section_pattern, line)\n        if section_match:\n            # Save previous section content if we had one\n            if current_section and section_content:\n                sections[current_section.lower()] = section_content\n\n            # Start new section\n            current_section = section_match.group(1)\n            section_content = []\n\n            # Skip decoration lines (like \"-----\")\n            if i + 1 &lt; len(lines) and re.match(r\"^-+$\", lines[i + 1].strip()):\n                i += 1\n\n        # If not a section header and we're in a section, add the line\n        elif current_section:\n            # For Commands section, preserve original line with indentation\n            if current_section.lower() == \"commands\":\n                section_content.append(lines[i])  # Keep original indentation\n            else:\n                # For other sections, just add the content if not empty\n                if line:\n                    section_content.append(line)\n\n        i += 1\n\n    # Save the last section\n    if current_section and section_content:\n        sections[current_section.lower()] = section_content\n\n    return sections\n</code></pre>"},{"location":"api/#nearai.cli_help._parse_param_description","title":"_parse_param_description","text":"<pre><code>_parse_param_description(section_lines: List[str], current_index: int, param_pattern: str) -&gt; Tuple[str, int]\n</code></pre> <p>Parse the description lines for a parameter.</p> <pre><code>section_lines: Lines in the current section\ncurrent_index: Current position in the lines\nparam_pattern: Pattern to identify parameter definitions\n</code></pre> <pre><code>Tuple of (description_text, next_index)\n</code></pre> Source code in <code>nearai/cli_help.py</code> <pre><code>def _parse_param_description(section_lines: List[str], current_index: int, param_pattern: str) -&gt; Tuple[str, int]:\n    \"\"\"Parse the description lines for a parameter.\n\n    Args:\n    ----\n        section_lines: Lines in the current section\n        current_index: Current position in the lines\n        param_pattern: Pattern to identify parameter definitions\n\n    Returns:\n    -------\n        Tuple of (description_text, next_index)\n\n    \"\"\"\n    description_lines = []\n    i = current_index + 1  # Start from next line\n\n    # Continue collecting description lines until we hit the next parameter or the end\n    while i &lt; len(section_lines):\n        next_line = section_lines[i].rstrip()\n\n        # Skip empty lines in description\n        if not next_line:\n            i += 1\n            continue\n\n        # If we encounter another parameter definition, stop collecting description\n        if re.match(param_pattern, next_line):\n            break\n\n        # Add the line to the description, removing excess indentation\n        description_lines.append(next_line.lstrip())\n        i += 1\n\n    description = \" \".join(description_lines) if description_lines else \"\"\n    return description, i\n</code></pre>"},{"location":"api/#nearai.cli_help.format_help","title":"format_help","text":"<pre><code>format_help(obj, method_name: str = '__class__') -&gt; None\n</code></pre> <p>Format a class or method's docstring as a help message and display it with rich formatting.</p> <pre><code>obj : Any\n    The object containing the docstring (class or method)\nmethod_name : str\n    The name of the method to format, or \"__class__\" to format the class's docstring\n</code></pre> Source code in <code>nearai/cli_help.py</code> <pre><code>def format_help(obj, method_name: str = \"__class__\") -&gt; None:\n    \"\"\"Format a class or method's docstring as a help message and display it with rich formatting.\n\n    Args:\n    ----\n        obj : Any\n            The object containing the docstring (class or method)\n        method_name : str\n            The name of the method to format, or \"__class__\" to format the class's docstring\n\n    \"\"\"\n    console = Console()\n\n    # Special case for CLI main menu\n    if method_name == \"__class__\" and obj.__class__.__name__ == \"CLI\":\n        generate_main_cli_help(obj)\n        return\n\n    # Get docstring info from class or method\n    docstring, cmd_title, is_class, sections = get_docstring_info(obj, method_name)\n    if docstring is None or sections is None:\n        return\n\n    # Display command group / name\n    console.print(f\"\\n[bold green]{cmd_title}[/bold green]\\n\")\n\n    # Process each type of section\n    _display_description_section(console, sections)\n\n    if is_class and \"commands\" in sections:\n        _display_commands_section(console, sections)\n\n    # Process parameter sections\n    param_pattern = r\"^\\s*(\\S+)\\s*\\((\\S+)\\)\\s*:\\s*$\"\n    if \"args\" in sections:\n        _display_param_section(console, sections, \"args\", \"Args\", param_pattern, obj, method_name)\n\n    if \"options\" in sections:\n        _display_param_section(console, sections, \"options\", \"Options\", param_pattern, obj, method_name)\n\n    if \"examples\" in sections:\n        _display_examples_section(console, sections)\n\n    if \"documentation\" in sections:\n        _display_documentation_section(console, sections)\n</code></pre>"},{"location":"api/#nearai.cli_help.generate_main_cli_help","title":"generate_main_cli_help","text":"<pre><code>generate_main_cli_help(cli: CLI) -&gt; None\n</code></pre> <p>Format the main CLI menu help display.</p> <pre><code>cli: The CLI class instance\n</code></pre> Source code in <code>nearai/cli_help.py</code> <pre><code>def generate_main_cli_help(cli: \"CLI\") -&gt; None:\n    \"\"\"Format the main CLI menu help display.\n\n    Args:\n    ----\n        cli: The CLI class instance\n\n    \"\"\"\n    console = Console()\n\n    # Display banner and version\n    version = importlib.metadata.version(\"nearai\")\n    console.print(NEAR_AI_BANNER)\n    console.print(f\"[bold cyan]NEAR AI CLI[/bold cyan] [dim]v{version}[/dim]\")\n\n    # Get CLI docstring\n    docstring = inspect.getdoc(cli)\n    if not docstring:\n        console.print(\"[bold red]No documentation available for the CLI[/bold red]\")\n        return\n\n    # Single table for all commands\n    table = Table(\n        box=ROUNDED,\n        expand=False,\n        show_header=True,\n        header_style=\"bold cyan\",\n        border_style=\"green\",\n    )\n    table.add_column(\"Command\", style=\"cyan\")\n    table.add_column(\"Description\", style=\"white\")\n\n    # Parse docstring into sections\n    sections = {}\n    current_section = None\n    current_lines: List[str] = []\n\n    # Process the docstring line by line\n    for line in docstring.strip().split(\"\\n\"):\n        line = line.strip()\n        # Skip empty lines\n        if not line:\n            continue\n        # Check if this is a section header\n        if line.endswith(\":\"):\n            # Save previous section if we had one\n            if current_section:\n                sections[current_section.lower()] = current_lines\n            # Start a new section\n            current_section = line.rstrip(\":\")\n            current_lines = []\n        elif current_section:\n            # Add content to the current section\n            current_lines.append(line)\n\n    # Save the last section if we have one\n    if current_section:\n        sections[current_section.lower()] = current_lines\n\n    # Process each section in order they appeared in the docstring\n    first_section = True\n    for section_name, section_lines in sections.items():\n        # Add separator between sections (except first one)\n        if not first_section:\n            table.add_row(\"\", \"\")  # Blank row as separator\n        else:\n            first_section = False\n\n        # Add section header\n        table.add_row(f\"[bold green]{section_name.title()}[/bold green]\", \"\")\n\n        # Add commands for this section\n        for cmd_line in section_lines:\n            # Process command line - split by 2+ spaces\n            parts = re.split(r\"\\s{2,}\", cmd_line, maxsplit=1)\n            if len(parts) == 2:\n                cmd, desc = parts\n                # Use the command as is without adding prefix\n                cmd = cmd.strip()\n                table.add_row(cmd, desc.strip())\n            else:\n                # For single-word commands, use as is\n                cmd = cmd_line.strip()\n                if not cmd.startswith(\"[\"):\n                    table.add_row(cmd, \"\")\n\n    console.print(table)\n    console.print(\n        \"[bold white] Run [bold green]`nearai &lt;command&gt; --help`[/bold green] for more info about a command.\\n[/bold white]\"  # noqa: E501\n    )\n    console.print(\n        \"[white] - Docs: [bold blue]https://docs.near.ai/[/bold blue][/white]\\n\"\n        \"[white] - Dev Support: [bold blue]https://t.me/nearaialpha[/bold blue][/white]\\n\"\n    )\n</code></pre>"},{"location":"api/#nearai.cli_help.get_docstring_info","title":"get_docstring_info","text":"<pre><code>get_docstring_info(obj, method_name: str = '__class__') -&gt; Tuple[Optional[str], Optional[str], bool, Optional[Dict[str, List[str]]]]\n</code></pre> <p>Get the docstring, command title, and parsed sections for a class or method.</p> <pre><code>obj : Any\n    The object containing the docstring (class or method)\nmethod_name : str\n    The name of the method to format, or \"__class__\" to format the class's docstring\n</code></pre> <pre><code>Tuple of (docstring, command_title, is_class, sections)\n</code></pre> Source code in <code>nearai/cli_help.py</code> <pre><code>def get_docstring_info(\n    obj, method_name: str = \"__class__\"\n) -&gt; Tuple[Optional[str], Optional[str], bool, Optional[Dict[str, List[str]]]]:\n    \"\"\"Get the docstring, command title, and parsed sections for a class or method.\n\n    Args:\n    ----\n        obj : Any\n            The object containing the docstring (class or method)\n        method_name : str\n            The name of the method to format, or \"__class__\" to format the class's docstring\n\n    Returns:\n    -------\n        Tuple of (docstring, command_title, is_class, sections)\n\n    \"\"\"\n    console = Console()\n\n    # Extract basic metadata and docstring\n    is_class, docstring, display_name = _extract_basic_metadata(obj, method_name, console)\n    if docstring is None:\n        return None, None, False, None\n\n    # Create command title based on whether it's a class or method\n    if is_class:\n        cmd_title = f\"NEAR AI {display_name} Commands\"\n    else:\n        cmd_title = f\"[bold white]nearai {display_name.lower()} {method_name} [/bold white]\"\n\n    # Parse docstring into sections\n    sections = _parse_docstring_sections(docstring)\n\n    return docstring, cmd_title, is_class, sections\n</code></pre>"},{"location":"api/#nearai.cli_help.handle_help_request","title":"handle_help_request","text":"<pre><code>handle_help_request(args: Optional[List[str]] = None) -&gt; bool\n</code></pre> <p>Common handler for CLI help requests.</p> <pre><code>args (Optional[List[str]]) :\n    Command line arguments (uses sys.argv if None)\n</code></pre> <pre><code>bool : True if help was displayed, False otherwise\n</code></pre> Source code in <code>nearai/cli_help.py</code> <pre><code>def handle_help_request(args: Optional[List[str]] = None) -&gt; bool:\n    \"\"\"Common handler for CLI help requests.\n\n    Args:\n    ----\n        args (Optional[List[str]]) :\n            Command line arguments (uses sys.argv if None)\n\n    Returns:\n    -------\n        bool : True if help was displayed, False otherwise\n\n    \"\"\"\n    if args is None:\n        import sys\n\n        args = sys.argv\n\n    # Create CLI instance\n    from nearai.cli import CLI\n\n    cli = CLI()\n\n    # Special case for agent upload, which is an alias for registry upload\n    if len(args) == 4 and args[1] == \"agent\" and args[2] == \"upload\" and args[3] == \"--help\":\n        # Display help for registry upload subcommand\n        if hasattr(cli, \"registry\"):\n            registry_obj = cli.registry\n            if hasattr(registry_obj, \"upload\"):\n                format_help(registry_obj, \"upload\")\n                return True\n        return False\n\n    # No arguments - show main help\n    if len(args) == 1:\n        format_help(cli, \"__class__\")\n        return True\n\n    # Help with no specific command\n    if len(args) == 2 and args[1] == \"--help\":\n        format_help(cli, \"__class__\")\n        return True\n\n    # Help for a specific command\n    if len(args) == 3 and args[2] == \"--help\":\n        command = args[1]\n        if hasattr(cli, command):\n            format_help(getattr(cli, command))\n            return True\n        return False\n\n    # Help for a specific subcommand\n    if len(args) == 4 and args[3] == \"--help\":\n        command = args[1]\n        subcommand = args[2]\n        if hasattr(cli, command):\n            cmd_obj = getattr(cli, command)\n            if hasattr(cmd_obj, subcommand):\n                format_help(cmd_obj, subcommand)\n                return True\n        return False\n\n    return False\n</code></pre>"},{"location":"api/#nearai.cli_helpers","title":"cli_helpers","text":""},{"location":"api/#nearai.cli_helpers.assert_user_auth","title":"assert_user_auth","text":"<pre><code>assert_user_auth() -&gt; None\n</code></pre> <p>Ensure the user is authenticated.</p>"},{"location":"api/#nearai.cli_helpers.assert_user_auth--raises","title":"Raises","text":"<pre><code>SystemExit: If the user is not authenticated\n</code></pre> Source code in <code>nearai/cli_helpers.py</code> <pre><code>def assert_user_auth() -&gt; None:\n    \"\"\"Ensure the user is authenticated.\n\n    Raises\n    ------\n        SystemExit: If the user is not authenticated\n\n    \"\"\"\n    from nearai.config import CONFIG\n\n    if CONFIG.auth is None:\n        print(\"Please login with `nearai login` first\")\n        exit(1)\n</code></pre>"},{"location":"api/#nearai.cli_helpers.display_agents_in_columns","title":"display_agents_in_columns","text":"<pre><code>display_agents_in_columns(agents: list[Path]) -&gt; None\n</code></pre> <p>Display agents in a rich table format.</p> <pre><code>agents: List of Path objects pointing to agent locations (pre-sorted)\n</code></pre> Source code in <code>nearai/cli_helpers.py</code> <pre><code>def display_agents_in_columns(agents: list[Path]) -&gt; None:\n    \"\"\"Display agents in a rich table format.\n\n    Args:\n    ----\n        agents: List of Path objects pointing to agent locations (pre-sorted)\n\n    \"\"\"\n    # Create table\n    table = Table(title=\"Available Agents\", show_header=True, header_style=\"bold\", show_lines=True, expand=True)\n\n    # Add columns\n    table.add_column(\"#\", style=\"bold\", width=4)\n    table.add_column(\"Namespace\", style=\"blue\")\n    table.add_column(\"Agent Name\", style=\"cyan\")\n    table.add_column(\"Version\", style=\"green\")\n    table.add_column(\"Description\", style=\"white\")\n    table.add_column(\"Tags\", style=\"yellow\")\n\n    # Add rows\n    for idx, agent_path in enumerate(agents, 1):\n        try:\n            # Read metadata for additional info\n            with open(agent_path / \"metadata.json\") as f:\n                metadata = json.load(f)\n                description = metadata.get(\"description\", \"No description\")\n                tags = metadata.get(\"tags\", [])\n        except (FileNotFoundError, json.JSONDecodeError):\n            description = \"Unable to load metadata\"\n            tags = []\n\n        # Add row to table with separated path components\n        table.add_row(\n            str(idx),\n            agent_path.parts[-3],  # namespace\n            agent_path.parts[-2],  # agent name\n            agent_path.parts[-1],  # version\n            description,\n            \", \".join(tags) if tags else \"\u2014\",\n        )\n\n    # Display table\n    console = Console()\n    console.print(\"\\n\")\n    console.print(table)\n    console.print(\"\\n\")\n</code></pre>"},{"location":"api/#nearai.cli_helpers.display_version_check","title":"display_version_check","text":"<pre><code>display_version_check(namespace: str, name: str, version: str, exists: bool) -&gt; None\n</code></pre> <p>Display formatted message about version existence check.</p> <pre><code>namespace: The namespace\nname: The agent name\nversion: The version being checked\nexists: Whether the version exists\n</code></pre> Source code in <code>nearai/cli_helpers.py</code> <pre><code>def display_version_check(namespace: str, name: str, version: str, exists: bool) -&gt; None:\n    \"\"\"Display formatted message about version existence check.\n\n    Args:\n    ----\n        namespace: The namespace\n        name: The agent name\n        version: The version being checked\n        exists: Whether the version exists\n\n    \"\"\"\n    console = Console()\n    console.print(\n        Text.assemble(\n            (\"\\n\ud83d\udd0e Checking if version \", \"white\"),\n            (f\"{version}\", \"green bold\"),\n            (\" exists for \", \"white\"),\n            (f\"{name} \", \"blue bold\"),\n            (\"in the registry under \", \"white\"),\n            (f\"{namespace}\", \"cyan bold\"),\n            (\"...\", \"white\"),\n        )\n    )\n\n    if exists:\n        console.print(f\"\\n\u274c [yellow]Version [cyan]{version}[/cyan] already exists.[/yellow]\")\n    else:\n        console.print(f\"\\n\u2705 [green]Version [cyan]{version}[/cyan] is available.[/green]\")\n</code></pre>"},{"location":"api/#nearai.cli_helpers.has_pending_input","title":"has_pending_input","text":"<pre><code>has_pending_input()\n</code></pre> <p>Check if there's input waiting to be read without blocking.</p> Source code in <code>nearai/cli_helpers.py</code> <pre><code>def has_pending_input():\n    \"\"\"Check if there's input waiting to be read without blocking.\"\"\"\n    if os.name == \"nt\":  # Windows\n        import msvcrt\n\n        return msvcrt.kbhit()\n    else:  # Unix/Linux/Mac\n        rlist, _, _ = select.select([sys.stdin], [], [], 0)\n        return bool(rlist)\n</code></pre>"},{"location":"api/#nearai.cli_helpers.load_and_validate_metadata","title":"load_and_validate_metadata","text":"<pre><code>load_and_validate_metadata(metadata_path: Path) -&gt; Tuple[Optional[Dict[str, Any]], Optional[str]]\n</code></pre> <p>Load and validate metadata file, including version format.</p> <pre><code>metadata_path: Path to metadata.json file\n</code></pre> <pre><code>Tuple of (metadata_dict, error_message)\n</code></pre> Source code in <code>nearai/cli_helpers.py</code> <pre><code>def load_and_validate_metadata(metadata_path: Path) -&gt; Tuple[Optional[Dict[str, Any]], Optional[str]]:\n    \"\"\"Load and validate metadata file, including version format.\n\n    Args:\n    ----\n        metadata_path: Path to metadata.json file\n\n    Returns:\n    -------\n        Tuple of (metadata_dict, error_message)\n\n    \"\"\"\n    try:\n        with open(metadata_path) as f:\n            metadata = json.load(f)\n\n        # Validate version format\n        if \"version\" not in metadata:\n            return None, \"Metadata file must contain a 'version' field\"\n\n        is_valid, error = validate_version(metadata[\"version\"])\n        if not is_valid:\n            return None, error\n\n        return metadata, None\n    except FileNotFoundError:\n        return None, f\"Metadata file not found at {metadata_path}\"\n    except json.JSONDecodeError:\n        return None, f\"Invalid JSON in metadata file at {metadata_path}\"\n    except Exception as e:\n        return None, f\"Error reading metadata file: {str(e)}\"\n</code></pre>"},{"location":"api/#nearai.config","title":"config","text":""},{"location":"api/#nearai.config.Config","title":"Config","text":"<p>               Bases: <code>BaseModel</code></p> Source code in <code>nearai/config.py</code> <pre><code>class Config(BaseModel):\n    origin: Optional[str] = None\n    api_url: Optional[str] = \"https://api.near.ai\"\n    inference_url: str = \"http://localhost:5000/v1/\"\n    inference_api_key: str = \"n/a\"\n    scheduler_account_id: str = \"nearaischeduler.near\"\n    nearai_hub: NearAiHubConfig = NearAiHubConfig()\n    confirm_commands: bool = True\n    auth: Optional[AuthData] = None\n    num_inference_retries: int = 1\n\n    def update_with(self, extra_config: Dict[str, Any], map_key: Callable[[str], str] = lambda x: x) -&gt; \"Config\":\n        \"\"\"Update the config with the given dictionary.\"\"\"\n        dict_repr = self.model_dump()\n        keys = list(map(map_key, dict_repr.keys()))\n\n        for key in keys:\n            value = extra_config.get(key, None)\n\n            if value:\n                # This will skip empty values, even if they are set in the `extra_config`\n                dict_repr[key] = value\n\n        return Config.model_validate(dict_repr)\n\n    def get(self, key: str, default: Optional[Any] = None) -&gt; Optional[Any]:\n        \"\"\"Get the value of a key in the config if it exists.\"\"\"\n        return getattr(self, key, default)\n\n    def get_client_config(self) -&gt; ClientConfig:  # noqa: D102\n        return ClientConfig(\n            base_url=self.nearai_hub.base_url,\n            auth=self.auth,\n            custom_llm_provider=self.nearai_hub.custom_llm_provider,\n            default_provider=self.nearai_hub.default_provider,\n            num_inference_retries=self.num_inference_retries,\n        )\n</code></pre>"},{"location":"api/#nearai.config.Config.get","title":"get","text":"<pre><code>get(key: str, default: Optional[Any] = None) -&gt; Optional[Any]\n</code></pre> <p>Get the value of a key in the config if it exists.</p> Source code in <code>nearai/config.py</code> <pre><code>def get(self, key: str, default: Optional[Any] = None) -&gt; Optional[Any]:\n    \"\"\"Get the value of a key in the config if it exists.\"\"\"\n    return getattr(self, key, default)\n</code></pre>"},{"location":"api/#nearai.config.Config.update_with","title":"update_with","text":"<pre><code>update_with(extra_config: Dict[str, Any], map_key: Callable[[str], str] = lambda x: x) -&gt; Config\n</code></pre> <p>Update the config with the given dictionary.</p> Source code in <code>nearai/config.py</code> <pre><code>def update_with(self, extra_config: Dict[str, Any], map_key: Callable[[str], str] = lambda x: x) -&gt; \"Config\":\n    \"\"\"Update the config with the given dictionary.\"\"\"\n    dict_repr = self.model_dump()\n    keys = list(map(map_key, dict_repr.keys()))\n\n    for key in keys:\n        value = extra_config.get(key, None)\n\n        if value:\n            # This will skip empty values, even if they are set in the `extra_config`\n            dict_repr[key] = value\n\n    return Config.model_validate(dict_repr)\n</code></pre>"},{"location":"api/#nearai.config.NearAiHubConfig","title":"NearAiHubConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>NearAiHub Config.</p> <p>login_with_near (Optional[bool]): Indicates whether to attempt login using Near Auth.</p> <p>api_key (Optional[str]): The API key to use if Near Auth is not being utilized</p> <p>base_url (Optional[str]): NEAR AI Hub url</p> <p>default_provider (Optional[str]): Default provider name</p> <p>default_model (Optional[str]): Default model name</p> <p>custom_llm_provider (Optional[str]): provider to be used by litellm proxy</p> Source code in <code>nearai/config.py</code> <pre><code>class NearAiHubConfig(BaseModel):\n    \"\"\"NearAiHub Config.\n\n    login_with_near (Optional[bool]): Indicates whether to attempt login using Near Auth.\n\n    api_key (Optional[str]): The API key to use if Near Auth is not being utilized\n\n    base_url (Optional[str]): NEAR AI Hub url\n\n    default_provider (Optional[str]): Default provider name\n\n    default_model (Optional[str]): Default model name\n\n    custom_llm_provider (Optional[str]): provider to be used by litellm proxy\n    \"\"\"\n\n    base_url: str = \"https://api.near.ai/v1\"\n    default_provider: str = DEFAULT_PROVIDER\n    default_model: str = DEFAULT_PROVIDER_MODEL\n    custom_llm_provider: str = \"openai\"\n    login_with_near: Optional[bool] = True\n    api_key: Optional[str] = \"\"\n</code></pre>"},{"location":"api/#nearai.dataset","title":"dataset","text":""},{"location":"api/#nearai.dataset.get_dataset","title":"get_dataset","text":"<pre><code>get_dataset(name: str, verbose: bool = True) -&gt; Path\n</code></pre> <p>Download the dataset from the registry and download it locally if it hasn't been downloaded yet.</p> <p>:param name: The name of the entry to download the dataset. The format should be namespace/name/version. :return: The path to the downloaded dataset</p> Source code in <code>nearai/dataset.py</code> <pre><code>def get_dataset(name: str, verbose: bool = True) -&gt; Path:\n    \"\"\"Download the dataset from the registry and download it locally if it hasn't been downloaded yet.\n\n    :param name: The name of the entry to download the dataset. The format should be namespace/name/version.\n    :return: The path to the downloaded dataset\n    \"\"\"\n    return registry.download(name, verbose=verbose)\n</code></pre>"},{"location":"api/#nearai.dataset.load_dataset","title":"load_dataset","text":"<pre><code>load_dataset(alias_or_name: str, verbose: bool = True) -&gt; Union[Dataset, DatasetDict]\n</code></pre> <p>Load a dataset from the registry.</p> Source code in <code>nearai/dataset.py</code> <pre><code>def load_dataset(alias_or_name: str, verbose: bool = True) -&gt; Union[Dataset, DatasetDict]:\n    \"\"\"Load a dataset from the registry.\"\"\"\n    path = get_dataset(alias_or_name, verbose=verbose)\n    return load_from_disk(path.as_posix())\n</code></pre>"},{"location":"api/#nearai.delegation","title":"delegation","text":""},{"location":"api/#nearai.delegation.OnBehalfOf","title":"OnBehalfOf","text":"<p>Create a context manager that allows you to delegate actions to another account.</p> <pre><code>with OnBehalfOf(\"scheduler.ai\"):\n    # Upload is done on behalf of scheduler.ai\n    # If delegation permission is not granted, this will raise an exception\n    registry.upload()\n</code></pre> Source code in <code>nearai/delegation.py</code> <pre><code>class OnBehalfOf:\n    \"\"\"Create a context manager that allows you to delegate actions to another account.\n\n    ```python\n    with OnBehalfOf(\"scheduler.ai\"):\n        # Upload is done on behalf of scheduler.ai\n        # If delegation permission is not granted, this will raise an exception\n        registry.upload()\n    ```\n    \"\"\"\n\n    def __init__(self, on_behalf_of: str):\n        \"\"\"Context manager that creates a scope where all actions are done on behalf of another account.\"\"\"\n        self.target_on_behalf_of = on_behalf_of\n        self.original_access_token = None\n\n    def __enter__(self):\n        \"\"\"Set the default client to the account we are acting on behalf of.\"\"\"\n        default_client = ApiClient.get_default()\n        self.original_access_token = default_client.configuration.access_token\n\n        if not isinstance(self.original_access_token, str):\n            return\n\n        assert self.original_access_token.startswith(\"Bearer \")\n        auth = self.original_access_token[len(\"Bearer \") :]\n        auth_data = AuthData.model_validate_json(auth)\n        auth_data.on_behalf_of = self.target_on_behalf_of\n        new_access_token = f\"Bearer {auth_data.generate_bearer_token()}\"\n        default_client.configuration.access_token = new_access_token\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        \"\"\"Reset the default client to the original account.\"\"\"\n        default_client = ApiClient.get_default()\n        default_client.configuration.access_token = self.original_access_token\n        self.original_access_token = None\n</code></pre>"},{"location":"api/#nearai.delegation.OnBehalfOf.__enter__","title":"__enter__","text":"<pre><code>__enter__()\n</code></pre> <p>Set the default client to the account we are acting on behalf of.</p> Source code in <code>nearai/delegation.py</code> <pre><code>def __enter__(self):\n    \"\"\"Set the default client to the account we are acting on behalf of.\"\"\"\n    default_client = ApiClient.get_default()\n    self.original_access_token = default_client.configuration.access_token\n\n    if not isinstance(self.original_access_token, str):\n        return\n\n    assert self.original_access_token.startswith(\"Bearer \")\n    auth = self.original_access_token[len(\"Bearer \") :]\n    auth_data = AuthData.model_validate_json(auth)\n    auth_data.on_behalf_of = self.target_on_behalf_of\n    new_access_token = f\"Bearer {auth_data.generate_bearer_token()}\"\n    default_client.configuration.access_token = new_access_token\n</code></pre>"},{"location":"api/#nearai.delegation.OnBehalfOf.__exit__","title":"__exit__","text":"<pre><code>__exit__(exc_type, exc_val, exc_tb)\n</code></pre> <p>Reset the default client to the original account.</p> Source code in <code>nearai/delegation.py</code> <pre><code>def __exit__(self, exc_type, exc_val, exc_tb):\n    \"\"\"Reset the default client to the original account.\"\"\"\n    default_client = ApiClient.get_default()\n    default_client.configuration.access_token = self.original_access_token\n    self.original_access_token = None\n</code></pre>"},{"location":"api/#nearai.delegation.OnBehalfOf.__init__","title":"__init__","text":"<pre><code>__init__(on_behalf_of: str)\n</code></pre> <p>Context manager that creates a scope where all actions are done on behalf of another account.</p> Source code in <code>nearai/delegation.py</code> <pre><code>def __init__(self, on_behalf_of: str):\n    \"\"\"Context manager that creates a scope where all actions are done on behalf of another account.\"\"\"\n    self.target_on_behalf_of = on_behalf_of\n    self.original_access_token = None\n</code></pre>"},{"location":"api/#nearai.delegation.check_on_behalf_of","title":"check_on_behalf_of","text":"<pre><code>check_on_behalf_of()\n</code></pre> <p>Check if the request is being made on behalf of another account.</p> Source code in <code>nearai/delegation.py</code> <pre><code>def check_on_behalf_of():\n    \"\"\"Check if the request is being made on behalf of another account.\"\"\"\n    api = DelegationApi()\n    return api.api_client.configuration.access_token\n</code></pre>"},{"location":"api/#nearai.delegation.revoke_delegation","title":"revoke_delegation","text":"<pre><code>revoke_delegation(delegate_account_id: str)\n</code></pre> <p>Revoke delegation to a specific account.</p> Source code in <code>nearai/delegation.py</code> <pre><code>def revoke_delegation(delegate_account_id: str):\n    \"\"\"Revoke delegation to a specific account.\"\"\"\n    DelegationApi().revoke_delegation_v1_delegation_revoke_delegation_post(delegate_account_id)\n</code></pre>"},{"location":"api/#nearai.evaluation","title":"evaluation","text":""},{"location":"api/#nearai.evaluation._print_metrics_tables","title":"_print_metrics_tables","text":"<pre><code>_print_metrics_tables(rows: List[Dict[str, str]], metric_names: List[str], num_columns: int, all_key_columns: bool, metric_name_max_length: int)\n</code></pre> <p>Builds table(s) and prints them.</p> Source code in <code>nearai/evaluation.py</code> <pre><code>def _print_metrics_tables(\n    rows: List[Dict[str, str]],\n    metric_names: List[str],\n    num_columns: int,\n    all_key_columns: bool,\n    metric_name_max_length: int,\n):\n    \"\"\"Builds table(s) and prints them.\"\"\"\n    # Shorten metric names\n    short_metric_names = [_shorten_metric_name(name, metric_name_max_length) for name in metric_names]\n\n    # Prepare the base header and rows\n    base_header = [\"model\", \"agent\"]\n    if all_key_columns:\n        base_header.extend([\"namespace\", \"version\", \"provider\"])\n\n    base_rows = []\n    for row in rows:\n        base_row = [fill(row.pop(\"model\", \"\")), fill(row.pop(\"agent\", \"\"))]\n        namespace = row.pop(\"namespace\", \"\")\n        version = row.pop(\"version\", \"\")\n        provider = row.pop(\"provider\", \"\")\n        if all_key_columns:\n            base_row.extend([fill(namespace), fill(version), fill(provider)])\n        base_rows.append((base_row, row))\n\n    n_metrics_per_table = max(1, num_columns - len(base_header))\n    # Split metrics into groups\n    metric_groups = list(\n        zip(\n            [\n                short_metric_names[i : i + n_metrics_per_table]\n                for i in range(0, len(short_metric_names), n_metrics_per_table)\n            ],\n            [metric_names[i : i + n_metrics_per_table] for i in range(0, len(metric_names), n_metrics_per_table)],\n        )\n    )\n\n    # Print tables\n    for short_group, full_group in metric_groups:\n        header = base_header + short_group\n        table = []\n        for base_row, row_metrics in base_rows:\n            table_row = base_row + [fill(str(row_metrics.get(metric, \"\"))) for metric in full_group]\n            table.append(table_row)\n        print(tabulate(table, headers=header, tablefmt=\"simple_grid\"))\n</code></pre>"},{"location":"api/#nearai.evaluation._shorten_metric_name","title":"_shorten_metric_name","text":"<pre><code>_shorten_metric_name(name: str, max_length: int) -&gt; str\n</code></pre> <p>Shortens metric name if needed.</p> Source code in <code>nearai/evaluation.py</code> <pre><code>def _shorten_metric_name(name: str, max_length: int) -&gt; str:\n    \"\"\"Shortens metric name if needed.\"\"\"\n    if len(name) &lt;= max_length:\n        return name\n    keep = max_length - 2  # 2 dots\n    beginning = keep // 3\n    ending = keep - beginning\n    return name[:beginning] + \"..\" + name[-ending:]\n</code></pre>"},{"location":"api/#nearai.evaluation.load_benchmark_entry_info","title":"load_benchmark_entry_info","text":"<pre><code>load_benchmark_entry_info(info: str) -&gt; Any\n</code></pre> <p>Deserializes benchmark info entry from db data.</p> Source code in <code>nearai/evaluation.py</code> <pre><code>def load_benchmark_entry_info(info: str) -&gt; Any:\n    \"\"\"Deserializes benchmark info entry from db data.\"\"\"\n    first_decode = json.loads(info)\n    try:\n        second_decode = json.loads(first_decode)\n        return second_decode\n    except json.JSONDecodeError as e:\n        if \"Unterminated string\" in str(e):\n            last_brace = first_decode.rfind(\"}\")\n            if last_brace != -1:\n                try:\n                    return json.loads(first_decode[: last_brace + 1])\n                except json.JSONDecodeError as e:\n                    pass\n    return first_decode\n</code></pre>"},{"location":"api/#nearai.evaluation.print_evaluation_table","title":"print_evaluation_table","text":"<pre><code>print_evaluation_table(rows: List[Dict[str, str]], columns: List[str], important_columns: List[str], all_key_columns: bool, all_metrics: bool, num_columns: int, metric_name_max_length: int) -&gt; None\n</code></pre> <p>Prints table of evaluations.</p> Source code in <code>nearai/evaluation.py</code> <pre><code>def print_evaluation_table(\n    rows: List[Dict[str, str]],\n    columns: List[str],\n    important_columns: List[str],\n    all_key_columns: bool,\n    all_metrics: bool,\n    num_columns: int,\n    metric_name_max_length: int,\n) -&gt; None:\n    \"\"\"Prints table of evaluations.\"\"\"\n    metric_names = columns[5:] if all_metrics else important_columns[2:]\n    _print_metrics_tables(rows, metric_names, num_columns, all_key_columns, metric_name_max_length)\n</code></pre>"},{"location":"api/#nearai.evaluation.record_evaluation_metrics","title":"record_evaluation_metrics","text":"<pre><code>record_evaluation_metrics(solver_strategy: SolverStrategy, benchmark_id: int, data_tasks: Union[Dataset, List[dict]], metrics: Dict[str, Any], prepend_evaluation_name: bool = True) -&gt; None\n</code></pre> <p>Uploads evaluation metrics into registry.</p> Source code in <code>nearai/evaluation.py</code> <pre><code>def record_evaluation_metrics(\n    solver_strategy: SolverStrategy,\n    benchmark_id: int,\n    data_tasks: Union[Dataset, List[dict]],\n    metrics: Dict[str, Any],\n    prepend_evaluation_name: bool = True,\n) -&gt; None:\n    \"\"\"Uploads evaluation metrics into registry.\"\"\"\n    evaluation_name = solver_strategy.evaluation_name()\n    model = \"\"\n    agent = \"\"\n    version = \"\"\n    model = solver_strategy.model_name\n    agent = solver_strategy.agent_name()\n    version = solver_strategy.agent_version()\n\n    upload_evaluation(\n        evaluation_name,\n        benchmark_id,\n        data_tasks,\n        metrics if not prepend_evaluation_name else _prepend_name_to_metrics(evaluation_name, metrics),\n        model,\n        agent,\n        solver_strategy.evaluated_entry_namespace(),\n        version,\n        solver_strategy.model_provider(),\n    )\n</code></pre>"},{"location":"api/#nearai.evaluation.record_single_score_evaluation","title":"record_single_score_evaluation","text":"<pre><code>record_single_score_evaluation(solver_strategy: SolverStrategy, benchmark_id: int, data_tasks: Union[Dataset, List[dict]], score: float) -&gt; None\n</code></pre> <p>Uploads single score evaluation into registry.</p> Source code in <code>nearai/evaluation.py</code> <pre><code>def record_single_score_evaluation(\n    solver_strategy: SolverStrategy, benchmark_id: int, data_tasks: Union[Dataset, List[dict]], score: float\n) -&gt; None:\n    \"\"\"Uploads single score evaluation into registry.\"\"\"\n    evaluation_name = solver_strategy.evaluation_name()\n    record_evaluation_metrics(solver_strategy, benchmark_id, data_tasks, {evaluation_name: score}, False)\n</code></pre>"},{"location":"api/#nearai.evaluation.upload_evaluation","title":"upload_evaluation","text":"<pre><code>upload_evaluation(evaluation_name: str, benchmark_id: int, data_tasks: Union[Dataset, List[dict]], metrics: Dict[str, Any], model: str = '', agent: str = '', namespace: str = '', version: str = '', provider: str = '') -&gt; None\n</code></pre> <p>Uploads evaluation into registry.</p> <p><code>evaluation_name</code>: a unique name for (benchmark, solver) tuple, e.g. \"mbpp\" or \"live_bench\" or \"mmlu-5-shot\". <code>metrics</code>: metrics from evaluation. <code>model</code>: model that was used. <code>agent</code>: agent that was evaluated, in any. <code>namespace</code>: namespace of evaluated agent or evaluated model. <code>version</code>: version of evaluated agent or evaluated model. <code>provider</code>: provider of model used; pass <code>local</code> if running locally.</p> Source code in <code>nearai/evaluation.py</code> <pre><code>def upload_evaluation(\n    evaluation_name: str,\n    benchmark_id: int,\n    data_tasks: Union[Dataset, List[dict]],\n    metrics: Dict[str, Any],\n    model: str = \"\",\n    agent: str = \"\",\n    namespace: str = \"\",\n    version: str = \"\",\n    provider: str = \"\",\n) -&gt; None:\n    \"\"\"Uploads evaluation into registry.\n\n    `evaluation_name`: a unique name for (benchmark, solver) tuple, e.g. \"mbpp\" or \"live_bench\" or \"mmlu-5-shot\".\n    `metrics`: metrics from evaluation.\n    `model`: model that was used.\n    `agent`: agent that was evaluated, in any.\n    `namespace`: namespace of evaluated agent or evaluated model.\n    `version`: version of evaluated agent or evaluated model.\n    `provider`: provider of model used; pass `local` if running locally.\n    \"\"\"\n    key = f\"evaluation_{evaluation_name}\"\n    metrics[EVALUATED_ENTRY_METADATA] = {}\n    if agent != \"\":\n        metrics[EVALUATED_ENTRY_METADATA][\"agent\"] = agent\n        key += f\"_agent_{agent}\"\n    if model != \"\":\n        metrics[EVALUATED_ENTRY_METADATA][\"model\"] = model\n        key += f\"_model_{model}\"\n    if namespace != \"\":\n        metrics[EVALUATED_ENTRY_METADATA][\"namespace\"] = namespace\n        key += f\"_namespace_{namespace}\"\n    if version != \"\":\n        metrics[EVALUATED_ENTRY_METADATA][\"version\"] = version\n        key += f\"_version_{version}\"\n    if provider != \"\":\n        metrics[EVALUATED_ENTRY_METADATA][\"provider\"] = provider\n        # Url providers like 'https://api.openai.com/v1' can't be included into registry entry name\n        # because of special characters.\n        clean_provider = re.sub(r\"[^a-zA-Z0-9_\\-.]\", \"_\", provider)\n        key += f\"_provider_{clean_provider}\"\n\n    entry_path = get_registry_folder() / key\n    # Create folder entry_path if not present\n    entry_path.mkdir(parents=True, exist_ok=True)\n    # Write file metrics.json inside\n    metrics_file = entry_path / \"metrics.json\"\n    with metrics_file.open(\"w\") as f:\n        json.dump(metrics, f, indent=2)\n\n    # Get solutions from cache in benchmark.py\n    cache = BenchmarkApi().get_benchmark_result_v1_benchmark_get_result_get(benchmark_id)\n    solutions = []\n    for result in cache:\n        try:\n            solution = {\n                \"datum\": data_tasks[result.index],\n                \"status\": result.solved,\n                \"info\": load_benchmark_entry_info(result.info) if result.info else {},\n            }\n            solutions.append(solution)\n        except (AttributeError, json.JSONDecodeError, TypeError) as e:\n            print(f\"Exception while creating solutions data: {str(e)}.\")\n            # Skip entries that can't be properly formatted\n            continue\n\n    # Write solutions file\n    solutions_file = entry_path / \"solutions.json\"\n    with solutions_file.open(\"w\") as f:\n        json.dump(solutions, f, indent=2)\n\n    metadata_path = entry_path / \"metadata.json\"\n    # TODO(#273): Currently that will not update existing evaluation.\n    with open(metadata_path, \"w\") as f:\n        json.dump(\n            {\n                \"name\": key,\n                \"version\": \"0.1.0\",\n                \"description\": \"\",\n                \"category\": \"evaluation\",\n                \"tags\": [],\n                \"details\": {},\n                \"show_entry\": True,\n            },\n            f,\n            indent=2,\n        )\n\n    registry.upload(Path(entry_path), show_progress=True)\n</code></pre>"},{"location":"api/#nearai.hub","title":"hub","text":""},{"location":"api/#nearai.hub.Hub","title":"Hub","text":"<p>               Bases: <code>object</code></p> Source code in <code>nearai/hub.py</code> <pre><code>class Hub(object):\n    def __init__(self, config: Config) -&gt; None:\n        \"\"\"Initializes the Hub class with the given configuration.\"\"\"\n        self.info = None\n        self.provider = None\n        self.model = None\n        self.endpoint = None\n        self.query = None\n        self._config = config\n\n    def parse_hub_chat_params(self, kwargs):\n        \"\"\"Parses and sets instance attributes from the given keyword arguments, using default values if needed.\"\"\"\n        if self._config.nearai_hub is None:\n            self._config.nearai_hub = NearAiHubConfig()\n\n        self.query = kwargs.get(\"query\")\n        self.endpoint = kwargs.get(\"endpoint\", f\"{self._config.nearai_hub.base_url}/chat/completions\")\n        self.model = kwargs.get(\"model\", self._config.nearai_hub.default_model)\n        self.provider = kwargs.get(\"provider\", self._config.nearai_hub.default_provider)\n        self.info = kwargs.get(\"info\", False)\n\n    def chat(self, kwargs):\n        \"\"\"Processes a chat request by sending parameters to the NEAR AI Hub and printing the response.\"\"\"\n        try:\n            self.parse_hub_chat_params(kwargs)\n\n            if not self.query:\n                return print(\"Error: 'query' is required for the `hub chat` command.\")\n\n            if self._config.nearai_hub is None:\n                self._config.nearai_hub = NearAiHubConfig()\n\n            data = {\n                \"max_tokens\": 256,\n                \"temperature\": 1,\n                \"frequency_penalty\": 0,\n                \"n\": 1,\n                \"messages\": [{\"role\": \"user\", \"content\": str(self.query)}],\n                \"model\": self.model,\n            }\n\n            auth = self._config.auth\n\n            if self._config.nearai_hub.login_with_near:\n                bearer_token = auth.generate_bearer_token()\n                headers = {\"Content-Type\": \"application/json\", \"Authorization\": f\"Bearer {bearer_token}\"}\n\n                data[\"provider\"] = self.provider\n            elif self._config.nearai_hub.api_key:\n                headers = {\n                    \"Content-Type\": \"application/json\",\n                    \"Authorization\": \"Bearer {}\".format(self._config.nearai_hub.api_key),\n                }\n            else:\n                return print(\"Illegal NEAR AI Hub Config\")\n\n            if self.info:\n                print(f\"Requesting hub using NEAR Account {auth.account_id}\")\n\n            response = requests.post(self.endpoint, headers=headers, data=json.dumps(data))\n\n            completion = response.json()\n\n            print(completion[\"choices\"][0][\"message\"][\"content\"])\n\n        except Exception as e:\n            print(f\"Request failed: {e}\")\n</code></pre>"},{"location":"api/#nearai.hub.Hub.__init__","title":"__init__","text":"<pre><code>__init__(config: Config) -&gt; None\n</code></pre> <p>Initializes the Hub class with the given configuration.</p> Source code in <code>nearai/hub.py</code> <pre><code>def __init__(self, config: Config) -&gt; None:\n    \"\"\"Initializes the Hub class with the given configuration.\"\"\"\n    self.info = None\n    self.provider = None\n    self.model = None\n    self.endpoint = None\n    self.query = None\n    self._config = config\n</code></pre>"},{"location":"api/#nearai.hub.Hub.chat","title":"chat","text":"<pre><code>chat(kwargs)\n</code></pre> <p>Processes a chat request by sending parameters to the NEAR AI Hub and printing the response.</p> Source code in <code>nearai/hub.py</code> <pre><code>def chat(self, kwargs):\n    \"\"\"Processes a chat request by sending parameters to the NEAR AI Hub and printing the response.\"\"\"\n    try:\n        self.parse_hub_chat_params(kwargs)\n\n        if not self.query:\n            return print(\"Error: 'query' is required for the `hub chat` command.\")\n\n        if self._config.nearai_hub is None:\n            self._config.nearai_hub = NearAiHubConfig()\n\n        data = {\n            \"max_tokens\": 256,\n            \"temperature\": 1,\n            \"frequency_penalty\": 0,\n            \"n\": 1,\n            \"messages\": [{\"role\": \"user\", \"content\": str(self.query)}],\n            \"model\": self.model,\n        }\n\n        auth = self._config.auth\n\n        if self._config.nearai_hub.login_with_near:\n            bearer_token = auth.generate_bearer_token()\n            headers = {\"Content-Type\": \"application/json\", \"Authorization\": f\"Bearer {bearer_token}\"}\n\n            data[\"provider\"] = self.provider\n        elif self._config.nearai_hub.api_key:\n            headers = {\n                \"Content-Type\": \"application/json\",\n                \"Authorization\": \"Bearer {}\".format(self._config.nearai_hub.api_key),\n            }\n        else:\n            return print(\"Illegal NEAR AI Hub Config\")\n\n        if self.info:\n            print(f\"Requesting hub using NEAR Account {auth.account_id}\")\n\n        response = requests.post(self.endpoint, headers=headers, data=json.dumps(data))\n\n        completion = response.json()\n\n        print(completion[\"choices\"][0][\"message\"][\"content\"])\n\n    except Exception as e:\n        print(f\"Request failed: {e}\")\n</code></pre>"},{"location":"api/#nearai.hub.Hub.parse_hub_chat_params","title":"parse_hub_chat_params","text":"<pre><code>parse_hub_chat_params(kwargs)\n</code></pre> <p>Parses and sets instance attributes from the given keyword arguments, using default values if needed.</p> Source code in <code>nearai/hub.py</code> <pre><code>def parse_hub_chat_params(self, kwargs):\n    \"\"\"Parses and sets instance attributes from the given keyword arguments, using default values if needed.\"\"\"\n    if self._config.nearai_hub is None:\n        self._config.nearai_hub = NearAiHubConfig()\n\n    self.query = kwargs.get(\"query\")\n    self.endpoint = kwargs.get(\"endpoint\", f\"{self._config.nearai_hub.base_url}/chat/completions\")\n    self.model = kwargs.get(\"model\", self._config.nearai_hub.default_model)\n    self.provider = kwargs.get(\"provider\", self._config.nearai_hub.default_provider)\n    self.info = kwargs.get(\"info\", False)\n</code></pre>"},{"location":"api/#nearai.lib","title":"lib","text":""},{"location":"api/#nearai.lib.parse_location","title":"parse_location","text":"<pre><code>parse_location(entry_location: str) -&gt; EntryLocation\n</code></pre> <p>Create a EntryLocation from a string in the format namespace/name/version.</p> Source code in <code>nearai/lib.py</code> <pre><code>def parse_location(entry_location: str) -&gt; EntryLocation:\n    \"\"\"Create a EntryLocation from a string in the format namespace/name/version.\"\"\"\n    match = entry_location_pattern.match(entry_location)\n\n    if match is None:\n        raise ValueError(f\"Invalid entry format: {entry_location}. Should have the format &lt;namespace&gt;/&lt;name&gt;/&lt;version&gt;\")\n\n    return EntryLocation(\n        namespace=match.group(\"namespace\"),\n        name=match.group(\"name\"),\n        version=match.group(\"version\"),\n    )\n</code></pre>"},{"location":"api/#nearai.log","title":"log","text":""},{"location":"api/#nearai.log.LogCLI","title":"LogCLI","text":"Source code in <code>nearai/log.py</code> <pre><code>class LogCLI:\n    def __init__(self):\n        \"\"\"Log CLI.\"\"\"\n        self.api = LogsApi()\n\n    def push(self, target: str, **kwargs):\n        \"\"\"Push a log to the hub.\"\"\"\n        self.api.add_log_v1_logs_add_log_post(target=target, body=kwargs)\n\n    def track(self, target: str, follow: bool = False, max_wait_time: int = 60):\n        \"\"\"Start tracking logs.\"\"\"\n        if CONFIG.auth is None:\n            print(\"You need to login first.\")\n            return\n        account_id = CONFIG.auth.account_id\n        store = LogStore(account_id=account_id, target=target)\n        limit = 32\n\n        wait_time = 1\n\n        while True:\n            last_id = store.last_id()\n            logs = self.api.get_logs_v1_logs_get_logs_get(target=target, after_id=last_id, limit=limit)\n\n            for log in logs:\n                last_id = max(last_id, log.id)\n\n                if log.info is not None:\n                    store.add(log.info)\n                    print(\"Log: \", log.id)\n                    print(log.info)\n\n            if len(logs) == 0:\n                print(\"No new logs found.\")\n\n            if last_id &gt; store.last_id():\n                store.update_last_id(last_id)\n\n            if not follow:\n                break\n\n            if len(logs) == 0:\n                wait_time = min(wait_time * 2, max_wait_time)\n            else:\n                wait_time = 1\n\n            print(f\"Waiting for {wait_time} seconds...\")\n            time.sleep(wait_time)\n</code></pre>"},{"location":"api/#nearai.log.LogCLI.__init__","title":"__init__","text":"<pre><code>__init__()\n</code></pre> <p>Log CLI.</p> Source code in <code>nearai/log.py</code> <pre><code>def __init__(self):\n    \"\"\"Log CLI.\"\"\"\n    self.api = LogsApi()\n</code></pre>"},{"location":"api/#nearai.log.LogCLI.push","title":"push","text":"<pre><code>push(target: str, **kwargs)\n</code></pre> <p>Push a log to the hub.</p> Source code in <code>nearai/log.py</code> <pre><code>def push(self, target: str, **kwargs):\n    \"\"\"Push a log to the hub.\"\"\"\n    self.api.add_log_v1_logs_add_log_post(target=target, body=kwargs)\n</code></pre>"},{"location":"api/#nearai.log.LogCLI.track","title":"track","text":"<pre><code>track(target: str, follow: bool = False, max_wait_time: int = 60)\n</code></pre> <p>Start tracking logs.</p> Source code in <code>nearai/log.py</code> <pre><code>def track(self, target: str, follow: bool = False, max_wait_time: int = 60):\n    \"\"\"Start tracking logs.\"\"\"\n    if CONFIG.auth is None:\n        print(\"You need to login first.\")\n        return\n    account_id = CONFIG.auth.account_id\n    store = LogStore(account_id=account_id, target=target)\n    limit = 32\n\n    wait_time = 1\n\n    while True:\n        last_id = store.last_id()\n        logs = self.api.get_logs_v1_logs_get_logs_get(target=target, after_id=last_id, limit=limit)\n\n        for log in logs:\n            last_id = max(last_id, log.id)\n\n            if log.info is not None:\n                store.add(log.info)\n                print(\"Log: \", log.id)\n                print(log.info)\n\n        if len(logs) == 0:\n            print(\"No new logs found.\")\n\n        if last_id &gt; store.last_id():\n            store.update_last_id(last_id)\n\n        if not follow:\n            break\n\n        if len(logs) == 0:\n            wait_time = min(wait_time * 2, max_wait_time)\n        else:\n            wait_time = 1\n\n        print(f\"Waiting for {wait_time} seconds...\")\n        time.sleep(wait_time)\n</code></pre>"},{"location":"api/#nearai.log.LogStore","title":"LogStore","text":"Source code in <code>nearai/log.py</code> <pre><code>class LogStore:\n    def __init__(self, account_id: str, target: str):\n        \"\"\"Interface for log local storage.\"\"\"\n        self.folder = DATA_FOLDER / \"logs\" / account_id\n        self.folder.mkdir(parents=True, exist_ok=True)\n        self.file = self.folder / f\"{target}.jsonl\"\n        self.meta = self.folder / f\"{target}-meta.json\"\n\n        if not self.meta.exists():\n            with open(self.meta, \"w\") as f:\n                f.write(LogMetadata(last_id=0).model_dump_json())\n\n        if not self.file.exists():\n            self.file.touch()\n\n    def last_id(self) -&gt; int:\n        \"\"\"Get the id of the latest downloaded logs.\"\"\"\n        with open(self.meta) as f:\n            metadata = LogMetadata.model_validate_json(f.read())\n        return metadata.last_id\n\n    def update_last_id(self, id: int):\n        \"\"\"Update the id of the latest downloaded logs.\"\"\"\n        with open(self.meta, \"w\") as f:\n            f.write(LogMetadata(last_id=id).model_dump_json())\n\n    def add(self, info: Any):\n        \"\"\"Add a log to the local store.\"\"\"\n        with open(self.file, \"a\") as f:\n            info_json = json.dumps(info)\n            f.write(info_json + \"\\n\")\n</code></pre>"},{"location":"api/#nearai.log.LogStore.__init__","title":"__init__","text":"<pre><code>__init__(account_id: str, target: str)\n</code></pre> <p>Interface for log local storage.</p> Source code in <code>nearai/log.py</code> <pre><code>def __init__(self, account_id: str, target: str):\n    \"\"\"Interface for log local storage.\"\"\"\n    self.folder = DATA_FOLDER / \"logs\" / account_id\n    self.folder.mkdir(parents=True, exist_ok=True)\n    self.file = self.folder / f\"{target}.jsonl\"\n    self.meta = self.folder / f\"{target}-meta.json\"\n\n    if not self.meta.exists():\n        with open(self.meta, \"w\") as f:\n            f.write(LogMetadata(last_id=0).model_dump_json())\n\n    if not self.file.exists():\n        self.file.touch()\n</code></pre>"},{"location":"api/#nearai.log.LogStore.add","title":"add","text":"<pre><code>add(info: Any)\n</code></pre> <p>Add a log to the local store.</p> Source code in <code>nearai/log.py</code> <pre><code>def add(self, info: Any):\n    \"\"\"Add a log to the local store.\"\"\"\n    with open(self.file, \"a\") as f:\n        info_json = json.dumps(info)\n        f.write(info_json + \"\\n\")\n</code></pre>"},{"location":"api/#nearai.log.LogStore.last_id","title":"last_id","text":"<pre><code>last_id() -&gt; int\n</code></pre> <p>Get the id of the latest downloaded logs.</p> Source code in <code>nearai/log.py</code> <pre><code>def last_id(self) -&gt; int:\n    \"\"\"Get the id of the latest downloaded logs.\"\"\"\n    with open(self.meta) as f:\n        metadata = LogMetadata.model_validate_json(f.read())\n    return metadata.last_id\n</code></pre>"},{"location":"api/#nearai.log.LogStore.update_last_id","title":"update_last_id","text":"<pre><code>update_last_id(id: int)\n</code></pre> <p>Update the id of the latest downloaded logs.</p> Source code in <code>nearai/log.py</code> <pre><code>def update_last_id(self, id: int):\n    \"\"\"Update the id of the latest downloaded logs.\"\"\"\n    with open(self.meta, \"w\") as f:\n        f.write(LogMetadata(last_id=id).model_dump_json())\n</code></pre>"},{"location":"api/#nearai.login","title":"login","text":""},{"location":"api/#nearai.login.AuthHandler","title":"AuthHandler","text":"<p>               Bases: <code>SimpleHTTPRequestHandler</code></p> Source code in <code>nearai/login.py</code> <pre><code>class AuthHandler(http.server.SimpleHTTPRequestHandler):\n    def log_message(self, format, *args):\n        \"\"\"Webserver logging method.\"\"\"\n        pass  # Override to suppress logging\n\n    def do_GET(self):  # noqa: N802\n        \"\"\"Webserver GET method.\"\"\"\n        global NONCE, PORT\n\n        script_path = Path(__file__).resolve()\n        assets_folder = script_path.parent / \"assets\"\n\n        if self.path.startswith(\"/capture\"):\n            with open(os.path.join(assets_folder, \"auth_capture.html\"), \"r\", encoding=\"utf-8\") as file:\n                content = file.read()\n            self.send_response(200)\n            self.send_header(\"Content-type\", \"text/html\")\n            self.end_headers()\n            self.wfile.write(content.encode(\"utf-8\"))\n\n        if self.path.startswith(\"/auth\"):\n            parsed_url = urlparse.urlparse(self.path)\n            fragment = parsed_url.query\n            params = urlparse.parse_qs(fragment)\n\n            required_params = [\"accountId\", \"signature\", \"publicKey\"]\n\n            if all(param in params for param in required_params):\n                update_auth_config(\n                    params[\"accountId\"][0],\n                    params[\"signature\"][0],\n                    params[\"publicKey\"][0],\n                    callback_url=generate_callback_url(PORT),\n                    nonce=NONCE,\n                )\n            else:\n                print(\"Required parameters not found\")\n\n            with open(os.path.join(assets_folder, \"auth_complete.html\"), \"r\", encoding=\"utf-8\") as file:\n                content = file.read()\n            self.send_response(200)\n            self.send_header(\"Content-type\", \"text/html\")\n            self.end_headers()\n            self.wfile.write(content.encode(\"utf-8\"))\n\n            # Give the server some time to read the response before shutting it down\n            def shutdown_server():\n                global httpd\n                time.sleep(2)  # Wait 2 seconds before shutting down\n                if httpd:\n                    httpd.shutdown()\n\n            threading.Thread(target=shutdown_server).start()\n</code></pre>"},{"location":"api/#nearai.login.AuthHandler.do_GET","title":"do_GET","text":"<pre><code>do_GET()\n</code></pre> <p>Webserver GET method.</p> Source code in <code>nearai/login.py</code> <pre><code>def do_GET(self):  # noqa: N802\n    \"\"\"Webserver GET method.\"\"\"\n    global NONCE, PORT\n\n    script_path = Path(__file__).resolve()\n    assets_folder = script_path.parent / \"assets\"\n\n    if self.path.startswith(\"/capture\"):\n        with open(os.path.join(assets_folder, \"auth_capture.html\"), \"r\", encoding=\"utf-8\") as file:\n            content = file.read()\n        self.send_response(200)\n        self.send_header(\"Content-type\", \"text/html\")\n        self.end_headers()\n        self.wfile.write(content.encode(\"utf-8\"))\n\n    if self.path.startswith(\"/auth\"):\n        parsed_url = urlparse.urlparse(self.path)\n        fragment = parsed_url.query\n        params = urlparse.parse_qs(fragment)\n\n        required_params = [\"accountId\", \"signature\", \"publicKey\"]\n\n        if all(param in params for param in required_params):\n            update_auth_config(\n                params[\"accountId\"][0],\n                params[\"signature\"][0],\n                params[\"publicKey\"][0],\n                callback_url=generate_callback_url(PORT),\n                nonce=NONCE,\n            )\n        else:\n            print(\"Required parameters not found\")\n\n        with open(os.path.join(assets_folder, \"auth_complete.html\"), \"r\", encoding=\"utf-8\") as file:\n            content = file.read()\n        self.send_response(200)\n        self.send_header(\"Content-type\", \"text/html\")\n        self.end_headers()\n        self.wfile.write(content.encode(\"utf-8\"))\n\n        # Give the server some time to read the response before shutting it down\n        def shutdown_server():\n            global httpd\n            time.sleep(2)  # Wait 2 seconds before shutting down\n            if httpd:\n                httpd.shutdown()\n\n        threading.Thread(target=shutdown_server).start()\n</code></pre>"},{"location":"api/#nearai.login.AuthHandler.log_message","title":"log_message","text":"<pre><code>log_message(format, *args)\n</code></pre> <p>Webserver logging method.</p> Source code in <code>nearai/login.py</code> <pre><code>def log_message(self, format, *args):\n    \"\"\"Webserver logging method.\"\"\"\n    pass  # Override to suppress logging\n</code></pre>"},{"location":"api/#nearai.login.find_open_port","title":"find_open_port","text":"<pre><code>find_open_port() -&gt; int\n</code></pre> <p>Finds and returns an open port number by binding to a free port on the local machine.</p> Source code in <code>nearai/login.py</code> <pre><code>def find_open_port() -&gt; int:\n    \"\"\"Finds and returns an open port number by binding to a free port on the local machine.\"\"\"\n    with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:\n        s.bind((\"\", 0))\n        return s.getsockname()[1]\n</code></pre>"},{"location":"api/#nearai.login.generate_and_save_signature","title":"generate_and_save_signature","text":"<pre><code>generate_and_save_signature(account_id, private_key)\n</code></pre> <p>Generates a signature for the given account ID and private key, then updates the auth configuration.</p> Source code in <code>nearai/login.py</code> <pre><code>def generate_and_save_signature(account_id, private_key):\n    \"\"\"Generates a signature for the given account ID and private key, then updates the auth configuration.\"\"\"\n    nonce = generate_nonce()\n    payload = near.Payload(MESSAGE, nonce, RECIPIENT, \"\")\n\n    signature, public_key = near.create_signature(private_key, payload)\n\n    if update_auth_config(account_id, signature, public_key, \"\", nonce):\n        print_login_status()\n</code></pre>"},{"location":"api/#nearai.login.generate_callback_url","title":"generate_callback_url","text":"<pre><code>generate_callback_url(port)\n</code></pre> <p>Generates a callback URL using the specified port number.</p> Source code in <code>nearai/login.py</code> <pre><code>def generate_callback_url(port):\n    \"\"\"Generates a callback URL using the specified port number.\"\"\"\n    return f\"http://localhost:{port}/capture\"\n</code></pre>"},{"location":"api/#nearai.login.generate_nonce","title":"generate_nonce","text":"<pre><code>generate_nonce()\n</code></pre> <p>Generates a nonce based on the current time in milliseconds.</p> Source code in <code>nearai/login.py</code> <pre><code>def generate_nonce():\n    \"\"\"Generates a nonce based on the current time in milliseconds.\"\"\"\n    return str(int(time.time() * 1000))\n</code></pre>"},{"location":"api/#nearai.login.login_with_file_credentials","title":"login_with_file_credentials","text":"<pre><code>login_with_file_credentials(account_id)\n</code></pre> <p>Logs in using credentials from a file for the specified account ID, generating and saving a signature.</p> Source code in <code>nearai/login.py</code> <pre><code>def login_with_file_credentials(account_id):\n    \"\"\"Logs in using credentials from a file for the specified account ID, generating and saving a signature.\"\"\"\n    file_path = os.path.expanduser(os.path.join(\"~/.near-credentials/\", \"mainnet\", f\"{account_id}.json\"))\n\n    if os.path.exists(file_path):\n        with open(file_path, \"r\") as file:\n            content = file.read()\n            account_data = json.loads(content)\n            private_key = account_data.get(\"private_key\", None)\n            if not private_key:\n                return print(f\"Private key is missing for {account_id} on mainnet\")\n            generate_and_save_signature(account_id, account_data[\"private_key\"])\n\n    else:\n        return print(f\"Account data is missing for {account_id}\")\n</code></pre>"},{"location":"api/#nearai.login.login_with_near_auth","title":"login_with_near_auth","text":"<pre><code>login_with_near_auth(remote, auth_url)\n</code></pre> <p>Initiates the login process using NEAR authentication, either starting a local server to handle the callback or providing a URL for remote authentication.</p> Source code in <code>nearai/login.py</code> <pre><code>def login_with_near_auth(remote, auth_url):\n    \"\"\"Initiates the login process using NEAR authentication, either starting a local server to handle the callback or providing a URL for remote authentication.\"\"\"  # noqa: E501\n    global NONCE, PORT\n    NONCE = generate_nonce()\n\n    params = {\n        \"message\": MESSAGE,\n        \"nonce\": NONCE,\n        \"recipient\": RECIPIENT,\n    }\n\n    if not remote:\n        PORT = find_open_port()\n\n        global httpd\n        with socketserver.TCPServer((\"\", PORT), AuthHandler) as httpd:\n            params[\"callbackUrl\"] = f\"http://localhost:{PORT}/capture\"\n\n            encoded_params = urlparse.urlencode(params)\n\n            print_url_message(f\"{auth_url}?{encoded_params}\")\n\n            httpd.serve_forever()\n\n    else:\n        encoded_params = urlparse.urlencode(params)\n\n        print_url_message(f\"{auth_url}?{encoded_params}\")\n        print(\"After visiting the URL, follow the instructions to save your auth signature\")\n</code></pre>"},{"location":"api/#nearai.login.print_login_status","title":"print_login_status","text":"<pre><code>print_login_status()\n</code></pre> <p>Prints the current authentication status if available in the config file.</p> Source code in <code>nearai/login.py</code> <pre><code>def print_login_status():\n    \"\"\"Prints the current authentication status if available in the config file.\"\"\"\n    config = load_config_file()\n    if config.get(\"auth\") and config[\"auth\"].get(\"account_id\"):\n        print(f\"Auth data for: {config['auth']['account_id']}\")\n        print(f\"signature: {config['auth']['signature']}\")\n        print(f\"public_key: {config['auth']['public_key']}\")\n        print(f\"nonce: {config['auth']['nonce']}\")\n        print(f\"message: {config['auth']['message']}\")\n        print(f\"recipient: {config['auth']['recipient']}\")\n    else:\n        print(\"Near auth details not found\")\n</code></pre>"},{"location":"api/#nearai.login.print_url_message","title":"print_url_message","text":"<pre><code>print_url_message(url)\n</code></pre> <p>Prints a message instructing the user to visit the given URL to complete the login process.</p> Source code in <code>nearai/login.py</code> <pre><code>def print_url_message(url):\n    \"\"\"Prints a message instructing the user to visit the given URL to complete the login process.\"\"\"\n    print(f\"Please visit the following URL to complete the login process: {url}\")\n</code></pre>"},{"location":"api/#nearai.login.update_auth_config","title":"update_auth_config","text":"<pre><code>update_auth_config(account_id, signature, public_key, callback_url, nonce)\n</code></pre> <p>Update authentication configuration if the provided signature is valid.</p> Source code in <code>nearai/login.py</code> <pre><code>def update_auth_config(account_id, signature, public_key, callback_url, nonce):\n    \"\"\"Update authentication configuration if the provided signature is valid.\"\"\"\n    if near.verify_signed_message(\n        account_id,\n        public_key,\n        signature,\n        MESSAGE,\n        nonce,\n        RECIPIENT,\n        callback_url,\n    ):\n        config = load_config_file()\n\n        auth = AuthData.model_validate(\n            {\n                \"account_id\": account_id,\n                \"signature\": signature,\n                \"public_key\": public_key,\n                \"callback_url\": callback_url,\n                \"nonce\": nonce,\n                \"recipient\": RECIPIENT,\n                \"message\": MESSAGE,\n            }\n        )\n\n        config[\"auth\"] = auth.model_dump()\n        save_config_file(config)\n\n        print(f\"Auth data has been successfully saved! You are now logged in with account ID: {account_id}\")\n        return True\n    else:\n        print(\"Signature verification failed. Abort\")\n        return False\n</code></pre>"},{"location":"api/#nearai.model","title":"model","text":""},{"location":"api/#nearai.model.get_model","title":"get_model","text":"<pre><code>get_model(name: str) -&gt; Path\n</code></pre> <p>Download the model from the registry and download it locally if it hasn't been downloaded yet.</p> <p>:param name: The name of the entry to download the model. The format should be namespace/name/version. :return: The path to the downloaded model</p> Source code in <code>nearai/model.py</code> <pre><code>def get_model(name: str) -&gt; Path:\n    \"\"\"Download the model from the registry and download it locally if it hasn't been downloaded yet.\n\n    :param name: The name of the entry to download the model. The format should be namespace/name/version.\n    :return: The path to the downloaded model\n    \"\"\"\n    return registry.download(name)\n</code></pre>"},{"location":"api/#nearai.registry","title":"registry","text":""},{"location":"api/#nearai.registry.Registry","title":"Registry","text":"Source code in <code>nearai/registry.py</code> <pre><code>class Registry:\n    def __init__(self):\n        \"\"\"Create Registry object to interact with the registry programmatically.\"\"\"\n        self.download_folder = DATA_FOLDER / \"registry\"\n        self.api = RegistryApi()\n\n        if not self.download_folder.exists():\n            self.download_folder.mkdir(parents=True, exist_ok=True)\n\n    def update(self, entry_location: EntryLocation, metadata: EntryMetadataInput) -&gt; Dict[str, Any]:\n        \"\"\"Update metadata of a entry in the registry.\"\"\"\n        result = self.api.upload_metadata_v1_registry_upload_metadata_post(\n            BodyUploadMetadataV1RegistryUploadMetadataPost(metadata=metadata, entry_location=entry_location)\n        )\n        return result\n\n    def info(self, entry_location: EntryLocation) -&gt; Optional[EntryMetadata]:\n        \"\"\"Get metadata of a entry in the registry.\"\"\"\n        try:\n            return self.api.download_metadata_v1_registry_download_metadata_post(\n                BodyDownloadMetadataV1RegistryDownloadMetadataPost.from_dict(dict(entry_location=entry_location))\n            )\n        except NotFoundException:\n            return None\n\n    def upload_file(\n        self, entry_location: EntryLocation, local_path: Path, path: Path, encryption_key: Optional[str] = None\n    ) -&gt; bool:\n        \"\"\"Upload a file to the registry.\"\"\"\n        with open(local_path, \"rb\") as file:\n            data = file.read()\n\n            # Encrypt data if encryption is enabled\n            if encryption_key:\n                data = FileEncryption.encrypt_data(data, encryption_key)\n\n            try:\n                self.api.upload_file_v1_registry_upload_file_post(\n                    path=str(path),\n                    file=data,\n                    namespace=entry_location.namespace,\n                    name=entry_location.name,\n                    version=entry_location.version,\n                )\n                return True\n            except BadRequestException as e:\n                if isinstance(e.body, str) and \"already exists\" in e.body:\n                    return False\n\n                raise e\n\n    def download_file(\n        self, entry_location: EntryLocation, path: Path, local_path: Path, encryption_key: Optional[str] = None\n    ):\n        \"\"\"Download a file from the registry.\"\"\"\n        result = self.api.download_file_v1_registry_download_file_post_without_preload_content(\n            BodyDownloadFileV1RegistryDownloadFilePost.from_dict(\n                dict(\n                    entry_location=entry_location,\n                    path=str(path),\n                )\n            )\n        )\n\n        local_path.parent.mkdir(parents=True, exist_ok=True)\n\n        # Read all data first\n        data = result.read()\n\n        # Decrypt if necessary\n        if encryption_key:\n            try:\n                data = FileEncryption.decrypt_data(data, encryption_key)\n            except Exception as e:\n                print(f\"Error: Failed to decrypt file {path}: {str(e)}\")\n                # Continue with encrypted data - user might want to decrypt manually\n\n        with open(local_path, \"wb\") as f:\n            f.write(data)\n\n    def download(\n        self,\n        entry_location: Union[str, EntryLocation],\n        force: bool = False,\n        show_progress: bool = False,\n        verbose: bool = True,\n        encryption_key: Optional[str] = None,\n    ) -&gt; Path:\n        \"\"\"Download entry from the registry locally.\"\"\"\n        if isinstance(entry_location, str):\n            entry_location = parse_location(entry_location)\n\n        download_path = get_registry_folder() / entry_location.namespace / entry_location.name / entry_location.version\n\n        if download_path.exists():\n            if not force:\n                if verbose:\n                    print(\n                        f\"Entry {entry_location} already exists at {download_path}. Use --force to overwrite the entry.\"\n                    )\n                return download_path\n            if not encryption_key:\n                metadata_on_disk = get_metadata(download_path, local=True)\n                if metadata_on_disk:\n                    encryption_key_from_metadata_on_disk = metadata_on_disk.get(\"details\", {}).get(\n                        \"encryption_key\", None\n                    )\n                    if encryption_key_from_metadata_on_disk != OBFUSCATED_SECRET:\n                        encryption_key = encryption_key_from_metadata_on_disk\n\n        files = registry.list_files(entry_location)\n\n        metadata = registry.info(entry_location)\n\n        if metadata is None:\n            raise ValueError(f\"Entry {entry_location} not found.\")\n\n        encryption_key_from_registry = metadata.details.get(\"encryption_key\", None)\n        if encryption_key_from_registry is None and encryption_key:\n            print(\"This registry entry is not encrypted, but encryption_key has been provided. Aborting.\")\n            exit(1)\n        if not encryption_key:\n            encryption_key = encryption_key_from_registry\n        if encryption_key == OBFUSCATED_SECRET:\n            print(\n                \"This registry entry is encrypted. You must provide encryption_key (nearai registry download --encryption_key &lt;secret&gt;). Aborting.\"  # noqa: E501\n            )\n            exit(1)\n        if encryption_key:\n            metadata.details[\"encryption_key\"] = encryption_key\n\n        download_path.mkdir(parents=True, exist_ok=True)\n\n        metadata_path = download_path / \"metadata.json\"\n        with open(metadata_path, \"w\") as f:\n            f.write(metadata.model_dump_json(indent=2))\n\n        if encryption_key:\n            print(f\"Downloading encrypted registry entry and decrypting with encryption_key={encryption_key}\")\n\n        for file in (pbar := tqdm(files, disable=not show_progress)):\n            pbar.set_description(file)\n            registry.download_file(entry_location, file, download_path / file, encryption_key)\n\n        return download_path\n\n    def upload(\n        self,\n        local_path: Path,\n        show_progress: bool = False,\n        encrypt: bool = False,\n    ) -&gt; EntryLocation:\n        \"\"\"Upload entry to the registry.\n\n        `local_path` should have metadata.json present.\n        Files matching patterns in .gitignore (if present) will be excluded from upload.\n        If encryption_key is present in metadata.json, uploaded files will be encrypted.\n        If encrypt == True, will generate an encryption_key, if encryption_key is not present.\n        \"\"\"\n        path = Path(local_path).absolute()\n\n        if CONFIG.auth is None:\n            print(\"Please login with `nearai login`\")\n            exit(1)\n\n        metadata_path = path / \"metadata.json\"\n\n        check_metadata_present(metadata_path)\n\n        with open(metadata_path) as f:\n            metadata: Dict[str, Any] = json.load(f)\n\n        # Handle encryption key generation if --encrypt flag is used\n        if encrypt:\n            # Initialize details if not present\n            if \"details\" not in metadata:\n                metadata[\"details\"] = {}\n\n            if \"encryption_key\" in metadata[\"details\"]:\n                encryption_key = metadata[\"details\"][\"encryption_key\"]\n            else:\n                # Generate encryption key if not present\n                encryption_key = FileEncryption.generate_encryption_key()\n                metadata[\"details\"][\"encryption_key\"] = encryption_key\n\n                # Update metadata.json file with the new encryption key\n                with open(metadata_path, \"w\") as f:\n                    json.dump(metadata, f, indent=2)\n\n        namespace = get_namespace(local_path)\n        name = metadata.pop(\"name\")\n        assert \" \" not in name\n\n        entry_location = EntryLocation.model_validate(\n            dict(\n                namespace=namespace,\n                name=name,\n                version=metadata.pop(\"version\"),\n            )\n        )\n\n        entry_metadata = EntryMetadataInput.model_validate(metadata)\n        source = entry_metadata.details.get(\"_source\", None)\n\n        if source is not None:\n            print(f\"Only default source is allowed, found: {source}. Remove details._source from metadata.\")\n            exit(1)\n\n        encryption_key = entry_metadata.details.get(\"encryption_key\", None)\n        if encryption_key == OBFUSCATED_SECRET:\n            print(\n                f\"metadata/details/encryption_key is obfuscated: {encryption_key}. Can't upload without knowing encryption key.\"  # noqa: E501\n            )\n            exit(1)\n\n        if self.info(entry_location) is None:\n            # New entry location. Check for similar names in registry.\n            entries = self.list_all_visible()\n            canonical_namespace = get_canonical_name(namespace)\n            canonical_name = get_canonical_name(name)\n\n            for entry in entries:\n                if entry.name == name and entry.namespace == namespace:\n                    break\n                if (\n                    get_canonical_name(entry.name) == canonical_name\n                    and get_canonical_name(entry.namespace) == canonical_namespace\n                ):\n                    print(f\"A registry item with a similar name already exists: {entry.namespace}/{entry.name}\")\n                    exit(1)\n\n        entry_files = get_local_agent_files(path)\n        files_to_upload = []\n        total_size = 0\n\n        for file in entry_files:\n            relative = file.relative_to(path)\n\n            # Don't upload metadata file.\n            if file == metadata_path:\n                continue\n\n            size = file.stat().st_size\n            total_size += size\n\n            files_to_upload.append((file, relative, size))\n\n        print(\"\")\n        print(\"UPLOAD CONFIRMATION\")\n        print(\"\")\n        print(f\"\ud83d\udccd Entry Location: {entry_location.namespace}/{entry_location.name}/{entry_location.version}\")\n        print(f\"\ud83d\udcc1 Local Path: {path}\")\n        if encryption_key:\n            encryption_status = \"\ud83d\udd10 Private (encrypted)\"\n        else:\n            encryption_status = \"\ud83c\udf10 Open source (public)\"\n        print(f\"\ud83d\udd12 Visibility: {encryption_status}\")\n        print(\"\")\n        response = input(\"Do you want to proceed with the upload? (y/N): \").strip().lower()\n        if response not in [\"y\", \"yes\", \"Y\"]:\n            exit(0)\n\n        registry.update(entry_location, entry_metadata)\n        pbar = tqdm(total=total_size, unit=\"B\", unit_scale=True, disable=not show_progress)\n        for file, relative, size in files_to_upload:\n            registry.upload_file(entry_location, file, relative, encryption_key=encryption_key)\n            pbar.update(size)\n\n        if encrypt:\n            console = Console()\n            console.print(\n                Panel(\n                    Text.assemble(\n                        (\"\ud83d\udd10 Encryption enabled\\n\\n\", \"bold green\"),\n                        (f\"Encryption key {encryption_key} generated and stored in metadata.json\\n\", \"dim\"),\n                    ),\n                    title=\"Encryption\",\n                    border_style=\"green\",\n                    padding=(1, 2),\n                )\n            )\n\n        return entry_location\n\n    def list_files(self, entry_location: EntryLocation) -&gt; List[str]:\n        \"\"\"List files in from an entry in the registry.\n\n        Return the relative paths to all files with respect to the root of the entry.\n        \"\"\"\n        result = self.api.list_files_v1_registry_list_files_post(\n            BodyListFilesV1RegistryListFilesPost.from_dict(dict(entry_location=entry_location))\n        )\n        return [file.filename for file in result]\n\n    def list(\n        self,\n        namespace: str,\n        category: str,\n        tags: str,\n        total: int,\n        offset: int,\n        show_all: bool,\n        show_latest_version: bool,\n        starred_by: str = \"\",\n    ) -&gt; List[EntryInformation]:\n        \"\"\"List and filter entries in the registry.\"\"\"\n        return self.api.list_entries_v1_registry_list_entries_post(\n            namespace=namespace,\n            category=category,\n            tags=tags,\n            total=total,\n            offset=offset,\n            show_hidden=show_all,\n            show_latest_version=show_latest_version,\n            starred_by=starred_by,\n        )\n\n    def list_all_visible(self, category: str = \"\") -&gt; List[EntryInformation]:\n        \"\"\"List all visible entries.\"\"\"\n        total = 1000000\n        entries = self.list(\n            namespace=\"\",\n            category=category,\n            tags=\"\",\n            total=total,\n            offset=0,\n            show_all=False,\n            show_latest_version=True,\n        )\n        assert len(entries) &lt; total\n        return entries\n\n    def dict_models(self) -&gt; Dict[NamespacedName, NamespacedName]:\n        \"\"\"Returns a mapping canonical-&gt;name.\"\"\"\n        entries = self.list_all_visible(category=\"model\")\n        result: Dict[NamespacedName, NamespacedName] = {}\n        for entry in entries:\n            namespaced_name = NamespacedName(name=entry.name, namespace=entry.namespace)\n            canonical_namespaced_name = namespaced_name.canonical()\n            if canonical_namespaced_name in result:\n                raise ValueError(\n                    f\"Duplicate registry entry for model {namespaced_name}, canonical {canonical_namespaced_name}\"\n                )\n            result[canonical_namespaced_name] = namespaced_name\n        return result\n</code></pre>"},{"location":"api/#nearai.registry.Registry.__init__","title":"__init__","text":"<pre><code>__init__()\n</code></pre> <p>Create Registry object to interact with the registry programmatically.</p> Source code in <code>nearai/registry.py</code> <pre><code>def __init__(self):\n    \"\"\"Create Registry object to interact with the registry programmatically.\"\"\"\n    self.download_folder = DATA_FOLDER / \"registry\"\n    self.api = RegistryApi()\n\n    if not self.download_folder.exists():\n        self.download_folder.mkdir(parents=True, exist_ok=True)\n</code></pre>"},{"location":"api/#nearai.registry.Registry.dict_models","title":"dict_models","text":"<pre><code>dict_models() -&gt; Dict[NamespacedName, NamespacedName]\n</code></pre> <p>Returns a mapping canonical-&gt;name.</p> Source code in <code>nearai/registry.py</code> <pre><code>def dict_models(self) -&gt; Dict[NamespacedName, NamespacedName]:\n    \"\"\"Returns a mapping canonical-&gt;name.\"\"\"\n    entries = self.list_all_visible(category=\"model\")\n    result: Dict[NamespacedName, NamespacedName] = {}\n    for entry in entries:\n        namespaced_name = NamespacedName(name=entry.name, namespace=entry.namespace)\n        canonical_namespaced_name = namespaced_name.canonical()\n        if canonical_namespaced_name in result:\n            raise ValueError(\n                f\"Duplicate registry entry for model {namespaced_name}, canonical {canonical_namespaced_name}\"\n            )\n        result[canonical_namespaced_name] = namespaced_name\n    return result\n</code></pre>"},{"location":"api/#nearai.registry.Registry.download","title":"download","text":"<pre><code>download(entry_location: Union[str, EntryLocation], force: bool = False, show_progress: bool = False, verbose: bool = True, encryption_key: Optional[str] = None) -&gt; Path\n</code></pre> <p>Download entry from the registry locally.</p> Source code in <code>nearai/registry.py</code> <pre><code>def download(\n    self,\n    entry_location: Union[str, EntryLocation],\n    force: bool = False,\n    show_progress: bool = False,\n    verbose: bool = True,\n    encryption_key: Optional[str] = None,\n) -&gt; Path:\n    \"\"\"Download entry from the registry locally.\"\"\"\n    if isinstance(entry_location, str):\n        entry_location = parse_location(entry_location)\n\n    download_path = get_registry_folder() / entry_location.namespace / entry_location.name / entry_location.version\n\n    if download_path.exists():\n        if not force:\n            if verbose:\n                print(\n                    f\"Entry {entry_location} already exists at {download_path}. Use --force to overwrite the entry.\"\n                )\n            return download_path\n        if not encryption_key:\n            metadata_on_disk = get_metadata(download_path, local=True)\n            if metadata_on_disk:\n                encryption_key_from_metadata_on_disk = metadata_on_disk.get(\"details\", {}).get(\n                    \"encryption_key\", None\n                )\n                if encryption_key_from_metadata_on_disk != OBFUSCATED_SECRET:\n                    encryption_key = encryption_key_from_metadata_on_disk\n\n    files = registry.list_files(entry_location)\n\n    metadata = registry.info(entry_location)\n\n    if metadata is None:\n        raise ValueError(f\"Entry {entry_location} not found.\")\n\n    encryption_key_from_registry = metadata.details.get(\"encryption_key\", None)\n    if encryption_key_from_registry is None and encryption_key:\n        print(\"This registry entry is not encrypted, but encryption_key has been provided. Aborting.\")\n        exit(1)\n    if not encryption_key:\n        encryption_key = encryption_key_from_registry\n    if encryption_key == OBFUSCATED_SECRET:\n        print(\n            \"This registry entry is encrypted. You must provide encryption_key (nearai registry download --encryption_key &lt;secret&gt;). Aborting.\"  # noqa: E501\n        )\n        exit(1)\n    if encryption_key:\n        metadata.details[\"encryption_key\"] = encryption_key\n\n    download_path.mkdir(parents=True, exist_ok=True)\n\n    metadata_path = download_path / \"metadata.json\"\n    with open(metadata_path, \"w\") as f:\n        f.write(metadata.model_dump_json(indent=2))\n\n    if encryption_key:\n        print(f\"Downloading encrypted registry entry and decrypting with encryption_key={encryption_key}\")\n\n    for file in (pbar := tqdm(files, disable=not show_progress)):\n        pbar.set_description(file)\n        registry.download_file(entry_location, file, download_path / file, encryption_key)\n\n    return download_path\n</code></pre>"},{"location":"api/#nearai.registry.Registry.download_file","title":"download_file","text":"<pre><code>download_file(entry_location: EntryLocation, path: Path, local_path: Path, encryption_key: Optional[str] = None)\n</code></pre> <p>Download a file from the registry.</p> Source code in <code>nearai/registry.py</code> <pre><code>def download_file(\n    self, entry_location: EntryLocation, path: Path, local_path: Path, encryption_key: Optional[str] = None\n):\n    \"\"\"Download a file from the registry.\"\"\"\n    result = self.api.download_file_v1_registry_download_file_post_without_preload_content(\n        BodyDownloadFileV1RegistryDownloadFilePost.from_dict(\n            dict(\n                entry_location=entry_location,\n                path=str(path),\n            )\n        )\n    )\n\n    local_path.parent.mkdir(parents=True, exist_ok=True)\n\n    # Read all data first\n    data = result.read()\n\n    # Decrypt if necessary\n    if encryption_key:\n        try:\n            data = FileEncryption.decrypt_data(data, encryption_key)\n        except Exception as e:\n            print(f\"Error: Failed to decrypt file {path}: {str(e)}\")\n            # Continue with encrypted data - user might want to decrypt manually\n\n    with open(local_path, \"wb\") as f:\n        f.write(data)\n</code></pre>"},{"location":"api/#nearai.registry.Registry.info","title":"info","text":"<pre><code>info(entry_location: EntryLocation) -&gt; Optional[EntryMetadata]\n</code></pre> <p>Get metadata of a entry in the registry.</p> Source code in <code>nearai/registry.py</code> <pre><code>def info(self, entry_location: EntryLocation) -&gt; Optional[EntryMetadata]:\n    \"\"\"Get metadata of a entry in the registry.\"\"\"\n    try:\n        return self.api.download_metadata_v1_registry_download_metadata_post(\n            BodyDownloadMetadataV1RegistryDownloadMetadataPost.from_dict(dict(entry_location=entry_location))\n        )\n    except NotFoundException:\n        return None\n</code></pre>"},{"location":"api/#nearai.registry.Registry.list","title":"list","text":"<pre><code>list(namespace: str, category: str, tags: str, total: int, offset: int, show_all: bool, show_latest_version: bool, starred_by: str = '') -&gt; List[EntryInformation]\n</code></pre> <p>List and filter entries in the registry.</p> Source code in <code>nearai/registry.py</code> <pre><code>def list(\n    self,\n    namespace: str,\n    category: str,\n    tags: str,\n    total: int,\n    offset: int,\n    show_all: bool,\n    show_latest_version: bool,\n    starred_by: str = \"\",\n) -&gt; List[EntryInformation]:\n    \"\"\"List and filter entries in the registry.\"\"\"\n    return self.api.list_entries_v1_registry_list_entries_post(\n        namespace=namespace,\n        category=category,\n        tags=tags,\n        total=total,\n        offset=offset,\n        show_hidden=show_all,\n        show_latest_version=show_latest_version,\n        starred_by=starred_by,\n    )\n</code></pre>"},{"location":"api/#nearai.registry.Registry.list_all_visible","title":"list_all_visible","text":"<pre><code>list_all_visible(category: str = '') -&gt; List[EntryInformation]\n</code></pre> <p>List all visible entries.</p> Source code in <code>nearai/registry.py</code> <pre><code>def list_all_visible(self, category: str = \"\") -&gt; List[EntryInformation]:\n    \"\"\"List all visible entries.\"\"\"\n    total = 1000000\n    entries = self.list(\n        namespace=\"\",\n        category=category,\n        tags=\"\",\n        total=total,\n        offset=0,\n        show_all=False,\n        show_latest_version=True,\n    )\n    assert len(entries) &lt; total\n    return entries\n</code></pre>"},{"location":"api/#nearai.registry.Registry.list_files","title":"list_files","text":"<pre><code>list_files(entry_location: EntryLocation) -&gt; List[str]\n</code></pre> <p>List files in from an entry in the registry.</p> <p>Return the relative paths to all files with respect to the root of the entry.</p> Source code in <code>nearai/registry.py</code> <pre><code>def list_files(self, entry_location: EntryLocation) -&gt; List[str]:\n    \"\"\"List files in from an entry in the registry.\n\n    Return the relative paths to all files with respect to the root of the entry.\n    \"\"\"\n    result = self.api.list_files_v1_registry_list_files_post(\n        BodyListFilesV1RegistryListFilesPost.from_dict(dict(entry_location=entry_location))\n    )\n    return [file.filename for file in result]\n</code></pre>"},{"location":"api/#nearai.registry.Registry.update","title":"update","text":"<pre><code>update(entry_location: EntryLocation, metadata: EntryMetadataInput) -&gt; Dict[str, Any]\n</code></pre> <p>Update metadata of a entry in the registry.</p> Source code in <code>nearai/registry.py</code> <pre><code>def update(self, entry_location: EntryLocation, metadata: EntryMetadataInput) -&gt; Dict[str, Any]:\n    \"\"\"Update metadata of a entry in the registry.\"\"\"\n    result = self.api.upload_metadata_v1_registry_upload_metadata_post(\n        BodyUploadMetadataV1RegistryUploadMetadataPost(metadata=metadata, entry_location=entry_location)\n    )\n    return result\n</code></pre>"},{"location":"api/#nearai.registry.Registry.upload","title":"upload","text":"<pre><code>upload(local_path: Path, show_progress: bool = False, encrypt: bool = False) -&gt; EntryLocation\n</code></pre> <p>Upload entry to the registry.</p> <p><code>local_path</code> should have metadata.json present. Files matching patterns in .gitignore (if present) will be excluded from upload. If encryption_key is present in metadata.json, uploaded files will be encrypted. If encrypt == True, will generate an encryption_key, if encryption_key is not present.</p> Source code in <code>nearai/registry.py</code> <pre><code>def upload(\n    self,\n    local_path: Path,\n    show_progress: bool = False,\n    encrypt: bool = False,\n) -&gt; EntryLocation:\n    \"\"\"Upload entry to the registry.\n\n    `local_path` should have metadata.json present.\n    Files matching patterns in .gitignore (if present) will be excluded from upload.\n    If encryption_key is present in metadata.json, uploaded files will be encrypted.\n    If encrypt == True, will generate an encryption_key, if encryption_key is not present.\n    \"\"\"\n    path = Path(local_path).absolute()\n\n    if CONFIG.auth is None:\n        print(\"Please login with `nearai login`\")\n        exit(1)\n\n    metadata_path = path / \"metadata.json\"\n\n    check_metadata_present(metadata_path)\n\n    with open(metadata_path) as f:\n        metadata: Dict[str, Any] = json.load(f)\n\n    # Handle encryption key generation if --encrypt flag is used\n    if encrypt:\n        # Initialize details if not present\n        if \"details\" not in metadata:\n            metadata[\"details\"] = {}\n\n        if \"encryption_key\" in metadata[\"details\"]:\n            encryption_key = metadata[\"details\"][\"encryption_key\"]\n        else:\n            # Generate encryption key if not present\n            encryption_key = FileEncryption.generate_encryption_key()\n            metadata[\"details\"][\"encryption_key\"] = encryption_key\n\n            # Update metadata.json file with the new encryption key\n            with open(metadata_path, \"w\") as f:\n                json.dump(metadata, f, indent=2)\n\n    namespace = get_namespace(local_path)\n    name = metadata.pop(\"name\")\n    assert \" \" not in name\n\n    entry_location = EntryLocation.model_validate(\n        dict(\n            namespace=namespace,\n            name=name,\n            version=metadata.pop(\"version\"),\n        )\n    )\n\n    entry_metadata = EntryMetadataInput.model_validate(metadata)\n    source = entry_metadata.details.get(\"_source\", None)\n\n    if source is not None:\n        print(f\"Only default source is allowed, found: {source}. Remove details._source from metadata.\")\n        exit(1)\n\n    encryption_key = entry_metadata.details.get(\"encryption_key\", None)\n    if encryption_key == OBFUSCATED_SECRET:\n        print(\n            f\"metadata/details/encryption_key is obfuscated: {encryption_key}. Can't upload without knowing encryption key.\"  # noqa: E501\n        )\n        exit(1)\n\n    if self.info(entry_location) is None:\n        # New entry location. Check for similar names in registry.\n        entries = self.list_all_visible()\n        canonical_namespace = get_canonical_name(namespace)\n        canonical_name = get_canonical_name(name)\n\n        for entry in entries:\n            if entry.name == name and entry.namespace == namespace:\n                break\n            if (\n                get_canonical_name(entry.name) == canonical_name\n                and get_canonical_name(entry.namespace) == canonical_namespace\n            ):\n                print(f\"A registry item with a similar name already exists: {entry.namespace}/{entry.name}\")\n                exit(1)\n\n    entry_files = get_local_agent_files(path)\n    files_to_upload = []\n    total_size = 0\n\n    for file in entry_files:\n        relative = file.relative_to(path)\n\n        # Don't upload metadata file.\n        if file == metadata_path:\n            continue\n\n        size = file.stat().st_size\n        total_size += size\n\n        files_to_upload.append((file, relative, size))\n\n    print(\"\")\n    print(\"UPLOAD CONFIRMATION\")\n    print(\"\")\n    print(f\"\ud83d\udccd Entry Location: {entry_location.namespace}/{entry_location.name}/{entry_location.version}\")\n    print(f\"\ud83d\udcc1 Local Path: {path}\")\n    if encryption_key:\n        encryption_status = \"\ud83d\udd10 Private (encrypted)\"\n    else:\n        encryption_status = \"\ud83c\udf10 Open source (public)\"\n    print(f\"\ud83d\udd12 Visibility: {encryption_status}\")\n    print(\"\")\n    response = input(\"Do you want to proceed with the upload? (y/N): \").strip().lower()\n    if response not in [\"y\", \"yes\", \"Y\"]:\n        exit(0)\n\n    registry.update(entry_location, entry_metadata)\n    pbar = tqdm(total=total_size, unit=\"B\", unit_scale=True, disable=not show_progress)\n    for file, relative, size in files_to_upload:\n        registry.upload_file(entry_location, file, relative, encryption_key=encryption_key)\n        pbar.update(size)\n\n    if encrypt:\n        console = Console()\n        console.print(\n            Panel(\n                Text.assemble(\n                    (\"\ud83d\udd10 Encryption enabled\\n\\n\", \"bold green\"),\n                    (f\"Encryption key {encryption_key} generated and stored in metadata.json\\n\", \"dim\"),\n                ),\n                title=\"Encryption\",\n                border_style=\"green\",\n                padding=(1, 2),\n            )\n        )\n\n    return entry_location\n</code></pre>"},{"location":"api/#nearai.registry.Registry.upload_file","title":"upload_file","text":"<pre><code>upload_file(entry_location: EntryLocation, local_path: Path, path: Path, encryption_key: Optional[str] = None) -&gt; bool\n</code></pre> <p>Upload a file to the registry.</p> Source code in <code>nearai/registry.py</code> <pre><code>def upload_file(\n    self, entry_location: EntryLocation, local_path: Path, path: Path, encryption_key: Optional[str] = None\n) -&gt; bool:\n    \"\"\"Upload a file to the registry.\"\"\"\n    with open(local_path, \"rb\") as file:\n        data = file.read()\n\n        # Encrypt data if encryption is enabled\n        if encryption_key:\n            data = FileEncryption.encrypt_data(data, encryption_key)\n\n        try:\n            self.api.upload_file_v1_registry_upload_file_post(\n                path=str(path),\n                file=data,\n                namespace=entry_location.namespace,\n                name=entry_location.name,\n                version=entry_location.version,\n            )\n            return True\n        except BadRequestException as e:\n            if isinstance(e.body, str) and \"already exists\" in e.body:\n                return False\n\n            raise e\n</code></pre>"},{"location":"api/#nearai.registry.check_version_exists","title":"check_version_exists","text":"<pre><code>check_version_exists(namespace: str, name: str, version: str) -&gt; Tuple[bool, Optional[str]]\n</code></pre> <p>Check if a version already exists in the registry.</p> <pre><code>namespace: The namespace\nname: The agent name\nversion: The version to check\n</code></pre> <pre><code>Tuple of (exists, error)\nIf exists is True, the version exists\nIf error is not None, an error occurred during checking\n</code></pre> Source code in <code>nearai/registry.py</code> <pre><code>def check_version_exists(namespace: str, name: str, version: str) -&gt; Tuple[bool, Optional[str]]:\n    \"\"\"Check if a version already exists in the registry.\n\n    Args:\n    ----\n        namespace: The namespace\n        name: The agent name\n        version: The version to check\n\n    Returns:\n    -------\n        Tuple of (exists, error)\n        If exists is True, the version exists\n        If error is not None, an error occurred during checking\n\n    \"\"\"\n    entry_location = f\"{namespace}/{name}/{version}\"\n    try:\n        existing_entry = registry.info(parse_location(entry_location))\n\n        if existing_entry:\n            return True, None\n        return False, None\n    except Exception as e:\n        # Only proceed if the error indicates the entry doesn't exist\n        if \"not found\" in str(e).lower() or \"does not exist\" in str(e).lower():\n            return False, None\n        return False, f\"Error checking registry: {str(e)}\"\n</code></pre>"},{"location":"api/#nearai.registry.get_namespace","title":"get_namespace","text":"<pre><code>get_namespace(local_path: Path) -&gt; str\n</code></pre> <p>Returns namespace of an item or user namespace.</p> Source code in <code>nearai/registry.py</code> <pre><code>def get_namespace(local_path: Path) -&gt; str:\n    \"\"\"Returns namespace of an item or user namespace.\"\"\"\n    registry_folder = get_registry_folder()\n\n    try:\n        # Check if the path matches the expected structure\n        relative_path = local_path.relative_to(registry_folder)\n\n    except ValueError:\n        # If local_path is not relative to registry_folder, try resolving it to an absolute path\n        local_path = local_path.resolve()\n        try:\n            # Retry checking if the now absolute path is within registry_folder\n            relative_path = local_path.relative_to(registry_folder)\n        except ValueError:\n            relative_path = None\n            pass\n\n    if relative_path:\n        parts = relative_path.parts\n\n        # If the path has 3 parts (namespace, item_name, version),\n        # return the first part as the namespace\n        if len(parts) == 3:\n            return str(parts[0])\n\n    # If we couldn't extract a namespace from the path, return the default\n    if CONFIG.auth is None:\n        raise ValueError(\"AuthData is None\")\n    return CONFIG.auth.namespace\n</code></pre>"},{"location":"api/#nearai.registry.get_registry_folder","title":"get_registry_folder","text":"<pre><code>get_registry_folder() -&gt; Path\n</code></pre> <p>Path to local registry.</p> Source code in <code>nearai/registry.py</code> <pre><code>def get_registry_folder() -&gt; Path:\n    \"\"\"Path to local registry.\"\"\"\n    return DATA_FOLDER / REGISTRY_FOLDER\n</code></pre>"},{"location":"api/#nearai.registry.increment_version_by_type","title":"increment_version_by_type","text":"<pre><code>increment_version_by_type(version: str, increment_type: str) -&gt; str\n</code></pre> <p>Increment version according to PEP 440.</p> <pre><code>version: Current version string\nincrement_type: Type of increment ('major', 'minor', or 'patch')\n</code></pre> <pre><code>New version string\n</code></pre> <pre><code>ValueError: If increment_type is invalid or version is invalid\n</code></pre> Source code in <code>nearai/registry.py</code> <pre><code>def increment_version_by_type(version: str, increment_type: str) -&gt; str:\n    \"\"\"Increment version according to PEP 440.\n\n    Args:\n    ----\n        version: Current version string\n        increment_type: Type of increment ('major', 'minor', or 'patch')\n\n    Returns:\n    -------\n        New version string\n\n    Raises:\n    ------\n        ValueError: If increment_type is invalid or version is invalid\n\n    \"\"\"\n    try:\n        v = Version(version)\n        major, minor, micro = v.release[:3]\n\n        if increment_type == \"major\":\n            return f\"{major + 1}.0.0\"\n        elif increment_type == \"minor\":\n            return f\"{major}.{minor + 1}.0\"\n        elif increment_type == \"patch\":\n            return f\"{major}.{minor}.{micro + 1}\"\n        else:\n            raise ValueError(f\"Invalid increment type: {increment_type}\")\n    except InvalidVersion as e:\n        raise ValueError(f\"Invalid version format: {str(e)}\") from e\n</code></pre>"},{"location":"api/#nearai.registry.resolve_local_path","title":"resolve_local_path","text":"<pre><code>resolve_local_path(local_path: Path) -&gt; Path\n</code></pre> <p>Determines if the <code>local_path</code> is <code>local_path</code> or <code>registry_folder/local_path</code>.</p> <p>Raises FileNotFoundError if folder or parent folder is not present.</p> Source code in <code>nearai/registry.py</code> <pre><code>def resolve_local_path(local_path: Path) -&gt; Path:\n    \"\"\"Determines if the `local_path` is `local_path` or `registry_folder/local_path`.\n\n    Raises FileNotFoundError if folder or parent folder is not present.\n    \"\"\"\n    if local_path.exists() or local_path.parent.exists():\n        return local_path\n\n    registry_path = get_registry_folder() / local_path\n    if registry_path.exists() or registry_path.parent.exists():\n        return registry_path\n\n    # If neither exists, raise an error\n    raise FileNotFoundError(f\"Path not found: {local_path} or {registry_path}\")\n</code></pre>"},{"location":"api/#nearai.registry.validate_version","title":"validate_version","text":"<pre><code>validate_version(version: str) -&gt; Tuple[bool, Optional[str]]\n</code></pre> <p>Validate version string according to PEP 440.</p> <pre><code>version: Version string to validate\n</code></pre> <pre><code>Tuple of (is_valid, error_message)\n</code></pre> Source code in <code>nearai/registry.py</code> <pre><code>def validate_version(version: str) -&gt; Tuple[bool, Optional[str]]:\n    \"\"\"Validate version string according to PEP 440.\n\n    Args:\n    ----\n        version: Version string to validate\n\n    Returns:\n    -------\n        Tuple of (is_valid, error_message)\n\n    \"\"\"\n    try:\n        Version(version)\n        return True, None\n    except InvalidVersion as e:\n        return False, f\"Invalid version format: {str(e)}. Version must follow PEP 440:https://peps.python.org/pep-0440.\"\n</code></pre>"},{"location":"api/#nearai.shared","title":"shared","text":""},{"location":"api/#nearai.shared.auth_data","title":"auth_data","text":""},{"location":"api/#nearai.shared.auth_data.AuthData","title":"AuthData","text":"<p>               Bases: <code>BaseModel</code></p> Source code in <code>nearai/shared/auth_data.py</code> <pre><code>class AuthData(BaseModel):\n    account_id: str\n    signature: str\n    public_key: str\n    callback_url: str\n    nonce: str\n    recipient: str\n    message: str\n    on_behalf_of: Optional[str] = None\n\n    def generate_bearer_token(self):\n        \"\"\"Generates a JSON-encoded bearer token containing authentication data.\"\"\"\n        required_keys = {\"account_id\", \"public_key\", \"signature\", \"callback_url\", \"message\", \"nonce\", \"recipient\"}\n\n        for key in required_keys:\n            if getattr(self, key) is None:\n                raise ValueError(f\"Missing required auth data: {key}\")\n\n        if self.on_behalf_of is not None:\n            required_keys.add(\"on_behalf_of\")\n\n        bearer_data = {key: getattr(self, key) for key in required_keys}\n\n        return json.dumps(bearer_data)\n\n    @property\n    def namespace(self):\n        \"\"\"Get the account ID for the auth data.\n\n        In case you are running a request on behalf of another account, this will return the account ID of the account.\n        \"\"\"\n        if self.on_behalf_of is not None:\n            return self.on_behalf_of\n        return self.account_id\n</code></pre>"},{"location":"api/#nearai.shared.auth_data.AuthData.namespace","title":"namespace  <code>property</code>","text":"<pre><code>namespace\n</code></pre> <p>Get the account ID for the auth data.</p> <p>In case you are running a request on behalf of another account, this will return the account ID of the account.</p>"},{"location":"api/#nearai.shared.auth_data.AuthData.generate_bearer_token","title":"generate_bearer_token","text":"<pre><code>generate_bearer_token()\n</code></pre> <p>Generates a JSON-encoded bearer token containing authentication data.</p> Source code in <code>nearai/shared/auth_data.py</code> <pre><code>def generate_bearer_token(self):\n    \"\"\"Generates a JSON-encoded bearer token containing authentication data.\"\"\"\n    required_keys = {\"account_id\", \"public_key\", \"signature\", \"callback_url\", \"message\", \"nonce\", \"recipient\"}\n\n    for key in required_keys:\n        if getattr(self, key) is None:\n            raise ValueError(f\"Missing required auth data: {key}\")\n\n    if self.on_behalf_of is not None:\n        required_keys.add(\"on_behalf_of\")\n\n    bearer_data = {key: getattr(self, key) for key in required_keys}\n\n    return json.dumps(bearer_data)\n</code></pre>"},{"location":"api/#nearai.shared.cache","title":"cache","text":""},{"location":"api/#nearai.shared.cache.mem_cache_with_timeout","title":"mem_cache_with_timeout","text":"<pre><code>mem_cache_with_timeout(timeout: int)\n</code></pre> <p>Decorator to cache function results for a specified timeout period.</p> Source code in <code>nearai/shared/cache.py</code> <pre><code>def mem_cache_with_timeout(timeout: int):\n    \"\"\"Decorator to cache function results for a specified timeout period.\"\"\"\n\n    def decorator(func):\n        cache = {}\n\n        @wraps(func)\n        def wrapper(*args, **kwargs):\n            now = time.time()\n            key = (args, frozenset(kwargs.items()))\n            if key in cache:\n                result, timestamp = cache[key]\n                if now - timestamp &lt; timeout:\n                    return result\n            result = func(*args, **kwargs)\n            cache[key] = (result, now)\n            return result\n\n        return wrapper\n\n    return decorator\n</code></pre>"},{"location":"api/#nearai.shared.client_config","title":"client_config","text":""},{"location":"api/#nearai.shared.client_config.ClientConfig","title":"ClientConfig","text":"<p>               Bases: <code>BaseModel</code></p> Source code in <code>nearai/shared/client_config.py</code> <pre><code>class ClientConfig(BaseModel):\n    base_url: str = \"https://api.near.ai/v1\"\n    custom_llm_provider: str = \"openai\"\n    auth: Optional[AuthData] = None\n    default_provider: Optional[str] = None  # future: remove in favor of api decision\n    num_inference_retries: int = 1\n\n    def get_hub_client(self):\n        \"\"\"Get the hub client.\"\"\"\n        signature = f\"Bearer {self.auth.model_dump_json()}\" if self.auth else None\n        base_url = self.base_url\n        return openai.OpenAI(\n            base_url=base_url, api_key=signature, timeout=DEFAULT_TIMEOUT, max_retries=DEFAULT_MAX_RETRIES\n        )\n</code></pre>"},{"location":"api/#nearai.shared.client_config.ClientConfig.get_hub_client","title":"get_hub_client","text":"<pre><code>get_hub_client()\n</code></pre> <p>Get the hub client.</p> Source code in <code>nearai/shared/client_config.py</code> <pre><code>def get_hub_client(self):\n    \"\"\"Get the hub client.\"\"\"\n    signature = f\"Bearer {self.auth.model_dump_json()}\" if self.auth else None\n    base_url = self.base_url\n    return openai.OpenAI(\n        base_url=base_url, api_key=signature, timeout=DEFAULT_TIMEOUT, max_retries=DEFAULT_MAX_RETRIES\n    )\n</code></pre>"},{"location":"api/#nearai.shared.file_encryption","title":"file_encryption","text":""},{"location":"api/#nearai.shared.file_encryption.FileEncryption","title":"FileEncryption","text":"<p>Handles file encryption and decryption for registry uploads.</p> Source code in <code>nearai/shared/file_encryption.py</code> <pre><code>class FileEncryption:\n    \"\"\"Handles file encryption and decryption for registry uploads.\"\"\"\n\n    @staticmethod\n    def generate_encryption_key() -&gt; str:\n        \"\"\"Generate a new encryption key.\n\n        Returns:\n            Base64-encoded encryption key string\n\n        \"\"\"\n        key = Fernet.generate_key()\n        return base64.urlsafe_b64encode(key).decode(\"utf-8\")\n\n    @staticmethod\n    def encrypt_data(data: bytes, encryption_key: str) -&gt; bytes:\n        \"\"\"Encrypt data using the provided encryption key.\n\n        Args:\n            data: Raw bytes to encrypt\n            encryption_key: Base64-encoded encryption key\n\n        Returns:\n            Encrypted bytes\n\n        \"\"\"\n        # Decode the base64 key\n        key_bytes = base64.urlsafe_b64decode(encryption_key.encode(\"utf-8\"))\n        fernet = Fernet(key_bytes)\n        return fernet.encrypt(data)\n\n    @staticmethod\n    def decrypt_data(encrypted_data: bytes, encryption_key: str) -&gt; bytes:\n        \"\"\"Decrypt data using the provided encryption key.\n\n        Args:\n            encrypted_data: Encrypted bytes to decrypt\n            encryption_key: Base64-encoded encryption key\n\n        Returns:\n            Decrypted bytes\n\n        \"\"\"\n        # Decode the base64 key\n        key_bytes = base64.urlsafe_b64decode(encryption_key.encode(\"utf-8\"))\n        fernet = Fernet(key_bytes)\n        return fernet.decrypt(encrypted_data)\n</code></pre>"},{"location":"api/#nearai.shared.file_encryption.FileEncryption.decrypt_data","title":"decrypt_data  <code>staticmethod</code>","text":"<pre><code>decrypt_data(encrypted_data: bytes, encryption_key: str) -&gt; bytes\n</code></pre> <p>Decrypt data using the provided encryption key.</p> <p>Parameters:</p> Name Type Description Default <code>encrypted_data</code> <code>bytes</code> <p>Encrypted bytes to decrypt</p> required <code>encryption_key</code> <code>str</code> <p>Base64-encoded encryption key</p> required <p>Returns:</p> Type Description <code>bytes</code> <p>Decrypted bytes</p> Source code in <code>nearai/shared/file_encryption.py</code> <pre><code>@staticmethod\ndef decrypt_data(encrypted_data: bytes, encryption_key: str) -&gt; bytes:\n    \"\"\"Decrypt data using the provided encryption key.\n\n    Args:\n        encrypted_data: Encrypted bytes to decrypt\n        encryption_key: Base64-encoded encryption key\n\n    Returns:\n        Decrypted bytes\n\n    \"\"\"\n    # Decode the base64 key\n    key_bytes = base64.urlsafe_b64decode(encryption_key.encode(\"utf-8\"))\n    fernet = Fernet(key_bytes)\n    return fernet.decrypt(encrypted_data)\n</code></pre>"},{"location":"api/#nearai.shared.file_encryption.FileEncryption.encrypt_data","title":"encrypt_data  <code>staticmethod</code>","text":"<pre><code>encrypt_data(data: bytes, encryption_key: str) -&gt; bytes\n</code></pre> <p>Encrypt data using the provided encryption key.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>bytes</code> <p>Raw bytes to encrypt</p> required <code>encryption_key</code> <code>str</code> <p>Base64-encoded encryption key</p> required <p>Returns:</p> Type Description <code>bytes</code> <p>Encrypted bytes</p> Source code in <code>nearai/shared/file_encryption.py</code> <pre><code>@staticmethod\ndef encrypt_data(data: bytes, encryption_key: str) -&gt; bytes:\n    \"\"\"Encrypt data using the provided encryption key.\n\n    Args:\n        data: Raw bytes to encrypt\n        encryption_key: Base64-encoded encryption key\n\n    Returns:\n        Encrypted bytes\n\n    \"\"\"\n    # Decode the base64 key\n    key_bytes = base64.urlsafe_b64decode(encryption_key.encode(\"utf-8\"))\n    fernet = Fernet(key_bytes)\n    return fernet.encrypt(data)\n</code></pre>"},{"location":"api/#nearai.shared.file_encryption.FileEncryption.generate_encryption_key","title":"generate_encryption_key  <code>staticmethod</code>","text":"<pre><code>generate_encryption_key() -&gt; str\n</code></pre> <p>Generate a new encryption key.</p> <p>Returns:</p> Type Description <code>str</code> <p>Base64-encoded encryption key string</p> Source code in <code>nearai/shared/file_encryption.py</code> <pre><code>@staticmethod\ndef generate_encryption_key() -&gt; str:\n    \"\"\"Generate a new encryption key.\n\n    Returns:\n        Base64-encoded encryption key string\n\n    \"\"\"\n    key = Fernet.generate_key()\n    return base64.urlsafe_b64encode(key).decode(\"utf-8\")\n</code></pre>"},{"location":"api/#nearai.shared.inference_client","title":"inference_client","text":""},{"location":"api/#nearai.shared.inference_client.InferenceClient","title":"InferenceClient","text":"<p>               Bases: <code>object</code></p> Source code in <code>nearai/shared/inference_client.py</code> <pre><code>class InferenceClient(object):\n    def __init__(self, config: ClientConfig, runner_api_key: str = \"\", agent_identifier: str = \"\") -&gt; None:  # noqa: D107\n        self._config = config\n        self.runner_api_key = runner_api_key\n        self.agent_identifier = agent_identifier\n        self._auth = None\n        self.generate_auth_for_current_agent(config, agent_identifier)\n        self.client = openai.OpenAI(base_url=self._config.base_url, api_key=self._auth)\n        self._provider_models: Optional[ProviderModels] = None\n\n    def generate_auth_for_current_agent(self, config, agent_identifier):\n        \"\"\"Regenerate auth for the current agent.\"\"\"\n        self.agent_identifier = agent_identifier\n        if config.auth is not None:\n            auth_bearer_token = config.auth.generate_bearer_token()\n            new_token = json.loads(auth_bearer_token)\n            new_token[\"runner_data\"] = json.dumps({\"agent\": agent_identifier, \"runner_api_key\": self.runner_api_key})\n            auth_bearer_token = json.dumps(new_token)\n            self._auth = auth_bearer_token\n        else:\n            self._auth = None\n\n    # This makes sense in the CLI where we don't mind doing this request and caching it.\n    # In the aws_runner this is an extra request every time we run.\n    # TODO(#233): add a choice of a provider model in aws_runner, and then this step can be skipped.\n    @cached_property\n    def provider_models(self) -&gt; ProviderModels:  # noqa: D102\n        if self._provider_models is None:\n            self._provider_models = ProviderModels(self._config)\n        return self._provider_models\n\n    def set_provider_models(self, provider_models: Optional[ProviderModels]):\n        \"\"\"Set provider models. Used by external caching.\"\"\"\n        if provider_models is None:\n            self._provider_models = ProviderModels(self._config)\n        else:\n            self._provider_models = provider_models\n\n    def get_agent_public_key(self, agent_name: str) -&gt; str:\n        \"\"\"Request agent public key.\"\"\"\n        headers = {\n            \"Content-Type\": \"application/json\",\n        }\n\n        data = {\"agent_name\": agent_name}\n\n        endpoint = f\"{self._config.base_url}/get_agent_public_key\"\n\n        try:\n            response = requests.post(endpoint, headers=headers, params=data)\n            response.raise_for_status()\n            return response.json()\n        except requests.RequestException as e:\n            raise ValueError(f\"Failed to get agent public key: {e}\") from None\n\n    def _match_provider_model(self, model: str) -&gt; tuple[str, str]:\n        \"\"\"Returns provider and model for given `model` and `_config`.\"\"\"\n        if self._config.base_url != self._config.default_provider:\n            provider, model = self.provider_models.match_provider_model(model)\n        else:\n            provider = self._config.default_provider\n        return provider, model\n\n    def completions(\n        self,\n        model: str,\n        messages: Iterable[ChatCompletionMessageParam],\n        stream: bool = False,\n        temperature: Optional[float] = None,\n        max_tokens: Optional[int] = None,\n        **kwargs: Any,\n    ) -&gt; Union[ModelResponse, CustomStreamWrapper]:\n        \"\"\"Takes a `model` and `messages` and returns completions.\n\n        `model` can be:\n        1. full path `provider::model_full_path`.\n        2. `model_short_name`. Default provider will be used.\n        \"\"\"\n        provider, model = self._match_provider_model(model)\n\n        if temperature is None:\n            temperature = DEFAULT_MODEL_TEMPERATURE\n\n        if max_tokens is None:\n            max_tokens = DEFAULT_MODEL_MAX_TOKENS\n\n        # NOTE(#246): this is to disable \"Provider List\" messages.\n        litellm.suppress_debug_info = True\n\n        # lite_llm uses the openai.request_timeout to set the timeout for the request of \"openai\" custom provider\n        openai.timeout = DEFAULT_TIMEOUT\n        openai.max_retries = DEFAULT_MAX_RETRIES\n\n        for i in range(0, self._config.num_inference_retries):\n            try:\n                # Create a dictionary for the arguments\n                completion_args = {\n                    \"model\": model,\n                    \"messages\": messages,\n                    \"stream\": stream,\n                    \"custom_llm_provider\": self._config.custom_llm_provider,\n                    \"input_cost_per_token\": 0,\n                    \"output_cost_per_token\": 0,\n                    \"temperature\": temperature,\n                    \"max_tokens\": max_tokens,\n                    \"base_url\": self._config.base_url,\n                    \"api_key\": self._auth,\n                    \"timeout\": DEFAULT_TIMEOUT,\n                    \"request_timeout\": DEFAULT_TIMEOUT,\n                    \"num_retries\": 1,\n                }\n                # Only add provider parameter if base_url is different from default_provider\n                if self._config.base_url != self._config.default_provider:\n                    completion_args[\"provider\"] = provider\n                # Add any additional kwargs\n                completion_args.update(kwargs)\n                result: Union[ModelResponse, CustomStreamWrapper] = litellm_completion(**completion_args)\n                break\n            except Exception as e:\n                print(\"Completions exception:\", e)\n                if i == self._config.num_inference_retries - 1:\n                    raise ValueError(f\"Bad request: {e}\") from None\n                else:\n                    print(\"Retrying...\")\n\n        return result\n\n    def query_vector_store(\n        self, vector_store_id: str, query: str, full_files: bool = False\n    ) -&gt; Union[List[SimilaritySearch], List[SimilaritySearchFile]]:\n        \"\"\"Query a vector store.\"\"\"\n        if self._config is None:\n            raise ValueError(\"Missing NEAR AI Hub config\")\n\n        auth_bearer_token = self._auth\n\n        headers = {\n            \"Content-Type\": \"application/json\",\n            \"Authorization\": f\"Bearer {auth_bearer_token}\",\n        }\n\n        data = {\"query\": query, \"full_files\": full_files}\n\n        endpoint = f\"{self._config.base_url}/vector_stores/{vector_store_id}/search\"\n\n        try:\n            response = requests.post(endpoint, headers=headers, json=data)\n            response.raise_for_status()\n            return response.json()\n        except requests.RequestException as e:\n            raise ValueError(f\"Error querying vector store: {e}\") from None\n\n    def upload_file(\n        self,\n        file_content: str,\n        purpose: Literal[\"assistants\", \"batch\", \"fine-tune\", \"vision\"],\n        encoding: Optional[str] = \"utf-8\",\n        file_name: Optional[str] = \"file.txt\",\n        file_type: Optional[str] = \"text/plain\",\n    ) -&gt; Optional[FileObject]:\n        \"\"\"Uploads a file.\"\"\"\n        client = openai.OpenAI(base_url=self._config.base_url, api_key=self._auth)\n        if file_content:\n            file_data = io.BytesIO(file_content.encode(encoding or \"utf-8\"))\n            return client.files.create(file=(file_name, file_data, file_type), purpose=purpose)\n        else:\n            return None\n\n    def remove_file(self, file_id: str):\n        \"\"\"Removes a file.\"\"\"\n        client = openai.OpenAI(base_url=self._config.base_url, api_key=self._auth)\n        return client.files.delete(file_id=file_id)\n\n    def add_file_to_vector_store(self, vector_store_id: str, file_id: str) -&gt; VectorStoreFile:\n        \"\"\"Adds a file to vector store.\"\"\"\n        client = openai.OpenAI(base_url=self._config.base_url, api_key=self._auth)\n        return client.vector_stores.files.create(vector_store_id=vector_store_id, file_id=file_id)\n\n    def get_vector_store_files(self, vector_store_id: str) -&gt; Optional[List[VectorStoreFile]]:\n        \"\"\"Adds a file to vector store.\"\"\"\n        client = openai.OpenAI(base_url=self._config.base_url, api_key=self._auth)\n        return client.vector_stores.files.list(vector_store_id=vector_store_id).data\n\n    def create_vector_store_from_source(\n        self,\n        name: str,\n        source: Union[GitHubSource, GitLabSource],\n        source_auth: Optional[str] = None,\n        chunking_strategy: Optional[ChunkingStrategy] = None,\n        expires_after: Optional[ExpiresAfter] = None,\n        metadata: Optional[Dict[str, str]] = None,\n    ) -&gt; VectorStore:\n        \"\"\"Creates a vector store from the given source.\n\n        Args:\n        ----\n            name (str): The name of the vector store.\n            source (Union[GitHubSource, GitLabSource]): The source from which to create the vector store.\n            source_auth (Optional[str]): The source authentication token.\n            chunking_strategy (Optional[ChunkingStrategy]): The chunking strategy to use.\n            expires_after (Optional[ExpiresAfter]): The expiration policy.\n            metadata (Optional[Dict[str, str]]): Additional metadata.\n\n        Returns:\n        -------\n            VectorStore: The created vector store.\n\n        \"\"\"\n        print(f\"Creating vector store from source: {source}\")\n        headers = {\n            \"Authorization\": f\"Bearer {self._auth}\",\n            \"Content-Type\": \"application/json\",\n        }\n        data = {\n            \"name\": name,\n            \"source\": source,\n            \"source_auth\": source_auth,\n            \"chunking_strategy\": chunking_strategy,\n            \"expires_after\": expires_after,\n            \"metadata\": metadata,\n        }\n        endpoint = f\"{self._config.base_url}/vector_stores/from_source\"\n\n        try:\n            response = requests.post(endpoint, headers=headers, json=data)\n            print(response.json())\n            response.raise_for_status()\n            return VectorStore(**response.json())\n        except requests.RequestException as e:\n            raise ValueError(f\"Failed to create vector store: {e}\") from None\n\n    def create_vector_store(\n        self,\n        name: str,\n        file_ids: List[str],\n        expires_after: Union[ExpiresAfter, NotGiven] = NOT_GIVEN,\n        chunking_strategy: Union[\n            AutoFileChunkingStrategyParam, StaticFileChunkingStrategyObjectParam, NotGiven\n        ] = NOT_GIVEN,\n        metadata: Optional[Dict[str, str]] = None,\n    ) -&gt; VectorStore:\n        \"\"\"Creates Vector Store.\n\n        :param name: Vector store name.\n        :param file_ids: Files to be added to the vector store.\n        :param expires_after: Expiration policy.\n        :param chunking_strategy: Chunking strategy.\n        :param metadata: Additional metadata.\n        :return: Returns the created vector store or error.\n        \"\"\"\n        client = openai.OpenAI(base_url=self._config.base_url, api_key=self._auth)\n        return client.vector_stores.create(\n            file_ids=file_ids,\n            name=name,\n            expires_after=expires_after,\n            chunking_strategy=chunking_strategy,\n            metadata=metadata,\n        )\n\n    def get_vector_store(self, vector_store_id: str) -&gt; VectorStore:\n        \"\"\"Gets a vector store by id.\"\"\"\n        endpoint = f\"{self._config.base_url}/vector_stores/{vector_store_id}\"\n        auth_bearer_token = self._auth\n\n        headers = {\n            \"Content-Type\": \"application/json\",\n            \"Authorization\": f\"Bearer {auth_bearer_token}\",\n        }\n\n        response = requests.get(endpoint, headers=headers)\n        response.raise_for_status()\n        return VectorStore(**response.json())\n\n    def create_thread(self, messages):\n        \"\"\"Create a thread.\"\"\"\n        return self.client.beta.threads.create(messages=messages)\n\n    def get_thread(self, thread_id: str):\n        \"\"\"Get a thread.\"\"\"\n        return self.client.beta.threads.retrieve(thread_id=thread_id)\n\n    def threads_messages_create(self, thread_id: str, content: str, role: Literal[\"user\", \"assistant\"]):\n        \"\"\"Create a message in a thread.\"\"\"\n        return self.client.beta.threads.messages.create(thread_id=thread_id, content=content, role=role)\n\n    def threads_create_and_run_poll(self, assistant_id: str, model: str, messages: List[ChatCompletionMessageParam]):\n        \"\"\"Create a thread and run the assistant.\"\"\"\n        thread = self.create_thread(messages)\n        return self.client.beta.threads.create_and_run_poll(thread=thread, assistant_id=assistant_id, model=model)\n\n    def threads_list_messages(self, thread_id: str, order: Literal[\"asc\", \"desc\"] = \"asc\"):\n        \"\"\"List messages in a thread.\"\"\"\n        return self.client.beta.threads.messages.list(thread_id=thread_id, order=order)\n\n    def threads_fork(self, thread_id: str):\n        \"\"\"Fork a thread.\"\"\"\n        forked_thread = self.client.post(path=f\"{self._config.base_url}/threads/{thread_id}/fork\", cast_to=Thread)\n        return forked_thread\n\n    def create_subthread(\n        self,\n        thread_id: str,\n        messages_to_copy: Optional[List[str]] = None,\n        new_messages: Optional[List[ChatCompletionMessageParam]] = None,\n    ):\n        \"\"\"Create a subthread.\"\"\"\n        return self.client.post(\n            path=f\"{self._config.base_url}/threads/{thread_id}/subthread\",\n            body={messages_to_copy: messages_to_copy, new_messages: new_messages},\n            cast_to=Thread,\n        )\n\n    def threads_runs_create(self, thread_id: str, assistant_id: str, model: str):\n        \"\"\"Create a run in a thread.\"\"\"\n        return self.client.beta.threads.runs.create(thread_id=thread_id, assistant_id=assistant_id, model=model)\n\n    def run_agent(\n        self, run_on_thread_id: str, assistant_id: str, parent_run_id: str = \"\", run_mode: RunMode = RunMode.SIMPLE\n    ):\n        \"\"\"Starts a child agent run from a parent agent run.\"\"\"\n        extra_body = {}\n        if parent_run_id:\n            extra_body[\"parent_run_id\"] = parent_run_id\n        extra_body[\"run_mode\"] = run_mode.value  # type: ignore\n        return self.client.beta.threads.runs.create(\n            thread_id=run_on_thread_id,\n            assistant_id=assistant_id,\n            extra_body=extra_body,\n        )\n\n    def schedule_run(\n        self,\n        agent: str,\n        input_message: str,\n        thread_id: Optional[str],\n        run_params: Optional[Dict[str, str]],\n        run_at: datetime,\n    ):\n        \"\"\"Query a vector store.\"\"\"\n        if self._config is None:\n            raise ValueError(\"Missing NearAI Hub config\")\n\n        auth_bearer_token = self._auth\n\n        headers = {\n            \"Content-Type\": \"application/json\",\n            \"Authorization\": f\"Bearer {auth_bearer_token}\",\n        }\n\n        if run_params is None:\n            run_params = {}\n\n        data = {\n            \"agent\": agent,\n            \"input_message\": input_message,\n            \"thread_id\": thread_id,\n            \"run_params\": run_params,\n            \"run_at\": run_at,\n        }\n\n        endpoint = f\"{self._config.base_url}/schedule_run\"\n\n        try:\n            response = requests.post(endpoint, headers=headers, json=data)\n            response.raise_for_status()\n            return response.json()\n        except requests.RequestException as e:\n            raise ValueError(f\"Error querying schedule_run: {e}\") from None\n\n    def query_user_memory(self, query: str):\n        \"\"\"Query the user memory.\"\"\"\n        return self.client.post(\n            path=f\"{self._config.base_url}/vector_stores/memory/query\",\n            body={\"query\": query},\n            cast_to=str,\n        )\n\n    def add_user_memory(self, memory: str):\n        \"\"\"Add user memory.\"\"\"\n        return self.client.post(\n            path=f\"{self._config.base_url}/vector_stores/memory\",\n            body={\"memory\": memory},\n            cast_to=str,\n        )\n\n    def generate_image(self, prompt: str, model: Optional[str] = None):\n        \"\"\"Generate an image using the specified model or the default if none is provided.\"\"\"\n        if model:\n            _provider, model = self._match_provider_model(model)\n        return self.client.images.generate(prompt=prompt, model=model)\n\n    def save_agent_data(self, key: str, agent_data: Dict[str, Any]):\n        \"\"\"Save agent data for the agent this client was initialized with.\"\"\"\n        return self.client.post(\n            path=f\"{self._config.base_url}/agent_data\",\n            body={\n                \"key\": key,\n                \"value\": agent_data,\n            },\n            cast_to=Dict[str, Any],\n        )\n\n    def get_agent_data(self):\n        \"\"\"Get agent data for the agent this client was initialized with.\"\"\"\n        return self.client.get(\n            path=f\"{self._config.base_url}/agent_data\",\n            cast_to=Dict[str, str],\n        )\n\n    def get_agent_data_by_key(self, key: str):\n        \"\"\"Get agent data by key for the agent this client was initialized with.\"\"\"\n        return self.client.get(\n            path=f\"{self._config.base_url}/agent_data/{key}\",\n            cast_to=Dict[str, str],\n        )\n\n    def find_agents(\n        self,\n        owner_id: Optional[str] = None,\n        with_capabilities: Optional[bool] = False,\n        latest_versions_only: Optional[bool] = True,\n        limit: Optional[int] = None,\n        offset: Optional[int] = None,\n    ):\n        \"\"\"Filter agents.\"\"\"\n        return self.client.post(\n            path=f\"{self._config.base_url}/find_agents\",\n            body={\n                \"owner_id\": owner_id,\n                \"with_capabilities\": with_capabilities,\n                \"latest_versions_only\": latest_versions_only,\n                \"limit\": limit,\n                \"offset\": offset,\n            },\n            cast_to=List[Any],\n        )\n</code></pre>"},{"location":"api/#nearai.shared.inference_client.InferenceClient._match_provider_model","title":"_match_provider_model","text":"<pre><code>_match_provider_model(model: str) -&gt; tuple[str, str]\n</code></pre> <p>Returns provider and model for given <code>model</code> and <code>_config</code>.</p> Source code in <code>nearai/shared/inference_client.py</code> <pre><code>def _match_provider_model(self, model: str) -&gt; tuple[str, str]:\n    \"\"\"Returns provider and model for given `model` and `_config`.\"\"\"\n    if self._config.base_url != self._config.default_provider:\n        provider, model = self.provider_models.match_provider_model(model)\n    else:\n        provider = self._config.default_provider\n    return provider, model\n</code></pre>"},{"location":"api/#nearai.shared.inference_client.InferenceClient.add_file_to_vector_store","title":"add_file_to_vector_store","text":"<pre><code>add_file_to_vector_store(vector_store_id: str, file_id: str) -&gt; VectorStoreFile\n</code></pre> <p>Adds a file to vector store.</p> Source code in <code>nearai/shared/inference_client.py</code> <pre><code>def add_file_to_vector_store(self, vector_store_id: str, file_id: str) -&gt; VectorStoreFile:\n    \"\"\"Adds a file to vector store.\"\"\"\n    client = openai.OpenAI(base_url=self._config.base_url, api_key=self._auth)\n    return client.vector_stores.files.create(vector_store_id=vector_store_id, file_id=file_id)\n</code></pre>"},{"location":"api/#nearai.shared.inference_client.InferenceClient.add_user_memory","title":"add_user_memory","text":"<pre><code>add_user_memory(memory: str)\n</code></pre> <p>Add user memory.</p> Source code in <code>nearai/shared/inference_client.py</code> <pre><code>def add_user_memory(self, memory: str):\n    \"\"\"Add user memory.\"\"\"\n    return self.client.post(\n        path=f\"{self._config.base_url}/vector_stores/memory\",\n        body={\"memory\": memory},\n        cast_to=str,\n    )\n</code></pre>"},{"location":"api/#nearai.shared.inference_client.InferenceClient.completions","title":"completions","text":"<pre><code>completions(model: str, messages: Iterable[ChatCompletionMessageParam], stream: bool = False, temperature: Optional[float] = None, max_tokens: Optional[int] = None, **kwargs: Any) -&gt; Union[ModelResponse, CustomStreamWrapper]\n</code></pre> <p>Takes a <code>model</code> and <code>messages</code> and returns completions.</p> <p><code>model</code> can be: 1. full path <code>provider::model_full_path</code>. 2. <code>model_short_name</code>. Default provider will be used.</p> Source code in <code>nearai/shared/inference_client.py</code> <pre><code>def completions(\n    self,\n    model: str,\n    messages: Iterable[ChatCompletionMessageParam],\n    stream: bool = False,\n    temperature: Optional[float] = None,\n    max_tokens: Optional[int] = None,\n    **kwargs: Any,\n) -&gt; Union[ModelResponse, CustomStreamWrapper]:\n    \"\"\"Takes a `model` and `messages` and returns completions.\n\n    `model` can be:\n    1. full path `provider::model_full_path`.\n    2. `model_short_name`. Default provider will be used.\n    \"\"\"\n    provider, model = self._match_provider_model(model)\n\n    if temperature is None:\n        temperature = DEFAULT_MODEL_TEMPERATURE\n\n    if max_tokens is None:\n        max_tokens = DEFAULT_MODEL_MAX_TOKENS\n\n    # NOTE(#246): this is to disable \"Provider List\" messages.\n    litellm.suppress_debug_info = True\n\n    # lite_llm uses the openai.request_timeout to set the timeout for the request of \"openai\" custom provider\n    openai.timeout = DEFAULT_TIMEOUT\n    openai.max_retries = DEFAULT_MAX_RETRIES\n\n    for i in range(0, self._config.num_inference_retries):\n        try:\n            # Create a dictionary for the arguments\n            completion_args = {\n                \"model\": model,\n                \"messages\": messages,\n                \"stream\": stream,\n                \"custom_llm_provider\": self._config.custom_llm_provider,\n                \"input_cost_per_token\": 0,\n                \"output_cost_per_token\": 0,\n                \"temperature\": temperature,\n                \"max_tokens\": max_tokens,\n                \"base_url\": self._config.base_url,\n                \"api_key\": self._auth,\n                \"timeout\": DEFAULT_TIMEOUT,\n                \"request_timeout\": DEFAULT_TIMEOUT,\n                \"num_retries\": 1,\n            }\n            # Only add provider parameter if base_url is different from default_provider\n            if self._config.base_url != self._config.default_provider:\n                completion_args[\"provider\"] = provider\n            # Add any additional kwargs\n            completion_args.update(kwargs)\n            result: Union[ModelResponse, CustomStreamWrapper] = litellm_completion(**completion_args)\n            break\n        except Exception as e:\n            print(\"Completions exception:\", e)\n            if i == self._config.num_inference_retries - 1:\n                raise ValueError(f\"Bad request: {e}\") from None\n            else:\n                print(\"Retrying...\")\n\n    return result\n</code></pre>"},{"location":"api/#nearai.shared.inference_client.InferenceClient.create_subthread","title":"create_subthread","text":"<pre><code>create_subthread(thread_id: str, messages_to_copy: Optional[List[str]] = None, new_messages: Optional[List[ChatCompletionMessageParam]] = None)\n</code></pre> <p>Create a subthread.</p> Source code in <code>nearai/shared/inference_client.py</code> <pre><code>def create_subthread(\n    self,\n    thread_id: str,\n    messages_to_copy: Optional[List[str]] = None,\n    new_messages: Optional[List[ChatCompletionMessageParam]] = None,\n):\n    \"\"\"Create a subthread.\"\"\"\n    return self.client.post(\n        path=f\"{self._config.base_url}/threads/{thread_id}/subthread\",\n        body={messages_to_copy: messages_to_copy, new_messages: new_messages},\n        cast_to=Thread,\n    )\n</code></pre>"},{"location":"api/#nearai.shared.inference_client.InferenceClient.create_thread","title":"create_thread","text":"<pre><code>create_thread(messages)\n</code></pre> <p>Create a thread.</p> Source code in <code>nearai/shared/inference_client.py</code> <pre><code>def create_thread(self, messages):\n    \"\"\"Create a thread.\"\"\"\n    return self.client.beta.threads.create(messages=messages)\n</code></pre>"},{"location":"api/#nearai.shared.inference_client.InferenceClient.create_vector_store","title":"create_vector_store","text":"<pre><code>create_vector_store(name: str, file_ids: List[str], expires_after: Union[ExpiresAfter, NotGiven] = NOT_GIVEN, chunking_strategy: Union[AutoFileChunkingStrategyParam, StaticFileChunkingStrategyObjectParam, NotGiven] = NOT_GIVEN, metadata: Optional[Dict[str, str]] = None) -&gt; VectorStore\n</code></pre> <p>Creates Vector Store.</p> <p>:param name: Vector store name. :param file_ids: Files to be added to the vector store. :param expires_after: Expiration policy. :param chunking_strategy: Chunking strategy. :param metadata: Additional metadata. :return: Returns the created vector store or error.</p> Source code in <code>nearai/shared/inference_client.py</code> <pre><code>def create_vector_store(\n    self,\n    name: str,\n    file_ids: List[str],\n    expires_after: Union[ExpiresAfter, NotGiven] = NOT_GIVEN,\n    chunking_strategy: Union[\n        AutoFileChunkingStrategyParam, StaticFileChunkingStrategyObjectParam, NotGiven\n    ] = NOT_GIVEN,\n    metadata: Optional[Dict[str, str]] = None,\n) -&gt; VectorStore:\n    \"\"\"Creates Vector Store.\n\n    :param name: Vector store name.\n    :param file_ids: Files to be added to the vector store.\n    :param expires_after: Expiration policy.\n    :param chunking_strategy: Chunking strategy.\n    :param metadata: Additional metadata.\n    :return: Returns the created vector store or error.\n    \"\"\"\n    client = openai.OpenAI(base_url=self._config.base_url, api_key=self._auth)\n    return client.vector_stores.create(\n        file_ids=file_ids,\n        name=name,\n        expires_after=expires_after,\n        chunking_strategy=chunking_strategy,\n        metadata=metadata,\n    )\n</code></pre>"},{"location":"api/#nearai.shared.inference_client.InferenceClient.create_vector_store_from_source","title":"create_vector_store_from_source","text":"<pre><code>create_vector_store_from_source(name: str, source: Union[GitHubSource, GitLabSource], source_auth: Optional[str] = None, chunking_strategy: Optional[ChunkingStrategy] = None, expires_after: Optional[ExpiresAfter] = None, metadata: Optional[Dict[str, str]] = None) -&gt; VectorStore\n</code></pre> <p>Creates a vector store from the given source.</p> <pre><code>name (str): The name of the vector store.\nsource (Union[GitHubSource, GitLabSource]): The source from which to create the vector store.\nsource_auth (Optional[str]): The source authentication token.\nchunking_strategy (Optional[ChunkingStrategy]): The chunking strategy to use.\nexpires_after (Optional[ExpiresAfter]): The expiration policy.\nmetadata (Optional[Dict[str, str]]): Additional metadata.\n</code></pre> <pre><code>VectorStore: The created vector store.\n</code></pre> Source code in <code>nearai/shared/inference_client.py</code> <pre><code>def create_vector_store_from_source(\n    self,\n    name: str,\n    source: Union[GitHubSource, GitLabSource],\n    source_auth: Optional[str] = None,\n    chunking_strategy: Optional[ChunkingStrategy] = None,\n    expires_after: Optional[ExpiresAfter] = None,\n    metadata: Optional[Dict[str, str]] = None,\n) -&gt; VectorStore:\n    \"\"\"Creates a vector store from the given source.\n\n    Args:\n    ----\n        name (str): The name of the vector store.\n        source (Union[GitHubSource, GitLabSource]): The source from which to create the vector store.\n        source_auth (Optional[str]): The source authentication token.\n        chunking_strategy (Optional[ChunkingStrategy]): The chunking strategy to use.\n        expires_after (Optional[ExpiresAfter]): The expiration policy.\n        metadata (Optional[Dict[str, str]]): Additional metadata.\n\n    Returns:\n    -------\n        VectorStore: The created vector store.\n\n    \"\"\"\n    print(f\"Creating vector store from source: {source}\")\n    headers = {\n        \"Authorization\": f\"Bearer {self._auth}\",\n        \"Content-Type\": \"application/json\",\n    }\n    data = {\n        \"name\": name,\n        \"source\": source,\n        \"source_auth\": source_auth,\n        \"chunking_strategy\": chunking_strategy,\n        \"expires_after\": expires_after,\n        \"metadata\": metadata,\n    }\n    endpoint = f\"{self._config.base_url}/vector_stores/from_source\"\n\n    try:\n        response = requests.post(endpoint, headers=headers, json=data)\n        print(response.json())\n        response.raise_for_status()\n        return VectorStore(**response.json())\n    except requests.RequestException as e:\n        raise ValueError(f\"Failed to create vector store: {e}\") from None\n</code></pre>"},{"location":"api/#nearai.shared.inference_client.InferenceClient.find_agents","title":"find_agents","text":"<pre><code>find_agents(owner_id: Optional[str] = None, with_capabilities: Optional[bool] = False, latest_versions_only: Optional[bool] = True, limit: Optional[int] = None, offset: Optional[int] = None)\n</code></pre> <p>Filter agents.</p> Source code in <code>nearai/shared/inference_client.py</code> <pre><code>def find_agents(\n    self,\n    owner_id: Optional[str] = None,\n    with_capabilities: Optional[bool] = False,\n    latest_versions_only: Optional[bool] = True,\n    limit: Optional[int] = None,\n    offset: Optional[int] = None,\n):\n    \"\"\"Filter agents.\"\"\"\n    return self.client.post(\n        path=f\"{self._config.base_url}/find_agents\",\n        body={\n            \"owner_id\": owner_id,\n            \"with_capabilities\": with_capabilities,\n            \"latest_versions_only\": latest_versions_only,\n            \"limit\": limit,\n            \"offset\": offset,\n        },\n        cast_to=List[Any],\n    )\n</code></pre>"},{"location":"api/#nearai.shared.inference_client.InferenceClient.generate_auth_for_current_agent","title":"generate_auth_for_current_agent","text":"<pre><code>generate_auth_for_current_agent(config, agent_identifier)\n</code></pre> <p>Regenerate auth for the current agent.</p> Source code in <code>nearai/shared/inference_client.py</code> <pre><code>def generate_auth_for_current_agent(self, config, agent_identifier):\n    \"\"\"Regenerate auth for the current agent.\"\"\"\n    self.agent_identifier = agent_identifier\n    if config.auth is not None:\n        auth_bearer_token = config.auth.generate_bearer_token()\n        new_token = json.loads(auth_bearer_token)\n        new_token[\"runner_data\"] = json.dumps({\"agent\": agent_identifier, \"runner_api_key\": self.runner_api_key})\n        auth_bearer_token = json.dumps(new_token)\n        self._auth = auth_bearer_token\n    else:\n        self._auth = None\n</code></pre>"},{"location":"api/#nearai.shared.inference_client.InferenceClient.generate_image","title":"generate_image","text":"<pre><code>generate_image(prompt: str, model: Optional[str] = None)\n</code></pre> <p>Generate an image using the specified model or the default if none is provided.</p> Source code in <code>nearai/shared/inference_client.py</code> <pre><code>def generate_image(self, prompt: str, model: Optional[str] = None):\n    \"\"\"Generate an image using the specified model or the default if none is provided.\"\"\"\n    if model:\n        _provider, model = self._match_provider_model(model)\n    return self.client.images.generate(prompt=prompt, model=model)\n</code></pre>"},{"location":"api/#nearai.shared.inference_client.InferenceClient.get_agent_data","title":"get_agent_data","text":"<pre><code>get_agent_data()\n</code></pre> <p>Get agent data for the agent this client was initialized with.</p> Source code in <code>nearai/shared/inference_client.py</code> <pre><code>def get_agent_data(self):\n    \"\"\"Get agent data for the agent this client was initialized with.\"\"\"\n    return self.client.get(\n        path=f\"{self._config.base_url}/agent_data\",\n        cast_to=Dict[str, str],\n    )\n</code></pre>"},{"location":"api/#nearai.shared.inference_client.InferenceClient.get_agent_data_by_key","title":"get_agent_data_by_key","text":"<pre><code>get_agent_data_by_key(key: str)\n</code></pre> <p>Get agent data by key for the agent this client was initialized with.</p> Source code in <code>nearai/shared/inference_client.py</code> <pre><code>def get_agent_data_by_key(self, key: str):\n    \"\"\"Get agent data by key for the agent this client was initialized with.\"\"\"\n    return self.client.get(\n        path=f\"{self._config.base_url}/agent_data/{key}\",\n        cast_to=Dict[str, str],\n    )\n</code></pre>"},{"location":"api/#nearai.shared.inference_client.InferenceClient.get_agent_public_key","title":"get_agent_public_key","text":"<pre><code>get_agent_public_key(agent_name: str) -&gt; str\n</code></pre> <p>Request agent public key.</p> Source code in <code>nearai/shared/inference_client.py</code> <pre><code>def get_agent_public_key(self, agent_name: str) -&gt; str:\n    \"\"\"Request agent public key.\"\"\"\n    headers = {\n        \"Content-Type\": \"application/json\",\n    }\n\n    data = {\"agent_name\": agent_name}\n\n    endpoint = f\"{self._config.base_url}/get_agent_public_key\"\n\n    try:\n        response = requests.post(endpoint, headers=headers, params=data)\n        response.raise_for_status()\n        return response.json()\n    except requests.RequestException as e:\n        raise ValueError(f\"Failed to get agent public key: {e}\") from None\n</code></pre>"},{"location":"api/#nearai.shared.inference_client.InferenceClient.get_thread","title":"get_thread","text":"<pre><code>get_thread(thread_id: str)\n</code></pre> <p>Get a thread.</p> Source code in <code>nearai/shared/inference_client.py</code> <pre><code>def get_thread(self, thread_id: str):\n    \"\"\"Get a thread.\"\"\"\n    return self.client.beta.threads.retrieve(thread_id=thread_id)\n</code></pre>"},{"location":"api/#nearai.shared.inference_client.InferenceClient.get_vector_store","title":"get_vector_store","text":"<pre><code>get_vector_store(vector_store_id: str) -&gt; VectorStore\n</code></pre> <p>Gets a vector store by id.</p> Source code in <code>nearai/shared/inference_client.py</code> <pre><code>def get_vector_store(self, vector_store_id: str) -&gt; VectorStore:\n    \"\"\"Gets a vector store by id.\"\"\"\n    endpoint = f\"{self._config.base_url}/vector_stores/{vector_store_id}\"\n    auth_bearer_token = self._auth\n\n    headers = {\n        \"Content-Type\": \"application/json\",\n        \"Authorization\": f\"Bearer {auth_bearer_token}\",\n    }\n\n    response = requests.get(endpoint, headers=headers)\n    response.raise_for_status()\n    return VectorStore(**response.json())\n</code></pre>"},{"location":"api/#nearai.shared.inference_client.InferenceClient.get_vector_store_files","title":"get_vector_store_files","text":"<pre><code>get_vector_store_files(vector_store_id: str) -&gt; Optional[List[VectorStoreFile]]\n</code></pre> <p>Adds a file to vector store.</p> Source code in <code>nearai/shared/inference_client.py</code> <pre><code>def get_vector_store_files(self, vector_store_id: str) -&gt; Optional[List[VectorStoreFile]]:\n    \"\"\"Adds a file to vector store.\"\"\"\n    client = openai.OpenAI(base_url=self._config.base_url, api_key=self._auth)\n    return client.vector_stores.files.list(vector_store_id=vector_store_id).data\n</code></pre>"},{"location":"api/#nearai.shared.inference_client.InferenceClient.query_user_memory","title":"query_user_memory","text":"<pre><code>query_user_memory(query: str)\n</code></pre> <p>Query the user memory.</p> Source code in <code>nearai/shared/inference_client.py</code> <pre><code>def query_user_memory(self, query: str):\n    \"\"\"Query the user memory.\"\"\"\n    return self.client.post(\n        path=f\"{self._config.base_url}/vector_stores/memory/query\",\n        body={\"query\": query},\n        cast_to=str,\n    )\n</code></pre>"},{"location":"api/#nearai.shared.inference_client.InferenceClient.query_vector_store","title":"query_vector_store","text":"<pre><code>query_vector_store(vector_store_id: str, query: str, full_files: bool = False) -&gt; Union[List[SimilaritySearch], List[SimilaritySearchFile]]\n</code></pre> <p>Query a vector store.</p> Source code in <code>nearai/shared/inference_client.py</code> <pre><code>def query_vector_store(\n    self, vector_store_id: str, query: str, full_files: bool = False\n) -&gt; Union[List[SimilaritySearch], List[SimilaritySearchFile]]:\n    \"\"\"Query a vector store.\"\"\"\n    if self._config is None:\n        raise ValueError(\"Missing NEAR AI Hub config\")\n\n    auth_bearer_token = self._auth\n\n    headers = {\n        \"Content-Type\": \"application/json\",\n        \"Authorization\": f\"Bearer {auth_bearer_token}\",\n    }\n\n    data = {\"query\": query, \"full_files\": full_files}\n\n    endpoint = f\"{self._config.base_url}/vector_stores/{vector_store_id}/search\"\n\n    try:\n        response = requests.post(endpoint, headers=headers, json=data)\n        response.raise_for_status()\n        return response.json()\n    except requests.RequestException as e:\n        raise ValueError(f\"Error querying vector store: {e}\") from None\n</code></pre>"},{"location":"api/#nearai.shared.inference_client.InferenceClient.remove_file","title":"remove_file","text":"<pre><code>remove_file(file_id: str)\n</code></pre> <p>Removes a file.</p> Source code in <code>nearai/shared/inference_client.py</code> <pre><code>def remove_file(self, file_id: str):\n    \"\"\"Removes a file.\"\"\"\n    client = openai.OpenAI(base_url=self._config.base_url, api_key=self._auth)\n    return client.files.delete(file_id=file_id)\n</code></pre>"},{"location":"api/#nearai.shared.inference_client.InferenceClient.run_agent","title":"run_agent","text":"<pre><code>run_agent(run_on_thread_id: str, assistant_id: str, parent_run_id: str = '', run_mode: RunMode = SIMPLE)\n</code></pre> <p>Starts a child agent run from a parent agent run.</p> Source code in <code>nearai/shared/inference_client.py</code> <pre><code>def run_agent(\n    self, run_on_thread_id: str, assistant_id: str, parent_run_id: str = \"\", run_mode: RunMode = RunMode.SIMPLE\n):\n    \"\"\"Starts a child agent run from a parent agent run.\"\"\"\n    extra_body = {}\n    if parent_run_id:\n        extra_body[\"parent_run_id\"] = parent_run_id\n    extra_body[\"run_mode\"] = run_mode.value  # type: ignore\n    return self.client.beta.threads.runs.create(\n        thread_id=run_on_thread_id,\n        assistant_id=assistant_id,\n        extra_body=extra_body,\n    )\n</code></pre>"},{"location":"api/#nearai.shared.inference_client.InferenceClient.save_agent_data","title":"save_agent_data","text":"<pre><code>save_agent_data(key: str, agent_data: Dict[str, Any])\n</code></pre> <p>Save agent data for the agent this client was initialized with.</p> Source code in <code>nearai/shared/inference_client.py</code> <pre><code>def save_agent_data(self, key: str, agent_data: Dict[str, Any]):\n    \"\"\"Save agent data for the agent this client was initialized with.\"\"\"\n    return self.client.post(\n        path=f\"{self._config.base_url}/agent_data\",\n        body={\n            \"key\": key,\n            \"value\": agent_data,\n        },\n        cast_to=Dict[str, Any],\n    )\n</code></pre>"},{"location":"api/#nearai.shared.inference_client.InferenceClient.schedule_run","title":"schedule_run","text":"<pre><code>schedule_run(agent: str, input_message: str, thread_id: Optional[str], run_params: Optional[Dict[str, str]], run_at: datetime)\n</code></pre> <p>Query a vector store.</p> Source code in <code>nearai/shared/inference_client.py</code> <pre><code>def schedule_run(\n    self,\n    agent: str,\n    input_message: str,\n    thread_id: Optional[str],\n    run_params: Optional[Dict[str, str]],\n    run_at: datetime,\n):\n    \"\"\"Query a vector store.\"\"\"\n    if self._config is None:\n        raise ValueError(\"Missing NearAI Hub config\")\n\n    auth_bearer_token = self._auth\n\n    headers = {\n        \"Content-Type\": \"application/json\",\n        \"Authorization\": f\"Bearer {auth_bearer_token}\",\n    }\n\n    if run_params is None:\n        run_params = {}\n\n    data = {\n        \"agent\": agent,\n        \"input_message\": input_message,\n        \"thread_id\": thread_id,\n        \"run_params\": run_params,\n        \"run_at\": run_at,\n    }\n\n    endpoint = f\"{self._config.base_url}/schedule_run\"\n\n    try:\n        response = requests.post(endpoint, headers=headers, json=data)\n        response.raise_for_status()\n        return response.json()\n    except requests.RequestException as e:\n        raise ValueError(f\"Error querying schedule_run: {e}\") from None\n</code></pre>"},{"location":"api/#nearai.shared.inference_client.InferenceClient.set_provider_models","title":"set_provider_models","text":"<pre><code>set_provider_models(provider_models: Optional[ProviderModels])\n</code></pre> <p>Set provider models. Used by external caching.</p> Source code in <code>nearai/shared/inference_client.py</code> <pre><code>def set_provider_models(self, provider_models: Optional[ProviderModels]):\n    \"\"\"Set provider models. Used by external caching.\"\"\"\n    if provider_models is None:\n        self._provider_models = ProviderModels(self._config)\n    else:\n        self._provider_models = provider_models\n</code></pre>"},{"location":"api/#nearai.shared.inference_client.InferenceClient.threads_create_and_run_poll","title":"threads_create_and_run_poll","text":"<pre><code>threads_create_and_run_poll(assistant_id: str, model: str, messages: List[ChatCompletionMessageParam])\n</code></pre> <p>Create a thread and run the assistant.</p> Source code in <code>nearai/shared/inference_client.py</code> <pre><code>def threads_create_and_run_poll(self, assistant_id: str, model: str, messages: List[ChatCompletionMessageParam]):\n    \"\"\"Create a thread and run the assistant.\"\"\"\n    thread = self.create_thread(messages)\n    return self.client.beta.threads.create_and_run_poll(thread=thread, assistant_id=assistant_id, model=model)\n</code></pre>"},{"location":"api/#nearai.shared.inference_client.InferenceClient.threads_fork","title":"threads_fork","text":"<pre><code>threads_fork(thread_id: str)\n</code></pre> <p>Fork a thread.</p> Source code in <code>nearai/shared/inference_client.py</code> <pre><code>def threads_fork(self, thread_id: str):\n    \"\"\"Fork a thread.\"\"\"\n    forked_thread = self.client.post(path=f\"{self._config.base_url}/threads/{thread_id}/fork\", cast_to=Thread)\n    return forked_thread\n</code></pre>"},{"location":"api/#nearai.shared.inference_client.InferenceClient.threads_list_messages","title":"threads_list_messages","text":"<pre><code>threads_list_messages(thread_id: str, order: Literal['asc', 'desc'] = 'asc')\n</code></pre> <p>List messages in a thread.</p> Source code in <code>nearai/shared/inference_client.py</code> <pre><code>def threads_list_messages(self, thread_id: str, order: Literal[\"asc\", \"desc\"] = \"asc\"):\n    \"\"\"List messages in a thread.\"\"\"\n    return self.client.beta.threads.messages.list(thread_id=thread_id, order=order)\n</code></pre>"},{"location":"api/#nearai.shared.inference_client.InferenceClient.threads_messages_create","title":"threads_messages_create","text":"<pre><code>threads_messages_create(thread_id: str, content: str, role: Literal['user', 'assistant'])\n</code></pre> <p>Create a message in a thread.</p> Source code in <code>nearai/shared/inference_client.py</code> <pre><code>def threads_messages_create(self, thread_id: str, content: str, role: Literal[\"user\", \"assistant\"]):\n    \"\"\"Create a message in a thread.\"\"\"\n    return self.client.beta.threads.messages.create(thread_id=thread_id, content=content, role=role)\n</code></pre>"},{"location":"api/#nearai.shared.inference_client.InferenceClient.threads_runs_create","title":"threads_runs_create","text":"<pre><code>threads_runs_create(thread_id: str, assistant_id: str, model: str)\n</code></pre> <p>Create a run in a thread.</p> Source code in <code>nearai/shared/inference_client.py</code> <pre><code>def threads_runs_create(self, thread_id: str, assistant_id: str, model: str):\n    \"\"\"Create a run in a thread.\"\"\"\n    return self.client.beta.threads.runs.create(thread_id=thread_id, assistant_id=assistant_id, model=model)\n</code></pre>"},{"location":"api/#nearai.shared.inference_client.InferenceClient.upload_file","title":"upload_file","text":"<pre><code>upload_file(file_content: str, purpose: Literal['assistants', 'batch', 'fine-tune', 'vision'], encoding: Optional[str] = 'utf-8', file_name: Optional[str] = 'file.txt', file_type: Optional[str] = 'text/plain') -&gt; Optional[FileObject]\n</code></pre> <p>Uploads a file.</p> Source code in <code>nearai/shared/inference_client.py</code> <pre><code>def upload_file(\n    self,\n    file_content: str,\n    purpose: Literal[\"assistants\", \"batch\", \"fine-tune\", \"vision\"],\n    encoding: Optional[str] = \"utf-8\",\n    file_name: Optional[str] = \"file.txt\",\n    file_type: Optional[str] = \"text/plain\",\n) -&gt; Optional[FileObject]:\n    \"\"\"Uploads a file.\"\"\"\n    client = openai.OpenAI(base_url=self._config.base_url, api_key=self._auth)\n    if file_content:\n        file_data = io.BytesIO(file_content.encode(encoding or \"utf-8\"))\n        return client.files.create(file=(file_name, file_data, file_type), purpose=purpose)\n    else:\n        return None\n</code></pre>"},{"location":"api/#nearai.shared.models","title":"models","text":""},{"location":"api/#nearai.shared.models.AutoFileChunkingStrategyParam","title":"AutoFileChunkingStrategyParam","text":"<p>               Bases: <code>TypedDict</code></p> Source code in <code>nearai/shared/models.py</code> <pre><code>class AutoFileChunkingStrategyParam(TypedDict, total=False):\n    type: Required[Literal[\"auto\"]]\n    \"\"\"Always `auto`.\"\"\"\n</code></pre>"},{"location":"api/#nearai.shared.models.AutoFileChunkingStrategyParam.type","title":"type  <code>instance-attribute</code>","text":"<pre><code>type: Required[Literal['auto']]\n</code></pre> <p>Always <code>auto</code>.</p>"},{"location":"api/#nearai.shared.models.ChunkingStrategy","title":"ChunkingStrategy","text":"<p>               Bases: <code>BaseModel</code></p> <p>Defines the chunking strategy for vector stores.</p> Source code in <code>nearai/shared/models.py</code> <pre><code>class ChunkingStrategy(BaseModel):\n    \"\"\"Defines the chunking strategy for vector stores.\"\"\"\n\n    pass\n</code></pre>"},{"location":"api/#nearai.shared.models.CreateVectorStoreRequest","title":"CreateVectorStoreRequest","text":"<p>               Bases: <code>BaseModel</code></p> <p>Request model for creating a new vector store.</p> Source code in <code>nearai/shared/models.py</code> <pre><code>class CreateVectorStoreRequest(BaseModel):\n    \"\"\"Request model for creating a new vector store.\"\"\"\n\n    chunking_strategy: Union[AutoFileChunkingStrategyParam, StaticFileChunkingStrategyParam, None] = None\n    \"\"\"The chunking strategy to use for the vector store.\"\"\"\n    expires_after: Optional[ExpiresAfter] = None\n    \"\"\"The expiration time for the vector store.\"\"\"\n    file_ids: Optional[List[str]] = None\n    \"\"\"The file IDs to attach to the vector store.\"\"\"\n    metadata: Optional[Dict[str, str]] = None\n    \"\"\"The metadata to attach to the vector store.\"\"\"\n    name: str\n    \"\"\"The name of the vector store.\"\"\"\n</code></pre>"},{"location":"api/#nearai.shared.models.CreateVectorStoreRequest.chunking_strategy","title":"chunking_strategy  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>chunking_strategy: Union[AutoFileChunkingStrategyParam, StaticFileChunkingStrategyParam, None] = None\n</code></pre> <p>The chunking strategy to use for the vector store.</p>"},{"location":"api/#nearai.shared.models.CreateVectorStoreRequest.expires_after","title":"expires_after  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>expires_after: Optional[ExpiresAfter] = None\n</code></pre> <p>The expiration time for the vector store.</p>"},{"location":"api/#nearai.shared.models.CreateVectorStoreRequest.file_ids","title":"file_ids  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>file_ids: Optional[List[str]] = None\n</code></pre> <p>The file IDs to attach to the vector store.</p>"},{"location":"api/#nearai.shared.models.CreateVectorStoreRequest.metadata","title":"metadata  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>metadata: Optional[Dict[str, str]] = None\n</code></pre> <p>The metadata to attach to the vector store.</p>"},{"location":"api/#nearai.shared.models.CreateVectorStoreRequest.name","title":"name  <code>instance-attribute</code>","text":"<pre><code>name: str\n</code></pre> <p>The name of the vector store.</p>"},{"location":"api/#nearai.shared.models.ExpiresAfter","title":"ExpiresAfter","text":"<p>               Bases: <code>TypedDict</code></p> Source code in <code>nearai/shared/models.py</code> <pre><code>class ExpiresAfter(TypedDict, total=False):\n    anchor: Required[Literal[\"last_active_at\"]]\n    \"\"\"Anchor timestamp after which the expiration policy applies.\n\n    Supported anchors: `last_active_at`.\n    \"\"\"\n\n    days: Required[int]\n    \"\"\"The number of days after the anchor time that the vector store will expire.\"\"\"\n</code></pre>"},{"location":"api/#nearai.shared.models.ExpiresAfter.anchor","title":"anchor  <code>instance-attribute</code>","text":"<pre><code>anchor: Required[Literal['last_active_at']]\n</code></pre> <p>Anchor timestamp after which the expiration policy applies.</p> <p>Supported anchors: <code>last_active_at</code>.</p>"},{"location":"api/#nearai.shared.models.ExpiresAfter.days","title":"days  <code>instance-attribute</code>","text":"<pre><code>days: Required[int]\n</code></pre> <p>The number of days after the anchor time that the vector store will expire.</p>"},{"location":"api/#nearai.shared.models.StaticFileChunkingStrategyParam","title":"StaticFileChunkingStrategyParam","text":"<p>               Bases: <code>TypedDict</code></p> Source code in <code>nearai/shared/models.py</code> <pre><code>class StaticFileChunkingStrategyParam(TypedDict, total=False):\n    chunk_overlap_tokens: Required[int]\n    \"\"\"The number of tokens that overlap between chunks. The default value is `400`.\n\n    Note that the overlap must not exceed half of `max_chunk_size_tokens`.\n    \"\"\"\n\n    max_chunk_size_tokens: Required[int]\n    \"\"\"The maximum number of tokens in each chunk.\n\n    The default value is `800`. The minimum value is `100` and the maximum value is\n    `4096`.\n    \"\"\"\n</code></pre>"},{"location":"api/#nearai.shared.models.StaticFileChunkingStrategyParam.chunk_overlap_tokens","title":"chunk_overlap_tokens  <code>instance-attribute</code>","text":"<pre><code>chunk_overlap_tokens: Required[int]\n</code></pre> <p>The number of tokens that overlap between chunks. The default value is <code>400</code>.</p> <p>Note that the overlap must not exceed half of <code>max_chunk_size_tokens</code>.</p>"},{"location":"api/#nearai.shared.models.StaticFileChunkingStrategyParam.max_chunk_size_tokens","title":"max_chunk_size_tokens  <code>instance-attribute</code>","text":"<pre><code>max_chunk_size_tokens: Required[int]\n</code></pre> <p>The maximum number of tokens in each chunk.</p> <p>The default value is <code>800</code>. The minimum value is <code>100</code> and the maximum value is <code>4096</code>.</p>"},{"location":"api/#nearai.shared.models.VectorStoreFileCreate","title":"VectorStoreFileCreate","text":"<p>               Bases: <code>BaseModel</code></p> <p>Request model for creating a vector store file.</p> Source code in <code>nearai/shared/models.py</code> <pre><code>class VectorStoreFileCreate(BaseModel):\n    \"\"\"Request model for creating a vector store file.\"\"\"\n\n    file_id: str\n    \"\"\"File ID returned from upload file endpoint.\"\"\"\n</code></pre>"},{"location":"api/#nearai.shared.models.VectorStoreFileCreate.file_id","title":"file_id  <code>instance-attribute</code>","text":"<pre><code>file_id: str\n</code></pre> <p>File ID returned from upload file endpoint.</p>"},{"location":"api/#nearai.shared.naming","title":"naming","text":""},{"location":"api/#nearai.shared.naming.NamespacedName","title":"NamespacedName","text":"Source code in <code>nearai/shared/naming.py</code> <pre><code>class NamespacedName:\n    def __init__(self, name: str, namespace: str = \"\"):  # noqa: D107\n        self.name = name\n        self.namespace = namespace\n\n    def __eq__(self, other):  # noqa: D105\n        if not isinstance(other, NamespacedName):\n            return NotImplemented\n        return self.name == other.name and self.namespace == other.namespace\n\n    def __hash__(self):  # noqa: D105\n        return hash((self.name, self.namespace))\n\n    def __str__(self):  # noqa: D105\n        if self.namespace:\n            return f\"{self.namespace}/{self.name}\"\n        return self.name\n\n    def __repr__(self):  # noqa: D105\n        return f\"NamespacedName(name='{self.name}', namespace='{self.namespace}')\"\n\n    def canonical(self) -&gt; \"NamespacedName\":  # noqa: D105\n        \"\"\"Returns canonical NamespacedName.\"\"\"\n        return NamespacedName(\n            name=get_canonical_name(self.name),\n            namespace=get_canonical_name(self.namespace) if self.namespace != DEFAULT_NAMESPACE else \"\",\n        )\n</code></pre>"},{"location":"api/#nearai.shared.naming.NamespacedName.canonical","title":"canonical","text":"<pre><code>canonical() -&gt; NamespacedName\n</code></pre> <p>Returns canonical NamespacedName.</p> Source code in <code>nearai/shared/naming.py</code> <pre><code>def canonical(self) -&gt; \"NamespacedName\":  # noqa: D105\n    \"\"\"Returns canonical NamespacedName.\"\"\"\n    return NamespacedName(\n        name=get_canonical_name(self.name),\n        namespace=get_canonical_name(self.namespace) if self.namespace != DEFAULT_NAMESPACE else \"\",\n    )\n</code></pre>"},{"location":"api/#nearai.shared.naming.create_registry_name","title":"create_registry_name","text":"<pre><code>create_registry_name(name: str) -&gt; str\n</code></pre> <p>Formats <code>name</code> for a suitable registry name.</p> Source code in <code>nearai/shared/naming.py</code> <pre><code>def create_registry_name(name: str) -&gt; str:\n    \"\"\"Formats `name` for a suitable registry name.\"\"\"\n    # Convert to lowercase\n    name = name.lower()\n    # Convert '.' between digits to 'p'\n    name = re.sub(r\"(\\d)\\.(\\d)\", r\"\\1p\\2\", name)\n    # Convert '&lt;digit&gt;v&lt;digit&gt;' -&gt; '&lt;digit&gt;-&lt;digit&gt;'\n    name = re.sub(r\"(\\d)v(\\d)\", r\"\\1-\\2\", name)\n    # Convert '&lt;not letter&gt;v&lt;digit&gt;' -&gt; '&lt;not letter&gt;&lt;digit&gt;'\n    name = re.sub(r\"(^|[^a-z])v(\\d)\", r\"\\1\\2\", name)\n    # Replace non-alphanumeric characters between digits with '-'\n    name = re.sub(r\"(\\d)[^a-z0-9]+(\\d)\", r\"\\1-\\2\", name)\n    # Remove remaining non-alphanumeric characters, except '-'\n    name = re.sub(r\"[^a-z0-9-]\", \"\", name)\n    # Convert 'metallama' or 'meta-llama' to 'llama'\n    name = name.replace(\"metallama\", \"llama\")\n    name = name.replace(\"meta-llama\", \"llama\")\n    # Convert 'qwenq' or 'qwen-q' to 'q'\n    name = name.replace(\"qwenq\", \"q\")\n    name = name.replace(\"qwen-q\", \"q\")\n    return name\n</code></pre>"},{"location":"api/#nearai.shared.naming.get_canonical_name","title":"get_canonical_name","text":"<pre><code>get_canonical_name(name: str) -&gt; str\n</code></pre> <p>Returns a name that can be used for matching entities.</p> <p>Applies such transformations: 1. All letters lowercase. 2. Strips '.near' extensions. 3. Convert '.' between digits to 'p'. 4. Convert 'v' -&gt; '' 5. Remove all non-alphanumeric characters except between digits.     Use '_' between digits. 6. Convert 'metallama' -&gt; 'llama'. <p>e.g. \"llama-3.1-70b-instruct\" -&gt; \"llama3p1_70binstruct\"</p> Source code in <code>nearai/shared/naming.py</code> <pre><code>def get_canonical_name(name: str) -&gt; str:\n    \"\"\"Returns a name that can be used for matching entities.\n\n    Applies such transformations:\n    1. All letters lowercase.\n    2. Strips '.near' extensions.\n    3. Convert '.' between digits to 'p'.\n    4. Convert '&lt;not letter&gt;v&lt;digit&gt;' -&gt; '&lt;not letter&gt;&lt;digit&gt;'\n    5. Remove all non-alphanumeric characters except between digits.\n        Use '_' between digits.\n    6. Convert 'metallama' -&gt; 'llama'.\n\n    e.g. \"llama-3.1-70b-instruct\" -&gt; \"llama3p1_70binstruct\"\n    \"\"\"\n    # Convert to lowercase\n    name = name.lower()\n    # Strip .near extension if present\n    if name.endswith(\".near\"):\n        name = name[:-5]  # Remove last 5 characters ('.near')\n    # Convert '.' between digits to 'p'\n    name = re.sub(r\"(\\d)\\.(\\d)\", r\"\\1p\\2\", name)\n    # Convert '&lt;digit&gt;v&lt;digit&gt;' -&gt; '&lt;digit&gt;_&lt;digit&gt;'\n    name = re.sub(r\"(\\d)v(\\d)\", r\"\\1_\\2\", name)\n    # Convert '&lt;not letter&gt;v&lt;digit&gt;' -&gt; '&lt;not letter&gt;&lt;digit&gt;'\n    name = re.sub(r\"(^|[^a-z])v(\\d)\", r\"\\1\\2\", name)\n    # Replace non-alphanumeric characters between digits with '_'\n    name = re.sub(r\"(\\d)[^a-z0-9]+(\\d)\", r\"\\1_\\2\", name)\n    # Remove remaining non-alphanumeric characters, except '_'\n    name = re.sub(r\"[^a-z0-9_]\", \"\", name)\n    # Remove any remaining underscores that are not between digits\n    name = re.sub(r\"(?&lt;!\\d)_|_(?!\\d)\", \"\", name)\n    # Convert 'metallama' to 'llama'\n    name = name.replace(\"metallama\", \"llama\")\n    # Convert 'qwenq' to 'q'\n    name = name.replace(\"qwenq\", \"q\")\n    return name\n</code></pre>"},{"location":"api/#nearai.shared.near","title":"near","text":""},{"location":"api/#nearai.shared.near.sign","title":"sign","text":""},{"location":"api/#nearai.shared.near.sign.SignatureVerificationResult","title":"SignatureVerificationResult","text":"<p>               Bases: <code>Enum</code></p> Source code in <code>nearai/shared/near/sign.py</code> <pre><code>class SignatureVerificationResult(Enum):\n    TRUE = True\n    FALSE = False\n    VERIFY_ACCESS_KEY_OWNER_SERVICE_NOT_AVAILABLE = \"verify_access_key_owner_not_available\"\n\n    @classmethod\n    def from_bool(cls, value: bool):\n        \"\"\"Gets VerificationResult based on a boolean value.\"\"\"\n        return cls.TRUE if value else cls.FALSE\n\n    def __bool__(self):\n        \"\"\"Overrides the behavior when checking for truthiness.\"\"\"\n        return self == SignatureVerificationResult.TRUE\n</code></pre> __bool__ \u00b6 <pre><code>__bool__()\n</code></pre> <p>Overrides the behavior when checking for truthiness.</p> Source code in <code>nearai/shared/near/sign.py</code> <pre><code>def __bool__(self):\n    \"\"\"Overrides the behavior when checking for truthiness.\"\"\"\n    return self == SignatureVerificationResult.TRUE\n</code></pre> from_bool <code>classmethod</code> \u00b6 <pre><code>from_bool(value: bool)\n</code></pre> <p>Gets VerificationResult based on a boolean value.</p> Source code in <code>nearai/shared/near/sign.py</code> <pre><code>@classmethod\ndef from_bool(cls, value: bool):\n    \"\"\"Gets VerificationResult based on a boolean value.\"\"\"\n    return cls.TRUE if value else cls.FALSE\n</code></pre>"},{"location":"api/#nearai.shared.near.sign.convert_nonce","title":"convert_nonce","text":"<pre><code>convert_nonce(value: Union[str, bytes, list[int]])\n</code></pre> <p>Converts a given value to a 32-byte nonce.</p> Source code in <code>nearai/shared/near/sign.py</code> <pre><code>def convert_nonce(value: Union[str, bytes, list[int]]):\n    \"\"\"Converts a given value to a 32-byte nonce.\"\"\"\n    if isinstance(value, bytes):\n        if len(value) &gt; 32:\n            raise ValueError(\"Invalid nonce length\")\n        if len(value) &lt; 32:\n            value = value.rjust(32, b\"0\")\n        return value\n    elif isinstance(value, str):\n        nonce_bytes = value.encode(\"utf-8\")\n        if len(nonce_bytes) &gt; 32:\n            raise ValueError(\"Invalid nonce length\")\n        if len(nonce_bytes) &lt; 32:\n            nonce_bytes = nonce_bytes.rjust(32, b\"0\")\n        return nonce_bytes\n    elif isinstance(value, list):\n        if len(value) != 32:\n            raise ValueError(\"Invalid nonce length\")\n        return bytes(value)\n    else:\n        raise ValueError(\"Invalid nonce format\")\n</code></pre>"},{"location":"api/#nearai.shared.near.sign.create_inference_signature","title":"create_inference_signature","text":"<pre><code>create_inference_signature(private_key: str, payload: CompletionSignaturePayload) -&gt; tuple[str, str]\n</code></pre> <p>Creates a cryptographic signature for a given extended inference payload using a specified private key.</p> Source code in <code>nearai/shared/near/sign.py</code> <pre><code>def create_inference_signature(private_key: str, payload: CompletionSignaturePayload) -&gt; tuple[str, str]:\n    \"\"\"Creates a cryptographic signature for a given extended inference payload using a specified private key.\"\"\"\n    borsh_payload = BinarySerializer(dict(COMPLETION_PAYLOAD_SCHEMA)).serialize(payload)\n\n    to_sign = hashlib.sha256(borsh_payload).digest()\n\n    private_key_base58 = private_key[len(ED_PREFIX) :]\n    private_key_bytes = base58.b58decode(private_key_base58)\n\n    if len(private_key_bytes) != 64:\n        raise ValueError(\"The private key must be exactly 64 bytes long\")\n\n    private_key_seed = private_key_bytes[:32]\n\n    signing_key = nacl.signing.SigningKey(private_key_seed)\n    public_key = signing_key.verify_key\n\n    signed = signing_key.sign(to_sign)\n    signature = base64.b64encode(signed.signature).decode(\"utf-8\")\n\n    public_key_base58 = base58.b58encode(public_key.encode()).decode(\"utf-8\")\n    full_public_key = ED_PREFIX + public_key_base58\n\n    return signature, full_public_key\n</code></pre>"},{"location":"api/#nearai.shared.near.sign.create_signature","title":"create_signature","text":"<pre><code>create_signature(private_key: str, payload: Payload) -&gt; tuple[str, str]\n</code></pre> <p>Creates a cryptographic signature for a given payload using a specified private key.</p> Source code in <code>nearai/shared/near/sign.py</code> <pre><code>def create_signature(private_key: str, payload: Payload) -&gt; tuple[str, str]:\n    \"\"\"Creates a cryptographic signature for a given payload using a specified private key.\"\"\"\n    borsh_payload = BinarySerializer(dict(PAYLOAD_SCHEMA)).serialize(payload)\n\n    to_sign = hashlib.sha256(borsh_payload).digest()\n\n    # Extract and decode the private key\n    private_key_base58 = private_key[len(ED_PREFIX) :]\n    private_key_bytes = base58.b58decode(private_key_base58)\n\n    if len(private_key_bytes) != 64:\n        raise ValueError(\"The private key must be exactly 64 bytes long\")\n\n    # Use only the first 32 bytes as the seed\n    private_key_seed = private_key_bytes[:32]\n\n    signing_key = nacl.signing.SigningKey(private_key_seed)\n    public_key = signing_key.verify_key\n\n    signed = signing_key.sign(to_sign)\n    signature = base64.b64encode(signed.signature).decode(\"utf-8\")\n\n    public_key_base58 = base58.b58encode(public_key.encode()).decode(\"utf-8\")\n    full_public_key = ED_PREFIX + public_key_base58\n\n    return signature, full_public_key\n</code></pre>"},{"location":"api/#nearai.shared.near.sign.validate_completion_signature","title":"validate_completion_signature","text":"<pre><code>validate_completion_signature(public_key: str, signature: str, payload: CompletionSignaturePayload)\n</code></pre> <p>Validates a cryptographic signature for a given payload using a specified public key.</p> Source code in <code>nearai/shared/near/sign.py</code> <pre><code>def validate_completion_signature(public_key: str, signature: str, payload: CompletionSignaturePayload):\n    \"\"\"Validates a cryptographic signature for a given payload using a specified public key.\"\"\"\n    borsh_payload = BinarySerializer(dict(COMPLETION_PAYLOAD_SCHEMA)).serialize(payload)\n    to_sign = hashlib.sha256(borsh_payload).digest()\n    real_signature = base64.b64decode(signature)\n\n    verify_key: nacl.signing.VerifyKey = nacl.signing.VerifyKey(base58.b58decode(public_key[len(ED_PREFIX) :]))\n\n    try:\n        verify_key.verify(to_sign, real_signature)\n        return True\n    except nacl.exceptions.BadSignatureError:\n        return False\n</code></pre>"},{"location":"api/#nearai.shared.near.sign.validate_nonce","title":"validate_nonce","text":"<pre><code>validate_nonce(value: Union[str, bytes, list[int]])\n</code></pre> <p>Ensures that the nonce is a valid timestamp.</p> Source code in <code>nearai/shared/near/sign.py</code> <pre><code>def validate_nonce(value: Union[str, bytes, list[int]]):\n    \"\"\"Ensures that the nonce is a valid timestamp.\"\"\"\n    nonce = convert_nonce(value)\n    nonce_int = int(nonce.decode(\"utf-8\"))\n\n    now = int(time.time() * 1000)\n\n    if nonce_int &gt; now:\n        raise ValueError(\"Nonce is in the future\")\n    if now - nonce_int &gt; 10 * 365 * 24 * 60 * 60 * 1000:\n        \"\"\"If the timestamp is older than 10 years, it is considered invalid. Forcing apps to use unique nonces.\"\"\"\n        raise ValueError(\"Nonce is too old\")\n\n    return nonce\n</code></pre>"},{"location":"api/#nearai.shared.near.sign.validate_signature","title":"validate_signature","text":"<pre><code>validate_signature(public_key: str, signature: str, payload: Payload)\n</code></pre> <p>Validates a cryptographic signature for a given payload using a specified public key.</p> Source code in <code>nearai/shared/near/sign.py</code> <pre><code>def validate_signature(public_key: str, signature: str, payload: Payload):\n    \"\"\"Validates a cryptographic signature for a given payload using a specified public key.\"\"\"\n    borsh_payload = BinarySerializer(dict(PAYLOAD_SCHEMA)).serialize(payload)\n    to_sign = hashlib.sha256(borsh_payload).digest()\n    real_signature = base64.b64decode(signature)\n\n    verify_key: nacl.signing.VerifyKey = nacl.signing.VerifyKey(base58.b58decode(public_key[len(ED_PREFIX) :]))\n\n    try:\n        verify_key.verify(to_sign, real_signature)\n        # print(\"Signature is valid.\")\n        return True\n    except nacl.exceptions.BadSignatureError:\n        # print(\"Signature was forged or corrupt.\")\n        return False\n</code></pre>"},{"location":"api/#nearai.shared.near.sign.verify_access_key_owner","title":"verify_access_key_owner","text":"<pre><code>verify_access_key_owner(public_key, account_id) -&gt; SignatureVerificationResult\n</code></pre> <p>Verifies if a given public key belongs to a specified account ID using FastNEAR API.</p> Source code in <code>nearai/shared/near/sign.py</code> <pre><code>@mem_cache_with_timeout(300)\ndef verify_access_key_owner(public_key, account_id) -&gt; SignatureVerificationResult:\n    \"\"\"Verifies if a given public key belongs to a specified account ID using FastNEAR API.\"\"\"\n    try:\n        logger.info(f\"Verifying access key owner for public key: {public_key}, account_id: {account_id}\")\n        url = f\"https://api.fastnear.com/v0/public_key/{public_key}\"\n        response = requests.get(url)\n        response.raise_for_status()\n        content = response.json()\n        account_ids = content.get(\"account_ids\", [])\n        key_owner_verified = account_id in account_ids\n        if not key_owner_verified:\n            logger.info(\"Key's owner verification failed. Only NEAR Mainnet accounts are supported.\")\n        return SignatureVerificationResult.from_bool(key_owner_verified)\n    except requests.exceptions.HTTPError as http_err:\n        logger.error(f\"HTTP error occurred: {http_err}\")\n    except Exception as err:\n        logger.error(f\"Other error occurred: {err}\")\n\n    return SignatureVerificationResult.VERIFY_ACCESS_KEY_OWNER_SERVICE_NOT_AVAILABLE\n</code></pre>"},{"location":"api/#nearai.shared.near.sign.verify_signed_message","title":"verify_signed_message","text":"<pre><code>verify_signed_message(account_id, public_key, signature, message, nonce, recipient, callback_url) -&gt; SignatureVerificationResult\n</code></pre> <p>Verifies a signed message and ensures the public key belongs to the specified account.</p> Source code in <code>nearai/shared/near/sign.py</code> <pre><code>def verify_signed_message(\n    account_id, public_key, signature, message, nonce, recipient, callback_url\n) -&gt; SignatureVerificationResult:\n    \"\"\"Verifies a signed message and ensures the public key belongs to the specified account.\"\"\"\n    is_valid = validate_signature(public_key, signature, Payload(message, nonce, recipient, callback_url))\n\n    if not is_valid and callback_url is not None:\n        is_valid = validate_signature(public_key, signature, Payload(message, nonce, recipient, None))\n\n    if is_valid:\n        # verify that key belongs to `account_id`\n        return verify_access_key_owner(public_key, account_id)\n\n    # TODO verifies that key is a FULL ACCESS KEY\n\n    return SignatureVerificationResult.FALSE\n</code></pre>"},{"location":"api/#nearai.shared.provider_models","title":"provider_models","text":""},{"location":"api/#nearai.shared.provider_models.ProviderModels","title":"ProviderModels","text":"Source code in <code>nearai/shared/provider_models.py</code> <pre><code>class ProviderModels:\n    def __init__(self, config: ClientConfig) -&gt; None:  # noqa: D107\n        self._config = config\n\n    @cached_property\n    def provider_models(self) -&gt; Dict[NamespacedName, Dict[str, str]]:\n        \"\"\"Returns a mapping canonical-&gt;provider-&gt;model_full_name.\"\"\"\n        client = self._config.get_hub_client()\n\n        try:\n            models = client.models.list()\n\n            assert len(models.data) &gt; 0, \"No models found\"\n            result: Dict[NamespacedName, Dict[str, str]] = {}\n            for model in models.data:\n                provider, namespaced_model = get_provider_namespaced_model(model.id)\n                namespaced_model = namespaced_model.canonical()\n                if namespaced_model not in result:\n                    result[namespaced_model] = {}\n                if provider in result[namespaced_model]:\n                    raise ValueError(f\"Duplicate entry for provider {provider} and model {namespaced_model}\")\n                result[namespaced_model][provider] = model.id\n\n            return result\n\n        except requests.RequestException as e:\n            raise RuntimeError(f\"Error fetching models: {str(e)}\") from e\n\n    def available_provider_matches(self, model: NamespacedName) -&gt; Dict[str, str]:\n        \"\"\"Returns provider matches for `model`.\"\"\"\n        return self.provider_models.get(model.canonical(), {})\n\n    def match_provider_model(self, model: str, provider: Optional[str] = None) -&gt; Tuple[str, str]:\n        \"\"\"Returns provider and model_full_path for given `model` and optional `provider`.\n\n        `model` may take different formats. Supported ones:\n        1. model_full_path, e.g. \"fireworks::accounts/yi-01-ai/models/yi-large\"\n        2. model_full_path without provider, e.g. \"accounts/yi-01-ai/models/yi-large\"\n        3. model_short_name as used by provider, e.g. \"llama-v3-70b-instruct\"\n        4. namespace/model_short_name as used by provider, e.g. \"yi-01-ai/yi-large\"\n        5. model_name as used in registry, e.g. \"llama-3-70b-instruct\"\n        6. namespace/model_name as used in registry, e.g. \"near.ai/llama-3-70b-instruct\"\n        7. provider base url/model-name, e.g. \"https://api.openai.com/v1::gpt-4o\"\n        \"\"\"\n        if provider == \"\":\n            provider = None\n        matched_provider, namespaced_model = get_provider_namespaced_model(model, provider)\n        if matched_provider.startswith(\"https://\"):\n            return matched_provider, namespaced_model.name\n        namespaced_model = namespaced_model.canonical()\n        if namespaced_model not in self.provider_models:\n            raise ValueError(f\"Model {namespaced_model} not present in provider models {self.provider_models}\")\n        available_matches = self.provider_models[namespaced_model]\n        if matched_provider not in available_matches:\n            for match in available_matches.keys():\n                matched_provider = match\n                break\n        if provider and provider != matched_provider:\n            raise ValueError(\n                f\"Requested provider {provider} for model {model} does not match matched_provider {matched_provider}\"\n            )\n        return matched_provider, available_matches[matched_provider]\n\n    def get_unregistered_common_provider_models(\n        self, registry_models: Dict[NamespacedName, NamespacedName]\n    ) -&gt; List[Dict[str, str]]:\n        \"\"\"Returns provider matches for unregistered provider models with default namespace.\"\"\"\n        result: List[Dict[str, str]] = []\n        for namespaced_name, available_matches in self.provider_models.items():\n            if namespaced_name.namespace != \"\" or namespaced_name in registry_models:\n                continue\n            result.append(available_matches)\n        return result\n</code></pre>"},{"location":"api/#nearai.shared.provider_models.ProviderModels.provider_models","title":"provider_models  <code>cached</code> <code>property</code>","text":"<pre><code>provider_models: Dict[NamespacedName, Dict[str, str]]\n</code></pre> <p>Returns a mapping canonical-&gt;provider-&gt;model_full_name.</p>"},{"location":"api/#nearai.shared.provider_models.ProviderModels.available_provider_matches","title":"available_provider_matches","text":"<pre><code>available_provider_matches(model: NamespacedName) -&gt; Dict[str, str]\n</code></pre> <p>Returns provider matches for <code>model</code>.</p> Source code in <code>nearai/shared/provider_models.py</code> <pre><code>def available_provider_matches(self, model: NamespacedName) -&gt; Dict[str, str]:\n    \"\"\"Returns provider matches for `model`.\"\"\"\n    return self.provider_models.get(model.canonical(), {})\n</code></pre>"},{"location":"api/#nearai.shared.provider_models.ProviderModels.get_unregistered_common_provider_models","title":"get_unregistered_common_provider_models","text":"<pre><code>get_unregistered_common_provider_models(registry_models: Dict[NamespacedName, NamespacedName]) -&gt; List[Dict[str, str]]\n</code></pre> <p>Returns provider matches for unregistered provider models with default namespace.</p> Source code in <code>nearai/shared/provider_models.py</code> <pre><code>def get_unregistered_common_provider_models(\n    self, registry_models: Dict[NamespacedName, NamespacedName]\n) -&gt; List[Dict[str, str]]:\n    \"\"\"Returns provider matches for unregistered provider models with default namespace.\"\"\"\n    result: List[Dict[str, str]] = []\n    for namespaced_name, available_matches in self.provider_models.items():\n        if namespaced_name.namespace != \"\" or namespaced_name in registry_models:\n            continue\n        result.append(available_matches)\n    return result\n</code></pre>"},{"location":"api/#nearai.shared.provider_models.ProviderModels.match_provider_model","title":"match_provider_model","text":"<pre><code>match_provider_model(model: str, provider: Optional[str] = None) -&gt; Tuple[str, str]\n</code></pre> <p>Returns provider and model_full_path for given <code>model</code> and optional <code>provider</code>.</p> <p><code>model</code> may take different formats. Supported ones: 1. model_full_path, e.g. \"fireworks::accounts/yi-01-ai/models/yi-large\" 2. model_full_path without provider, e.g. \"accounts/yi-01-ai/models/yi-large\" 3. model_short_name as used by provider, e.g. \"llama-v3-70b-instruct\" 4. namespace/model_short_name as used by provider, e.g. \"yi-01-ai/yi-large\" 5. model_name as used in registry, e.g. \"llama-3-70b-instruct\" 6. namespace/model_name as used in registry, e.g. \"near.ai/llama-3-70b-instruct\" 7. provider base url/model-name, e.g. \"https://api.openai.com/v1::gpt-4o\"</p> Source code in <code>nearai/shared/provider_models.py</code> <pre><code>def match_provider_model(self, model: str, provider: Optional[str] = None) -&gt; Tuple[str, str]:\n    \"\"\"Returns provider and model_full_path for given `model` and optional `provider`.\n\n    `model` may take different formats. Supported ones:\n    1. model_full_path, e.g. \"fireworks::accounts/yi-01-ai/models/yi-large\"\n    2. model_full_path without provider, e.g. \"accounts/yi-01-ai/models/yi-large\"\n    3. model_short_name as used by provider, e.g. \"llama-v3-70b-instruct\"\n    4. namespace/model_short_name as used by provider, e.g. \"yi-01-ai/yi-large\"\n    5. model_name as used in registry, e.g. \"llama-3-70b-instruct\"\n    6. namespace/model_name as used in registry, e.g. \"near.ai/llama-3-70b-instruct\"\n    7. provider base url/model-name, e.g. \"https://api.openai.com/v1::gpt-4o\"\n    \"\"\"\n    if provider == \"\":\n        provider = None\n    matched_provider, namespaced_model = get_provider_namespaced_model(model, provider)\n    if matched_provider.startswith(\"https://\"):\n        return matched_provider, namespaced_model.name\n    namespaced_model = namespaced_model.canonical()\n    if namespaced_model not in self.provider_models:\n        raise ValueError(f\"Model {namespaced_model} not present in provider models {self.provider_models}\")\n    available_matches = self.provider_models[namespaced_model]\n    if matched_provider not in available_matches:\n        for match in available_matches.keys():\n            matched_provider = match\n            break\n    if provider and provider != matched_provider:\n        raise ValueError(\n            f\"Requested provider {provider} for model {model} does not match matched_provider {matched_provider}\"\n        )\n    return matched_provider, available_matches[matched_provider]\n</code></pre>"},{"location":"api/#nearai.shared.provider_models.fetch_models_from_provider","title":"fetch_models_from_provider","text":"<pre><code>fetch_models_from_provider(provider: str) -&gt; List[Dict]\n</code></pre> <p>Fetch models from a specific provider.</p> <p>Parameters:</p> Name Type Description Default <code>provider</code> <code>str</code> <p>The provider name (\"nearai\", \"fireworks\", \"openai\", \"anthropic\", \"crynux\")</p> required <p>Returns:</p> Type Description <code>List[Dict]</code> <p>List of model dictionaries</p> <p>The user is responsible for setting the appropriate API key in environment variables: - FIREWORKS_API_KEY for Fireworks - OPENAI_API_KEY for OpenAI - ANTHROPIC_API_KEY for Anthropic</p> Source code in <code>nearai/shared/provider_models.py</code> <pre><code>def fetch_models_from_provider(provider: str) -&gt; List[Dict]:\n    \"\"\"Fetch models from a specific provider.\n\n    Args:\n        provider: The provider name (\"nearai\", \"fireworks\", \"openai\", \"anthropic\", \"crynux\")\n\n    Returns:\n        List of model dictionaries\n\n    The user is responsible for setting the appropriate API key in environment variables:\n    - FIREWORKS_API_KEY for Fireworks\n    - OPENAI_API_KEY for OpenAI\n    - ANTHROPIC_API_KEY for Anthropic\n\n    \"\"\"\n    if provider == \"nearai\":\n        return fetch_models_from_endpoint(\"https://api.near.ai/v1/models\", provider=provider)\n    if provider == \"fireworks\":\n        return fetch_models_from_endpoint(\n            \"https://api.fireworks.ai/inference/v1/models\",\n            provider=provider,\n            api_key=os.environ.get(\"FIREWORKS_API_KEY\"),\n        )\n    if provider == \"openai\":\n        return fetch_models_from_endpoint(\n            \"https://api.openai.com/v1/models\", provider=provider, api_key=os.environ.get(\"OPENAI_API_KEY\")\n        )\n    if provider == \"anthropic\":\n        return fetch_models_from_endpoint(\n            \"https://api.anthropic.com/v1/models\", provider=provider, api_key=os.environ.get(\"ANTHROPIC_API_KEY\")\n        )\n    if provider == \"crynux\":\n        return fetch_models_from_endpoint(\n            \"https://bridge.crynux.ai/v1/models\",\n            provider=provider,\n            # This is a Crynux API key for public demonstration only, the rate limit is very low.\n            # Please use your own API key for production.\n            api_key=\"wo19nkaeWy4ly34iexE7DKtNIY6fZWErCAU8l--735U=\",\n        )\n    print(f\"Unrecognized provider: {provider}\")\n    exit(1)\n</code></pre>"},{"location":"api/#nearai.shared.provider_models.get_provider_model","title":"get_provider_model","text":"<pre><code>get_provider_model(provider: Optional[str], model: str) -&gt; Tuple[Optional[str], str]\n</code></pre> <p>Splits the <code>model</code> string based on a predefined separator and returns the components.</p> <pre><code>provider (Optional[str]): The default provider name. Can be `None` if the provider\n                          is included in the `model` string.\nmodel (str): The model identifier, which may include the provider name separated by\n             a specific delimiter (defined by `PROVIDER_MODEL_SEP`, e.g. `::`).\n</code></pre> Source code in <code>nearai/shared/provider_models.py</code> <pre><code>def get_provider_model(provider: Optional[str], model: str) -&gt; Tuple[Optional[str], str]:\n    \"\"\"Splits the `model` string based on a predefined separator and returns the components.\n\n    Args:\n    ----\n        provider (Optional[str]): The default provider name. Can be `None` if the provider\n                                  is included in the `model` string.\n        model (str): The model identifier, which may include the provider name separated by\n                     a specific delimiter (defined by `PROVIDER_MODEL_SEP`, e.g. `::`).\n\n    \"\"\"\n    if PROVIDER_MODEL_SEP in model:\n        parts = model.split(PROVIDER_MODEL_SEP)\n        assert len(parts) == 2\n        return parts[0], parts[1]\n    return provider, model\n</code></pre>"},{"location":"api/#nearai.shared.provider_models.get_provider_namespaced_model","title":"get_provider_namespaced_model","text":"<pre><code>get_provider_namespaced_model(provider_model: str, provider: Optional[str] = None) -&gt; Tuple[str, NamespacedName]\n</code></pre> <p>Given <code>provider_model</code> returns provider and namespaced model.</p> Source code in <code>nearai/shared/provider_models.py</code> <pre><code>def get_provider_namespaced_model(provider_model: str, provider: Optional[str] = None) -&gt; Tuple[str, NamespacedName]:\n    \"\"\"Given `provider_model` returns provider and namespaced model.\"\"\"\n    provider_opt, provider_model = get_provider_model(DEFAULT_PROVIDER if not provider else provider, provider_model)\n    provider = cast(str, provider_opt)\n\n    if provider.startswith(\"https://\"):\n        return provider, NamespacedName(name=provider_model)\n    if provider == \"local\":\n        return provider, NamespacedName(name=provider_model)\n\n    provider_model = provider_model.replace(\"accounts/\", \"\")\n    provider_model = provider_model.replace(\"fireworks/\", \"\")\n    provider_model = provider_model.replace(\"models/\", \"\")\n    if provider == \"hyperbolic\":\n        provider_model = re.sub(r\".*/\", \"\", provider_model)\n        return provider, NamespacedName(provider_model)\n    if provider == \"fireworks\":\n        parts = provider_model.split(\"/\")\n        if len(parts) == 1:\n            return provider, NamespacedName(name=parts[0])\n        elif len(parts) == 2:\n            return provider, NamespacedName(namespace=parts[0], name=parts[1])\n        else:\n            raise ValueError(f\"Invalid model format for Fireworks: {provider_model}\")\n    raise ValueError(f\"Unrecognized provider: {provider}\")\n</code></pre>"},{"location":"api/#nearai.shared.secure_openai_clients","title":"secure_openai_clients","text":""},{"location":"api/#nearai.shared.secure_openai_clients.SecureAsyncOpenAI","title":"SecureAsyncOpenAI","text":"<p>Secure Async OpenAI client where api key is only accessible in constructor.</p> Source code in <code>nearai/shared/secure_openai_clients.py</code> <pre><code>class SecureAsyncOpenAI:\n    \"\"\"Secure Async OpenAI client where api key is only accessible in constructor.\"\"\"\n\n    def __init__(self, api_key, base_url, **kwargs: Any) -&gt; None:\n        \"\"\"Initialize with auth token that's only accessible in constructor.\"\"\"\n        client = AsyncOpenAI(api_key=api_key, base_url=base_url, **kwargs)\n\n        # Define secure method using closure\n        async def create(self, **params: Any) -&gt; Any:\n            \"\"\"Create a chat completion with secure auth.\"\"\"\n            return await client.chat.completions.create(**params)\n\n        # Create completions class\n        CompletionsClass = type(\"AsyncCompletions\", (), {\"create\": create})  # noqa: N806\n\n        # Create chat class\n        ChatClass = type(\"AsyncChat\", (), {\"completions\": CompletionsClass()})  # noqa: N806\n\n        # Attach chat instance\n        self.chat = ChatClass()\n</code></pre>"},{"location":"api/#nearai.shared.secure_openai_clients.SecureAsyncOpenAI.__init__","title":"__init__","text":"<pre><code>__init__(api_key, base_url, **kwargs: Any) -&gt; None\n</code></pre> <p>Initialize with auth token that's only accessible in constructor.</p> Source code in <code>nearai/shared/secure_openai_clients.py</code> <pre><code>def __init__(self, api_key, base_url, **kwargs: Any) -&gt; None:\n    \"\"\"Initialize with auth token that's only accessible in constructor.\"\"\"\n    client = AsyncOpenAI(api_key=api_key, base_url=base_url, **kwargs)\n\n    # Define secure method using closure\n    async def create(self, **params: Any) -&gt; Any:\n        \"\"\"Create a chat completion with secure auth.\"\"\"\n        return await client.chat.completions.create(**params)\n\n    # Create completions class\n    CompletionsClass = type(\"AsyncCompletions\", (), {\"create\": create})  # noqa: N806\n\n    # Create chat class\n    ChatClass = type(\"AsyncChat\", (), {\"completions\": CompletionsClass()})  # noqa: N806\n\n    # Attach chat instance\n    self.chat = ChatClass()\n</code></pre>"},{"location":"api/#nearai.shared.secure_openai_clients.SecureOpenAI","title":"SecureOpenAI","text":"<p>Secure OpenAI client where api key is only accessible in constructor.</p> Source code in <code>nearai/shared/secure_openai_clients.py</code> <pre><code>class SecureOpenAI:\n    \"\"\"Secure OpenAI client where api key is only accessible in constructor.\"\"\"\n\n    def __init__(self, api_key, base_url, **kwargs: Any) -&gt; None:\n        \"\"\"Initialize with auth token that's only accessible in constructor.\"\"\"\n        client = OpenAI(api_key=api_key, base_url=base_url, **kwargs)\n\n        # Define secure method using closure\n        def create(self, **params: Any) -&gt; Any:\n            \"\"\"Create a chat completion with secure auth.\"\"\"\n            return client.chat.completions.create(**params)\n\n        # Create completions class\n        CompletionsClass = type(\"Completions\", (), {\"create\": create})  # noqa: N806\n\n        # Create chat class\n        ChatClass = type(\"Chat\", (), {\"completions\": CompletionsClass()})  # noqa: N806\n\n        # Attach chat instance\n        self.chat = ChatClass()\n</code></pre>"},{"location":"api/#nearai.shared.secure_openai_clients.SecureOpenAI.__init__","title":"__init__","text":"<pre><code>__init__(api_key, base_url, **kwargs: Any) -&gt; None\n</code></pre> <p>Initialize with auth token that's only accessible in constructor.</p> Source code in <code>nearai/shared/secure_openai_clients.py</code> <pre><code>def __init__(self, api_key, base_url, **kwargs: Any) -&gt; None:\n    \"\"\"Initialize with auth token that's only accessible in constructor.\"\"\"\n    client = OpenAI(api_key=api_key, base_url=base_url, **kwargs)\n\n    # Define secure method using closure\n    def create(self, **params: Any) -&gt; Any:\n        \"\"\"Create a chat completion with secure auth.\"\"\"\n        return client.chat.completions.create(**params)\n\n    # Create completions class\n    CompletionsClass = type(\"Completions\", (), {\"create\": create})  # noqa: N806\n\n    # Create chat class\n    ChatClass = type(\"Chat\", (), {\"completions\": CompletionsClass()})  # noqa: N806\n\n    # Attach chat instance\n    self.chat = ChatClass()\n</code></pre>"},{"location":"api/#nearai.solvers","title":"solvers","text":""},{"location":"api/#nearai.solvers.GSM8KSolverStrategy","title":"GSM8KSolverStrategy","text":"<p>               Bases: <code>SolverStrategy</code></p> <p>Solver strategy for the GSM8K dataset.</p> Source code in <code>nearai/solvers/gsm8k_solver.py</code> <pre><code>class GSM8KSolverStrategy(SolverStrategy):\n    \"\"\"Solver strategy for the GSM8K dataset.\"\"\"\n\n    SHOTS = 8\n\n    def __init__(self, dataset_ref: Union[Dataset, DatasetDict], model: str = \"\", agent: str = \"\") -&gt; None:  # noqa: D107\n        super().__init__(model, agent)\n        self.dataset_ref = dataset_ref\n\n    def evaluation_name(self) -&gt; str:  # noqa: D102\n        return \"gsm8k\"\n\n    def compatible_datasets(self) -&gt; List[str]:  # noqa: D102\n        return [\"gsm8k\"]\n\n    def solve(self, datum: dict) -&gt; bool:  # noqa: D102\n        parsed_datum: GSM8KDatum = GSM8KDatum(**datum)\n\n        problem_shots_indices = list(range(0, self.SHOTS))\n        problem_shots = list(\n            map(\n                lambda i: GSM8KDatum(**self.dataset_ref[\"train\"][i]).model_dump(),\n                problem_shots_indices,\n            )\n        )\n\n        session = self.start_inference_session(\"\")\n        session.add_system_message(\n            dedent(\n                \"\"\"\n                    You are a helpful assistant. You're goal is to answer word based math questions.\n                    \"\"\"\n                + \"\\n\\n\"\n                + \"Here are some examples of math questions and their answers:\"\n                + \"\\n\\n\".join([f\"Question: {shot['question']}\\nAnswer: {shot['answer']}\" for shot in problem_shots])\n                + \"\\n\\n\"\n                + \"Now, answer the next question provided in the user prompt. \"\n                + \"Think step by step about how to solve the problem. \"\n                + \"Then, provide the answer.\"\n            )\n        )\n        res_output = session.run_task(parsed_datum.question).strip()\n\n        ## cleanup the output\n        session = self.start_inference_session(\"\")\n        res_refined_output = session.run_task(\n            dedent(\n                f\"\"\"\n                    You are a helpful assistant. You're goal is to answer math questions.\n\n                    You have just answered a math question with the following response:\n\n                    --- BEGIN RESPONSE ---\n                    {res_output}\n                    --- END RESPONSE ---\n\n                    Please refine your answer.\n\n                    Only output the final number *without units* as your answer. Nothing else.\n                    \"\"\"\n            )\n        ).strip()\n        res_refined_output = res_refined_output.replace(\"$\", \"\").replace(\",\", \"\")\n        if \" \" in res_refined_output:\n            res_refined_output = res_refined_output.split(\" \")[0]\n        try:\n            res_refined_output = str(int(res_refined_output))\n        except Exception:\n            pass\n        try:\n            res_refined_output = str(int(float(res_refined_output)))\n        except Exception:\n            pass\n\n        refined_answer = parsed_datum.answer.replace(\"$\", \"\").replace(\",\", \"\")\n        print(res_refined_output, refined_answer)\n        return res_refined_output == refined_answer\n</code></pre>"},{"location":"api/#nearai.solvers.HellaswagSolverStrategy","title":"HellaswagSolverStrategy","text":"<p>               Bases: <code>SolverStrategy</code></p> <p>Solver strategy for the MMLU dataset.</p> Source code in <code>nearai/solvers/hellaswag_solver.py</code> <pre><code>class HellaswagSolverStrategy(SolverStrategy):\n    \"\"\"Solver strategy for the MMLU dataset.\"\"\"\n\n    def __init__(  # noqa: D107\n        self, dataset_ref: Union[Dataset, DatasetDict], model: str = \"\", agent: str = \"\", shots: int = 8\n    ) -&gt; None:\n        super().__init__(model, agent)\n        self.dataset_ref = dataset_ref\n        self.shots = shots\n\n    def evaluation_name(self) -&gt; str:  # noqa: D102\n        return f\"hellaswag_{self.shots}shots\"\n\n    def compatible_datasets(self) -&gt; List[str]:  # noqa: D102\n        return [\"hellaswag\"]\n\n    def solve(self, datum: dict) -&gt; bool:  # noqa: D102\n        datum = HellaswagDatum(**datum).model_dump()\n\n        choices = [\"A\", \"B\", \"C\", \"D\"]\n        example_problems_indices = list(range(0, 5 * self.shots, 5))\n        example_problems = list(\n            map(\n                lambda d: HellaswagDatum(**d).model_dump(),\n                [self.dataset_ref[\"validation\"][i] for i in example_problems_indices],\n            )\n        )\n        base_prompt = Template(\n            open(PROMPTS_FOLDER / \"hellaswag_verbose_answer.j2\").read(),\n            trim_blocks=True,\n        ).render(\n            example_problems=example_problems,\n            challenge_problem=datum,\n            choices=choices,\n        )\n        response = self.start_inference_session(\"\").run_task(base_prompt)\n\n        ## Extract the answer from the response\n        extract_answer_prompt = Template(\n            open(PROMPTS_FOLDER / \"hellaswag_extract_answer.j2\").read(),\n            trim_blocks=True,\n        ).render(\n            challenge_problem=datum,\n            answer_text=response,\n            choices=choices,\n        )\n        response = self.start_inference_session(\"\").run_task(extract_answer_prompt)\n\n        try:\n            answer = choices.index(response)\n            return bool(answer == int(datum[\"label\"]))\n        except Exception:\n            print(\"Failed to parse answer\")\n            return False\n</code></pre>"},{"location":"api/#nearai.solvers.LeanSolverStrategy","title":"LeanSolverStrategy","text":"<p>               Bases: <code>SolverStrategy</code></p> <p>Solver strategy to evaluate against Lean problems.</p> Source code in <code>nearai/solvers/lean_solver.py</code> <pre><code>class LeanSolverStrategy(SolverStrategy):\n    \"\"\"Solver strategy to evaluate against Lean problems.\"\"\"\n\n    def __init__(  # noqa: D107\n        self, dataset_ref: Union[Dataset, DatasetDict], model: str = \"\", agent: str = \"\"\n    ) -&gt; None:\n        super().__init__(model, agent)\n\n    def evaluation_name(self) -&gt; str:  # noqa: D102\n        assert self.dataset_evaluation_name\n        return self.dataset_evaluation_name\n\n    def compatible_datasets(self) -&gt; List[str]:  # noqa: D102\n        return [\"lean\"]\n\n    def solve(self, datum: dict) -&gt; Tuple[bool, dict]:  # noqa: D102\n        lean_datum = LeanDatum.model_validate(datum)\n        lean_datum.url = load_repository(lean_datum.url)\n\n        info: dict = {}\n        info[\"verbose\"] = {}\n\n        lean_task = LeanTaskInfo(\n            lean_datum.url,\n            lean_datum.commit,\n            lean_datum.filename,\n            lean_datum.theorem,\n            load_theorem(lean_datum),\n        )\n        info[\"verbose\"][\"theorem_raw\"] = lean_task.theorem_raw\n\n        base_prompt = Template(open(PROMPTS_FOLDER / \"lean_answer.j2\").read(), trim_blocks=True).render(\n            url=lean_task.url,\n            commit=lean_task.commit,\n            filepath=lean_task.filename,\n            theorem_name=lean_task.theorem,\n            theorem_raw=lean_task.theorem_raw,\n            begin_marker=BEGIN_MARKER,\n            end_marker=END_MARKER,\n        )\n        response = self.start_inference_session(\"\").run_task(base_prompt)\n\n        json_response = extract_between_markers(response)\n        if not json_response:\n            info[\"error\"] = \"Failed to extract between markers.\"\n            info[\"verbose\"][\"response\"] = response\n            return False, info\n\n        tactics = parse_tactics(json_response)\n        if not tactics:\n            info[\"error\"] = \"Failed to parse tactics.\"\n            info[\"verbose\"][\"response\"] = json_response\n            return False, info\n\n        # Sometimes, there are timeout errors.\n        num_attempts = 3\n        info[\"tactics\"] = tactics\n        for i in range(0, num_attempts):\n            if i != 0:\n                info[\"check_solution_attempts\"] = f\"{i + 1} (max: {num_attempts})\"\n            try:\n                r, m = check_solution(lean_datum, tactics)\n                if r:\n                    info[\"verbose\"][\"check_solution_message\"] = m\n                else:\n                    info[\"check_solution_message\"] = m\n                return r, info\n            except Exception as e:\n                if i == num_attempts - 1:\n                    error_message = f\"Exception while checking solution: {str(e)}.\"\n                    print(error_message)\n                    info[\"error\"] = error_message\n        return False, info\n</code></pre>"},{"location":"api/#nearai.solvers.LiveBenchSolverStrategy","title":"LiveBenchSolverStrategy","text":"<p>               Bases: <code>SolverStrategy</code></p> <p>Solver strategy for the live bench dataset.</p> Source code in <code>nearai/solvers/livebench_solver.py</code> <pre><code>class LiveBenchSolverStrategy(SolverStrategy):\n    \"\"\"Solver strategy for the live bench dataset.\"\"\"\n\n    def __init__(  # noqa: D107\n        self, dataset_ref: str, model: str = \"\", agent: str = \"\", step: str = \"all\"\n    ) -&gt; None:\n        super().__init__(model, agent)\n        self.dataset_ref = dataset_ref\n        self.step = step\n\n    def evaluation_name(self) -&gt; str:  # noqa: D102\n        return \"live_bench\"\n\n    def compatible_datasets(self) -&gt; List[str]:  # noqa: D102\n        return [\"live_bench\"]\n\n    def get_custom_tasks(self) -&gt; List[dict]:  # noqa: D102\n        return [{\"summary\": \"all\"}]\n\n    @property\n    def evaluated_entry_name(self) -&gt; str:  # noqa: D102\n        name = \"\"\n        if self.agent:\n            name = self.agent_name()\n            if self.model_name != \"\":\n                name += f\"_with_model_{self.model_name}\"\n        else:\n            name = self.model_name\n        clean_name = re.sub(r\"[^a-zA-Z0-9_\\-.]\", \"_\", name)\n        return clean_name.lower()\n\n    @SolverStrategyClassProperty\n    def scoring_method(self) -&gt; SolverScoringMethod:  # noqa: D102\n        return SolverScoringMethod.Custom\n\n    def solve(self, _datum: dict) -&gt; Tuple[bool, dict]:  # noqa: D102\n        if self.step == \"gen_model_answer\":\n            self.gen_model_answer()\n            return True, {}\n        if self.step == \"gen_ground_truth_judgement\":\n            return self.gen_ground_truth_judgement(), {}\n        if self.step == \"show_livebench_results\":\n            return self.show_livebench_results()\n        if self.step == \"all\":\n            self.gen_model_answer()\n            if not self.gen_ground_truth_judgement():\n                return False, {}\n            return self.show_livebench_results()\n        return False, {}\n\n    def gen_model_answer(self) -&gt; None:  # noqa: D102\n        print(\"\")\n        print(\"----------- Step gen_model_answer -----------\")\n        print(\"\")\n        list_of_question_files = glob.glob(f\"{self.dataset_ref}/**/question.jsonl\", recursive=True)\n        for question_file in list_of_question_files:\n            questions = load_questions_jsonl(question_file)\n            bench_name = os.path.dirname(question_file).split(str(self.dataset_ref))[-1]\n            answer_file = _get_answer_file_path(bench_name, self.evaluated_entry_name)\n            print(f\"Questions from {question_file}\")\n            print(f\"Output to {answer_file}\")\n            self.run_eval(questions, answer_file)\n\n    def run_eval(self, questions, answer_file) -&gt; None:  # noqa: D102\n        answer_file = os.path.expanduser(answer_file)\n\n        # Load existing answers\n        existing_answers = set()\n        if os.path.exists(answer_file):\n            print(\n                f\"Answer file {answer_file} exists. Will skip already answered questions. Delete this file if that is not intended.\"  # noqa: E501\n            )\n            with open(answer_file, \"r\") as fin:\n                for line in fin:\n                    answer = json.loads(line)\n                    existing_answers.add(answer[\"question_id\"])\n\n        for question in tqdm(questions):\n            if question[\"question_id\"] in existing_answers:\n                continue\n            choices = self.answer_question(question)\n\n            ans_json = {\n                \"question_id\": question[\"question_id\"],\n                \"answer_id\": shortuuid.uuid(),\n                \"model_id\": self.evaluated_entry_name,\n                \"choices\": choices,\n                \"tstamp\": time.time(),\n            }\n\n            os.makedirs(os.path.dirname(answer_file), exist_ok=True)\n            with open(answer_file, \"a\") as fout:\n                fout.write(json.dumps(ans_json) + \"\\n\")\n\n    def answer_question(self, question) -&gt; List[dict]:  # noqa: D102\n        turns = []\n        session = self.start_inference_session(question[\"question_id\"])\n        for qs in question[\"turns\"]:\n            output = session.run_task(qs)\n            turns.append(output)\n\n        return [{\"index\": 0, \"turns\": turns}]\n\n    def gen_ground_truth_judgement(self) -&gt; bool:  # noqa: D102\n        print(\"\")\n        print(\"----------- Step gen_ground_truth_judgement -----------\")\n        print(\"\")\n        script_path = \"nearai/projects/live_bench/gen_ground_truth_judgement.sh\"\n\n        try:\n            # Run the script without capturing output\n            subprocess.run([\"/bin/bash\", script_path, self.evaluated_entry_name, self.dataset_ref], check=True)\n            return True\n\n        except subprocess.CalledProcessError as e:\n            print(f\"An error occurred while running the script: {e}\")\n            return False\n\n    def show_livebench_results(self) -&gt; Tuple[bool, dict]:  # noqa: D102\n        print(\"\")\n        print(\"----------- Step show_livebench_results -----------\")\n        print(\"\")\n        script_path = \"nearai/projects/live_bench/show_livebench_results.sh\"\n\n        try:\n            # Run the script without capturing output\n            subprocess.run([\"/bin/bash\", script_path, self.evaluated_entry_name], check=True)\n\n        except subprocess.CalledProcessError as e:\n            print(f\"An error occurred while running the script: {e}\")\n            return False, {}\n\n        return self.create_result_dict()\n\n    def read_csv_to_dict(self, file_path) -&gt; dict:  # noqa: D102\n        file_path = os.path.expanduser(file_path)\n        with open(file_path, \"r\") as f:\n            reader = csv.DictReader(f)\n            matching_rows = [row for row in reader if row[\"model\"] == self.evaluated_entry_name]\n            return matching_rows[-1] if matching_rows else {}  # Get the last matching row\n\n    def create_result_dict(self) -&gt; Tuple[bool, dict]:  # noqa: D102\n        tasks_data = self.read_csv_to_dict(_get_all_tasks_csv_file())\n        groups_data = self.read_csv_to_dict(_get_all_groups_csv_file())\n\n        if not tasks_data or not groups_data:\n            return False, {}  # Return None if the model is not found in either file\n\n        result: dict = {\"tasks\": {}, \"groups\": {}}\n\n        for key, value in tasks_data.items():\n            if key != \"model\":\n                result[\"tasks\"][key] = float(value)\n\n        for key, value in groups_data.items():\n            if key != \"model\":\n                result[\"groups\"][key] = float(value)\n\n        return True, result\n\n    def get_evaluation_metrics(self, tasks_results: List[Tuple[bool, Any]]) -&gt; Dict[str, Any]:  # noqa: D102\n        results: Dict[str, Dict[str, Any]] = tasks_results[-1][1]\n        if len(results) == 0:\n            raise ValueError(\"Cache empty. Rerun the job with --force. Use --step arg to specify a step.\")\n        metrics: Dict[str, Any] = {\"average\": results[\"groups\"][\"average\"]}\n\n        for group, score in results[\"groups\"].items():\n            if group == \"average\":\n                continue\n            metrics[f\"group/{group}\"] = score\n\n        for task, score in results[\"tasks\"].items():\n            metrics[f\"task/{task}\"] = score\n\n        return metrics\n</code></pre>"},{"location":"api/#nearai.solvers.MBPPSolverStrategy","title":"MBPPSolverStrategy","text":"<p>               Bases: <code>SolverStrategy</code></p> <p>Solver strategy for the MBPP dataset.</p> Source code in <code>nearai/solvers/mbpp_solver.py</code> <pre><code>class MBPPSolverStrategy(SolverStrategy):\n    \"\"\"Solver strategy for the MBPP dataset.\"\"\"\n\n    def __init__(  # noqa: D107\n        self, dataset_ref: Union[Dataset, DatasetDict], model: str = \"\", agent: str = \"\", shots: int = 3\n    ) -&gt; None:\n        super().__init__(model, agent)\n        self.dataset_ref = dataset_ref\n        self.shots = shots\n\n    def evaluation_name(self) -&gt; str:  # noqa: D102\n        prefix = self.dataset_evaluation_name if self.dataset_evaluation_name else \"mbpp\"\n        return f\"{prefix}_{self.shots}shots\"\n\n    def compatible_datasets(self) -&gt; List[str]:  # noqa: D102\n        return [\"mbpp\"]\n\n    def solve(self, datum: dict) -&gt; bool:  # noqa: D102\n        datum = MBPPDatum(**datum).model_dump()\n\n        ## Allow LLM to think \"out loud\" for it's answer\n        function_name = get_function_name(datum[\"code\"])\n        example_problems = list(islice(self.dataset_ref[\"prompt\"], self.shots))\n        base_prompt = Template(open(PROMPTS_FOLDER / \"mbpp_verbose_answer.j2\").read(), trim_blocks=True).render(\n            function_name=function_name,\n            example_problems=example_problems,\n            challenge_problem=datum,\n        )\n        response = self.start_inference_session(str(datum[\"task_id\"])).run_task(base_prompt)\n\n        ## Extract the answer from the response\n        extract_answer_prompt = Template(\n            open(PROMPTS_FOLDER / \"mbpp_extract_answer.j2\").read(), trim_blocks=True\n        ).render(\n            function_name=function_name,\n            answer_text=response,\n        )\n        response = self.start_inference_session(str(datum[\"task_id\"])).run_task(extract_answer_prompt)\n\n        ## Parse the python code\n        python_code_blocks = parse_python_code_block(response) + parse_code_block(response)\n        code = \"\"\n        if len(python_code_blocks) == 0:\n            code = response\n        else:\n            code = python_code_blocks[0]\n\n        ## Evaluate the code\n        try:\n            for test in datum[\"test_list\"] + datum[\"challenge_test_list\"]:\n                test_code = code + \"\\n\" + test\n                if not run_with_timeout(test_code):\n                    return False\n            return True\n        except Exception:\n            return False\n</code></pre>"},{"location":"api/#nearai.solvers.MMLUSolverStrategy","title":"MMLUSolverStrategy","text":"<p>               Bases: <code>SolverStrategy</code></p> <p>Solver strategy for the MMLU dataset.</p> Source code in <code>nearai/solvers/mmlu_solver.py</code> <pre><code>class MMLUSolverStrategy(SolverStrategy):\n    \"\"\"Solver strategy for the MMLU dataset.\"\"\"\n\n    def __init__(  # noqa: D107\n        self, dataset_ref: Union[Dataset, DatasetDict], model: str = \"\", agent: str = \"\", shots: int = 8\n    ) -&gt; None:\n        super().__init__(model, agent)\n        self.dataset_ref = dataset_ref\n        self.shots = shots\n\n    def evaluation_name(self) -&gt; str:  # noqa: D102\n        prefix = self.dataset_evaluation_name if self.dataset_evaluation_name else \"mmlu\"\n        return f\"{prefix}_{self.shots}shots\"\n\n    def compatible_datasets(self) -&gt; List[str]:  # noqa: D102\n        return [\"mmlu\"]\n\n    def solve(self, datum: dict) -&gt; bool:  # noqa: D102\n        datum = MMLUDatum(**datum).model_dump()\n\n        choices = [\"A\", \"B\", \"C\", \"D\"]\n        example_problems_indices = list(range(0, 5 * self.shots, 5))\n        example_problems = list(\n            map(\n                lambda d: MMLUDatum(**d).model_dump(),\n                [self.dataset_ref[\"dev\"][i] for i in example_problems_indices],\n            )\n        )\n        base_prompt = Template(open(PROMPTS_FOLDER / \"mmlu_verbose_answer.j2\").read(), trim_blocks=True).render(\n            example_problems=example_problems,\n            challenge_problem=datum,\n            choices=choices,\n        )\n\n        response = self.start_inference_session(\"\").run_task(base_prompt)\n\n        ## Extract the answer from the response\n        extract_answer_prompt = Template(\n            open(PROMPTS_FOLDER / \"mmlu_extract_answer.j2\").read(), trim_blocks=True\n        ).render(\n            challenge_problem=datum,\n            answer_text=response,\n            choices=choices,\n        )\n        response = self.start_inference_session(\"\").run_task(extract_answer_prompt)\n\n        try:\n            answer = choices.index(response)\n            return bool(answer == datum[\"answer\"])\n        except Exception:\n            print(\"Failed to parse answer\")\n            return False\n</code></pre>"},{"location":"api/#nearai.solvers.SolverStrategy","title":"SolverStrategy","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract class for solver strategies.</p> Source code in <code>nearai/solvers/__init__.py</code> <pre><code>class SolverStrategy(ABC, metaclass=SolverStrategyMeta):\n    \"\"\"Abstract class for solver strategies.\"\"\"\n\n    def __init__(self, model: str = \"\", agent: str = \"\") -&gt; None:\n        CONFIG.confirm_commands = False\n        self.client_config = CONFIG.get_client_config()\n        self.client = InferenceClient(self.client_config)\n        assert model != \"\" or agent != \"\"\n        self.dataset_evaluation_name = \"\"\n\n        self.provider = \"\"\n        self.model_namespace = \"\"\n        self.model_full_path = \"\"\n        self.model_name = \"\"\n        if model != \"\":\n            self.provider, self.model_full_path = self.client.provider_models.match_provider_model(model)\n            self.provider, namespaced_model = get_provider_namespaced_model(self.model_full_path, self.provider)\n            self.model_namespace = namespaced_model.namespace\n            self.model_name = namespaced_model.name\n\n        # If provider specified is a url, recreate a `client`.\n        if self.provider.startswith(\"https://\"):\n            self.client_config.base_url = self.provider\n            self.client_config.auth = None\n            self.client_config.default_provider = self.provider\n            print(self.client_config)\n            self.client = InferenceClient(self.client_config)\n\n        self.agent = agent\n        self.agent_params = {\n            \"api_url\": CONFIG.api_url,\n            \"data_source\": \"local_files\",\n            \"temperature\": 0.0,\n            \"record_run\": False,\n            \"verbose\": False,\n            \"change_to_agent_temp_dir\": False,\n        }\n        if self.model_full_path:\n            self.agent_params[\"model\"] = self.model_full_path\n\n    @property\n    def name(self) -&gt; str:\n        \"\"\"Returns the name of the solver strategy.\"\"\"\n        return type(self).__name__\n\n    @SolverStrategyClassProperty\n    def scoring_method(self) -&gt; SolverScoringMethod:\n        return SolverScoringMethod.TrueOrFalseList\n\n    @abstractmethod\n    def evaluation_name(self) -&gt; str:\n        \"\"\"Returns a unique name for (benchmark, solver) tuple, e.g. 'mbpp' or 'live_bench' or 'mmlu-5-shot'.\"\"\"\n        ...\n\n    @abstractmethod\n    def compatible_datasets(self) -&gt; List[str]:\n        \"\"\"Returns the list of datasets that the solver strategy is compatible with.\"\"\"\n        ...\n\n    def agent_name(self) -&gt; str:\n        \"\"\"Returns agent name that is evaluated.\"\"\"\n        if not self.agent:\n            return \"\"\n        path = Path(self.agent)\n        return path.parent.name\n\n    def agent_version(self) -&gt; str:\n        \"\"\"Returns agent name that is evaluated.\"\"\"\n        if not self.agent:\n            return \"\"\n        path = Path(self.agent)\n        return path.name\n\n    def evaluated_entry_namespace(self) -&gt; str:\n        \"\"\"Returns namespace of a model or agent to be evaluated.\"\"\"\n        if self.agent:\n            path = Path(self.agent)\n            return path.parent.parent.name\n        return self.model_namespace\n\n    def model_provider(self) -&gt; str:\n        \"\"\"Returns model provider.\"\"\"\n        if self.provider != \"\":\n            return self.provider\n        if self.agent != \"\":\n            agent_obj = Agent.load_agent(self.agent, self.client_config, local=True)\n            return agent_obj.model_provider\n        return \"\"\n\n    @abstractmethod\n    def solve(self, datum: dict) -&gt; Union[bool, Tuple[bool, Any]]:\n        \"\"\"Solves the task for the given datum.\"\"\"\n        ...\n\n    def get_custom_tasks(self) -&gt; List[dict]:\n        \"\"\"Custom tasks for custom benchmark.\"\"\"\n        if self.scoring_method == SolverScoringMethod.Custom:\n            raise NotImplementedError(\"get_custom_tasks must be implemented for Custom scoring method\")\n        else:\n            raise AttributeError(\"get_custom_tasks is only applicable for Custom scoring method\")\n\n    def get_evaluation_metrics(self, tasks_results: List[Tuple[bool, Any]]) -&gt; Dict[str, Any]:\n        \"\"\"Given results for all datums, returns evaluation metrics.\n\n        Not used by TrueOrFalseList scoring method.\n        Do not prepend with evaluation_name. If hierarchical, use slashes /.\n        Expected metrics is a dict of scores, e.g.: {\"average\": &lt;val&gt;, \"group/coding\": &lt;val&gt;}.\n        \"\"\"\n        raise NotImplementedError(\"get_evaluation_metrics not implemented\")\n\n    def start_inference_session(self, task_id: str) -&gt; SolverInferenceSession:\n        return SolverInferenceSession(\n            self.agent, self.agent_params, self.model_full_path, self.client, self.evaluation_name()\n        ).start_inference_session(task_id)\n</code></pre>"},{"location":"api/#nearai.solvers.SolverStrategy.name","title":"name  <code>property</code>","text":"<pre><code>name: str\n</code></pre> <p>Returns the name of the solver strategy.</p>"},{"location":"api/#nearai.solvers.SolverStrategy.agent_name","title":"agent_name","text":"<pre><code>agent_name() -&gt; str\n</code></pre> <p>Returns agent name that is evaluated.</p> Source code in <code>nearai/solvers/__init__.py</code> <pre><code>def agent_name(self) -&gt; str:\n    \"\"\"Returns agent name that is evaluated.\"\"\"\n    if not self.agent:\n        return \"\"\n    path = Path(self.agent)\n    return path.parent.name\n</code></pre>"},{"location":"api/#nearai.solvers.SolverStrategy.agent_version","title":"agent_version","text":"<pre><code>agent_version() -&gt; str\n</code></pre> <p>Returns agent name that is evaluated.</p> Source code in <code>nearai/solvers/__init__.py</code> <pre><code>def agent_version(self) -&gt; str:\n    \"\"\"Returns agent name that is evaluated.\"\"\"\n    if not self.agent:\n        return \"\"\n    path = Path(self.agent)\n    return path.name\n</code></pre>"},{"location":"api/#nearai.solvers.SolverStrategy.compatible_datasets","title":"compatible_datasets  <code>abstractmethod</code>","text":"<pre><code>compatible_datasets() -&gt; List[str]\n</code></pre> <p>Returns the list of datasets that the solver strategy is compatible with.</p> Source code in <code>nearai/solvers/__init__.py</code> <pre><code>@abstractmethod\ndef compatible_datasets(self) -&gt; List[str]:\n    \"\"\"Returns the list of datasets that the solver strategy is compatible with.\"\"\"\n    ...\n</code></pre>"},{"location":"api/#nearai.solvers.SolverStrategy.evaluated_entry_namespace","title":"evaluated_entry_namespace","text":"<pre><code>evaluated_entry_namespace() -&gt; str\n</code></pre> <p>Returns namespace of a model or agent to be evaluated.</p> Source code in <code>nearai/solvers/__init__.py</code> <pre><code>def evaluated_entry_namespace(self) -&gt; str:\n    \"\"\"Returns namespace of a model or agent to be evaluated.\"\"\"\n    if self.agent:\n        path = Path(self.agent)\n        return path.parent.parent.name\n    return self.model_namespace\n</code></pre>"},{"location":"api/#nearai.solvers.SolverStrategy.evaluation_name","title":"evaluation_name  <code>abstractmethod</code>","text":"<pre><code>evaluation_name() -&gt; str\n</code></pre> <p>Returns a unique name for (benchmark, solver) tuple, e.g. 'mbpp' or 'live_bench' or 'mmlu-5-shot'.</p> Source code in <code>nearai/solvers/__init__.py</code> <pre><code>@abstractmethod\ndef evaluation_name(self) -&gt; str:\n    \"\"\"Returns a unique name for (benchmark, solver) tuple, e.g. 'mbpp' or 'live_bench' or 'mmlu-5-shot'.\"\"\"\n    ...\n</code></pre>"},{"location":"api/#nearai.solvers.SolverStrategy.get_custom_tasks","title":"get_custom_tasks","text":"<pre><code>get_custom_tasks() -&gt; List[dict]\n</code></pre> <p>Custom tasks for custom benchmark.</p> Source code in <code>nearai/solvers/__init__.py</code> <pre><code>def get_custom_tasks(self) -&gt; List[dict]:\n    \"\"\"Custom tasks for custom benchmark.\"\"\"\n    if self.scoring_method == SolverScoringMethod.Custom:\n        raise NotImplementedError(\"get_custom_tasks must be implemented for Custom scoring method\")\n    else:\n        raise AttributeError(\"get_custom_tasks is only applicable for Custom scoring method\")\n</code></pre>"},{"location":"api/#nearai.solvers.SolverStrategy.get_evaluation_metrics","title":"get_evaluation_metrics","text":"<pre><code>get_evaluation_metrics(tasks_results: List[Tuple[bool, Any]]) -&gt; Dict[str, Any]\n</code></pre> <p>Given results for all datums, returns evaluation metrics.</p> <p>Not used by TrueOrFalseList scoring method. Do not prepend with evaluation_name. If hierarchical, use slashes /. Expected metrics is a dict of scores, e.g.: {\"average\": , \"group/coding\": }. Source code in <code>nearai/solvers/__init__.py</code> <pre><code>def get_evaluation_metrics(self, tasks_results: List[Tuple[bool, Any]]) -&gt; Dict[str, Any]:\n    \"\"\"Given results for all datums, returns evaluation metrics.\n\n    Not used by TrueOrFalseList scoring method.\n    Do not prepend with evaluation_name. If hierarchical, use slashes /.\n    Expected metrics is a dict of scores, e.g.: {\"average\": &lt;val&gt;, \"group/coding\": &lt;val&gt;}.\n    \"\"\"\n    raise NotImplementedError(\"get_evaluation_metrics not implemented\")\n</code></pre>"},{"location":"api/#nearai.solvers.SolverStrategy.model_provider","title":"model_provider","text":"<pre><code>model_provider() -&gt; str\n</code></pre> <p>Returns model provider.</p> Source code in <code>nearai/solvers/__init__.py</code> <pre><code>def model_provider(self) -&gt; str:\n    \"\"\"Returns model provider.\"\"\"\n    if self.provider != \"\":\n        return self.provider\n    if self.agent != \"\":\n        agent_obj = Agent.load_agent(self.agent, self.client_config, local=True)\n        return agent_obj.model_provider\n    return \"\"\n</code></pre>"},{"location":"api/#nearai.solvers.SolverStrategy.solve","title":"solve  <code>abstractmethod</code>","text":"<pre><code>solve(datum: dict) -&gt; Union[bool, Tuple[bool, Any]]\n</code></pre> <p>Solves the task for the given datum.</p> Source code in <code>nearai/solvers/__init__.py</code> <pre><code>@abstractmethod\ndef solve(self, datum: dict) -&gt; Union[bool, Tuple[bool, Any]]:\n    \"\"\"Solves the task for the given datum.\"\"\"\n    ...\n</code></pre>"},{"location":"api/#nearai.solvers.SolverStrategyMeta","title":"SolverStrategyMeta","text":"<p>               Bases: <code>ABCMeta</code></p> <p>Metaclass that automatically registers subclasses in the SolverStrategyRegistry.</p> Source code in <code>nearai/solvers/__init__.py</code> <pre><code>class SolverStrategyMeta(ABCMeta):\n    \"\"\"Metaclass that automatically registers subclasses in the SolverStrategyRegistry.\"\"\"\n\n    def __new__(cls, name: str, bases: tuple, namespace: dict) -&gt; Any:\n        new_class = super().__new__(cls, name, bases, namespace)\n        if bases != (ABC,):  # Avoid registering the abstract base class itself\n            SolverStrategyRegistry[new_class.__name__] = new_class  # type: ignore\n        return new_class\n</code></pre>"},{"location":"api/#nearai.solvers.gsm8k_solver","title":"gsm8k_solver","text":""},{"location":"api/#nearai.solvers.gsm8k_solver.GSM8KSolverStrategy","title":"GSM8KSolverStrategy","text":"<p>               Bases: <code>SolverStrategy</code></p> <p>Solver strategy for the GSM8K dataset.</p> Source code in <code>nearai/solvers/gsm8k_solver.py</code> <pre><code>class GSM8KSolverStrategy(SolverStrategy):\n    \"\"\"Solver strategy for the GSM8K dataset.\"\"\"\n\n    SHOTS = 8\n\n    def __init__(self, dataset_ref: Union[Dataset, DatasetDict], model: str = \"\", agent: str = \"\") -&gt; None:  # noqa: D107\n        super().__init__(model, agent)\n        self.dataset_ref = dataset_ref\n\n    def evaluation_name(self) -&gt; str:  # noqa: D102\n        return \"gsm8k\"\n\n    def compatible_datasets(self) -&gt; List[str]:  # noqa: D102\n        return [\"gsm8k\"]\n\n    def solve(self, datum: dict) -&gt; bool:  # noqa: D102\n        parsed_datum: GSM8KDatum = GSM8KDatum(**datum)\n\n        problem_shots_indices = list(range(0, self.SHOTS))\n        problem_shots = list(\n            map(\n                lambda i: GSM8KDatum(**self.dataset_ref[\"train\"][i]).model_dump(),\n                problem_shots_indices,\n            )\n        )\n\n        session = self.start_inference_session(\"\")\n        session.add_system_message(\n            dedent(\n                \"\"\"\n                    You are a helpful assistant. You're goal is to answer word based math questions.\n                    \"\"\"\n                + \"\\n\\n\"\n                + \"Here are some examples of math questions and their answers:\"\n                + \"\\n\\n\".join([f\"Question: {shot['question']}\\nAnswer: {shot['answer']}\" for shot in problem_shots])\n                + \"\\n\\n\"\n                + \"Now, answer the next question provided in the user prompt. \"\n                + \"Think step by step about how to solve the problem. \"\n                + \"Then, provide the answer.\"\n            )\n        )\n        res_output = session.run_task(parsed_datum.question).strip()\n\n        ## cleanup the output\n        session = self.start_inference_session(\"\")\n        res_refined_output = session.run_task(\n            dedent(\n                f\"\"\"\n                    You are a helpful assistant. You're goal is to answer math questions.\n\n                    You have just answered a math question with the following response:\n\n                    --- BEGIN RESPONSE ---\n                    {res_output}\n                    --- END RESPONSE ---\n\n                    Please refine your answer.\n\n                    Only output the final number *without units* as your answer. Nothing else.\n                    \"\"\"\n            )\n        ).strip()\n        res_refined_output = res_refined_output.replace(\"$\", \"\").replace(\",\", \"\")\n        if \" \" in res_refined_output:\n            res_refined_output = res_refined_output.split(\" \")[0]\n        try:\n            res_refined_output = str(int(res_refined_output))\n        except Exception:\n            pass\n        try:\n            res_refined_output = str(int(float(res_refined_output)))\n        except Exception:\n            pass\n\n        refined_answer = parsed_datum.answer.replace(\"$\", \"\").replace(\",\", \"\")\n        print(res_refined_output, refined_answer)\n        return res_refined_output == refined_answer\n</code></pre>"},{"location":"api/#nearai.solvers.hellaswag_solver","title":"hellaswag_solver","text":""},{"location":"api/#nearai.solvers.hellaswag_solver.HellaswagSolverStrategy","title":"HellaswagSolverStrategy","text":"<p>               Bases: <code>SolverStrategy</code></p> <p>Solver strategy for the MMLU dataset.</p> Source code in <code>nearai/solvers/hellaswag_solver.py</code> <pre><code>class HellaswagSolverStrategy(SolverStrategy):\n    \"\"\"Solver strategy for the MMLU dataset.\"\"\"\n\n    def __init__(  # noqa: D107\n        self, dataset_ref: Union[Dataset, DatasetDict], model: str = \"\", agent: str = \"\", shots: int = 8\n    ) -&gt; None:\n        super().__init__(model, agent)\n        self.dataset_ref = dataset_ref\n        self.shots = shots\n\n    def evaluation_name(self) -&gt; str:  # noqa: D102\n        return f\"hellaswag_{self.shots}shots\"\n\n    def compatible_datasets(self) -&gt; List[str]:  # noqa: D102\n        return [\"hellaswag\"]\n\n    def solve(self, datum: dict) -&gt; bool:  # noqa: D102\n        datum = HellaswagDatum(**datum).model_dump()\n\n        choices = [\"A\", \"B\", \"C\", \"D\"]\n        example_problems_indices = list(range(0, 5 * self.shots, 5))\n        example_problems = list(\n            map(\n                lambda d: HellaswagDatum(**d).model_dump(),\n                [self.dataset_ref[\"validation\"][i] for i in example_problems_indices],\n            )\n        )\n        base_prompt = Template(\n            open(PROMPTS_FOLDER / \"hellaswag_verbose_answer.j2\").read(),\n            trim_blocks=True,\n        ).render(\n            example_problems=example_problems,\n            challenge_problem=datum,\n            choices=choices,\n        )\n        response = self.start_inference_session(\"\").run_task(base_prompt)\n\n        ## Extract the answer from the response\n        extract_answer_prompt = Template(\n            open(PROMPTS_FOLDER / \"hellaswag_extract_answer.j2\").read(),\n            trim_blocks=True,\n        ).render(\n            challenge_problem=datum,\n            answer_text=response,\n            choices=choices,\n        )\n        response = self.start_inference_session(\"\").run_task(extract_answer_prompt)\n\n        try:\n            answer = choices.index(response)\n            return bool(answer == int(datum[\"label\"]))\n        except Exception:\n            print(\"Failed to parse answer\")\n            return False\n</code></pre>"},{"location":"api/#nearai.solvers.lean_solver","title":"lean_solver","text":""},{"location":"api/#nearai.solvers.lean_solver.LeanSolverStrategy","title":"LeanSolverStrategy","text":"<p>               Bases: <code>SolverStrategy</code></p> <p>Solver strategy to evaluate against Lean problems.</p> Source code in <code>nearai/solvers/lean_solver.py</code> <pre><code>class LeanSolverStrategy(SolverStrategy):\n    \"\"\"Solver strategy to evaluate against Lean problems.\"\"\"\n\n    def __init__(  # noqa: D107\n        self, dataset_ref: Union[Dataset, DatasetDict], model: str = \"\", agent: str = \"\"\n    ) -&gt; None:\n        super().__init__(model, agent)\n\n    def evaluation_name(self) -&gt; str:  # noqa: D102\n        assert self.dataset_evaluation_name\n        return self.dataset_evaluation_name\n\n    def compatible_datasets(self) -&gt; List[str]:  # noqa: D102\n        return [\"lean\"]\n\n    def solve(self, datum: dict) -&gt; Tuple[bool, dict]:  # noqa: D102\n        lean_datum = LeanDatum.model_validate(datum)\n        lean_datum.url = load_repository(lean_datum.url)\n\n        info: dict = {}\n        info[\"verbose\"] = {}\n\n        lean_task = LeanTaskInfo(\n            lean_datum.url,\n            lean_datum.commit,\n            lean_datum.filename,\n            lean_datum.theorem,\n            load_theorem(lean_datum),\n        )\n        info[\"verbose\"][\"theorem_raw\"] = lean_task.theorem_raw\n\n        base_prompt = Template(open(PROMPTS_FOLDER / \"lean_answer.j2\").read(), trim_blocks=True).render(\n            url=lean_task.url,\n            commit=lean_task.commit,\n            filepath=lean_task.filename,\n            theorem_name=lean_task.theorem,\n            theorem_raw=lean_task.theorem_raw,\n            begin_marker=BEGIN_MARKER,\n            end_marker=END_MARKER,\n        )\n        response = self.start_inference_session(\"\").run_task(base_prompt)\n\n        json_response = extract_between_markers(response)\n        if not json_response:\n            info[\"error\"] = \"Failed to extract between markers.\"\n            info[\"verbose\"][\"response\"] = response\n            return False, info\n\n        tactics = parse_tactics(json_response)\n        if not tactics:\n            info[\"error\"] = \"Failed to parse tactics.\"\n            info[\"verbose\"][\"response\"] = json_response\n            return False, info\n\n        # Sometimes, there are timeout errors.\n        num_attempts = 3\n        info[\"tactics\"] = tactics\n        for i in range(0, num_attempts):\n            if i != 0:\n                info[\"check_solution_attempts\"] = f\"{i + 1} (max: {num_attempts})\"\n            try:\n                r, m = check_solution(lean_datum, tactics)\n                if r:\n                    info[\"verbose\"][\"check_solution_message\"] = m\n                else:\n                    info[\"check_solution_message\"] = m\n                return r, info\n            except Exception as e:\n                if i == num_attempts - 1:\n                    error_message = f\"Exception while checking solution: {str(e)}.\"\n                    print(error_message)\n                    info[\"error\"] = error_message\n        return False, info\n</code></pre>"},{"location":"api/#nearai.solvers.lean_solver.load_theorem","title":"load_theorem","text":"<pre><code>load_theorem(task: LeanDatum) -&gt; str\n</code></pre> <p>Use local copy of the repository.</p> Source code in <code>nearai/solvers/lean_solver.py</code> <pre><code>def load_theorem(task: LeanDatum) -&gt; str:\n    \"\"\"Use local copy of the repository.\"\"\"\n    repo = LeanGitRepo(task.url, task.commit)\n    theorem = Theorem(repo, task.filename, task.theorem)\n    with Dojo(theorem) as (_, state):\n        return state.pp\n</code></pre>"},{"location":"api/#nearai.solvers.livebench_solver","title":"livebench_solver","text":""},{"location":"api/#nearai.solvers.livebench_solver.LiveBenchSolverStrategy","title":"LiveBenchSolverStrategy","text":"<p>               Bases: <code>SolverStrategy</code></p> <p>Solver strategy for the live bench dataset.</p> Source code in <code>nearai/solvers/livebench_solver.py</code> <pre><code>class LiveBenchSolverStrategy(SolverStrategy):\n    \"\"\"Solver strategy for the live bench dataset.\"\"\"\n\n    def __init__(  # noqa: D107\n        self, dataset_ref: str, model: str = \"\", agent: str = \"\", step: str = \"all\"\n    ) -&gt; None:\n        super().__init__(model, agent)\n        self.dataset_ref = dataset_ref\n        self.step = step\n\n    def evaluation_name(self) -&gt; str:  # noqa: D102\n        return \"live_bench\"\n\n    def compatible_datasets(self) -&gt; List[str]:  # noqa: D102\n        return [\"live_bench\"]\n\n    def get_custom_tasks(self) -&gt; List[dict]:  # noqa: D102\n        return [{\"summary\": \"all\"}]\n\n    @property\n    def evaluated_entry_name(self) -&gt; str:  # noqa: D102\n        name = \"\"\n        if self.agent:\n            name = self.agent_name()\n            if self.model_name != \"\":\n                name += f\"_with_model_{self.model_name}\"\n        else:\n            name = self.model_name\n        clean_name = re.sub(r\"[^a-zA-Z0-9_\\-.]\", \"_\", name)\n        return clean_name.lower()\n\n    @SolverStrategyClassProperty\n    def scoring_method(self) -&gt; SolverScoringMethod:  # noqa: D102\n        return SolverScoringMethod.Custom\n\n    def solve(self, _datum: dict) -&gt; Tuple[bool, dict]:  # noqa: D102\n        if self.step == \"gen_model_answer\":\n            self.gen_model_answer()\n            return True, {}\n        if self.step == \"gen_ground_truth_judgement\":\n            return self.gen_ground_truth_judgement(), {}\n        if self.step == \"show_livebench_results\":\n            return self.show_livebench_results()\n        if self.step == \"all\":\n            self.gen_model_answer()\n            if not self.gen_ground_truth_judgement():\n                return False, {}\n            return self.show_livebench_results()\n        return False, {}\n\n    def gen_model_answer(self) -&gt; None:  # noqa: D102\n        print(\"\")\n        print(\"----------- Step gen_model_answer -----------\")\n        print(\"\")\n        list_of_question_files = glob.glob(f\"{self.dataset_ref}/**/question.jsonl\", recursive=True)\n        for question_file in list_of_question_files:\n            questions = load_questions_jsonl(question_file)\n            bench_name = os.path.dirname(question_file).split(str(self.dataset_ref))[-1]\n            answer_file = _get_answer_file_path(bench_name, self.evaluated_entry_name)\n            print(f\"Questions from {question_file}\")\n            print(f\"Output to {answer_file}\")\n            self.run_eval(questions, answer_file)\n\n    def run_eval(self, questions, answer_file) -&gt; None:  # noqa: D102\n        answer_file = os.path.expanduser(answer_file)\n\n        # Load existing answers\n        existing_answers = set()\n        if os.path.exists(answer_file):\n            print(\n                f\"Answer file {answer_file} exists. Will skip already answered questions. Delete this file if that is not intended.\"  # noqa: E501\n            )\n            with open(answer_file, \"r\") as fin:\n                for line in fin:\n                    answer = json.loads(line)\n                    existing_answers.add(answer[\"question_id\"])\n\n        for question in tqdm(questions):\n            if question[\"question_id\"] in existing_answers:\n                continue\n            choices = self.answer_question(question)\n\n            ans_json = {\n                \"question_id\": question[\"question_id\"],\n                \"answer_id\": shortuuid.uuid(),\n                \"model_id\": self.evaluated_entry_name,\n                \"choices\": choices,\n                \"tstamp\": time.time(),\n            }\n\n            os.makedirs(os.path.dirname(answer_file), exist_ok=True)\n            with open(answer_file, \"a\") as fout:\n                fout.write(json.dumps(ans_json) + \"\\n\")\n\n    def answer_question(self, question) -&gt; List[dict]:  # noqa: D102\n        turns = []\n        session = self.start_inference_session(question[\"question_id\"])\n        for qs in question[\"turns\"]:\n            output = session.run_task(qs)\n            turns.append(output)\n\n        return [{\"index\": 0, \"turns\": turns}]\n\n    def gen_ground_truth_judgement(self) -&gt; bool:  # noqa: D102\n        print(\"\")\n        print(\"----------- Step gen_ground_truth_judgement -----------\")\n        print(\"\")\n        script_path = \"nearai/projects/live_bench/gen_ground_truth_judgement.sh\"\n\n        try:\n            # Run the script without capturing output\n            subprocess.run([\"/bin/bash\", script_path, self.evaluated_entry_name, self.dataset_ref], check=True)\n            return True\n\n        except subprocess.CalledProcessError as e:\n            print(f\"An error occurred while running the script: {e}\")\n            return False\n\n    def show_livebench_results(self) -&gt; Tuple[bool, dict]:  # noqa: D102\n        print(\"\")\n        print(\"----------- Step show_livebench_results -----------\")\n        print(\"\")\n        script_path = \"nearai/projects/live_bench/show_livebench_results.sh\"\n\n        try:\n            # Run the script without capturing output\n            subprocess.run([\"/bin/bash\", script_path, self.evaluated_entry_name], check=True)\n\n        except subprocess.CalledProcessError as e:\n            print(f\"An error occurred while running the script: {e}\")\n            return False, {}\n\n        return self.create_result_dict()\n\n    def read_csv_to_dict(self, file_path) -&gt; dict:  # noqa: D102\n        file_path = os.path.expanduser(file_path)\n        with open(file_path, \"r\") as f:\n            reader = csv.DictReader(f)\n            matching_rows = [row for row in reader if row[\"model\"] == self.evaluated_entry_name]\n            return matching_rows[-1] if matching_rows else {}  # Get the last matching row\n\n    def create_result_dict(self) -&gt; Tuple[bool, dict]:  # noqa: D102\n        tasks_data = self.read_csv_to_dict(_get_all_tasks_csv_file())\n        groups_data = self.read_csv_to_dict(_get_all_groups_csv_file())\n\n        if not tasks_data or not groups_data:\n            return False, {}  # Return None if the model is not found in either file\n\n        result: dict = {\"tasks\": {}, \"groups\": {}}\n\n        for key, value in tasks_data.items():\n            if key != \"model\":\n                result[\"tasks\"][key] = float(value)\n\n        for key, value in groups_data.items():\n            if key != \"model\":\n                result[\"groups\"][key] = float(value)\n\n        return True, result\n\n    def get_evaluation_metrics(self, tasks_results: List[Tuple[bool, Any]]) -&gt; Dict[str, Any]:  # noqa: D102\n        results: Dict[str, Dict[str, Any]] = tasks_results[-1][1]\n        if len(results) == 0:\n            raise ValueError(\"Cache empty. Rerun the job with --force. Use --step arg to specify a step.\")\n        metrics: Dict[str, Any] = {\"average\": results[\"groups\"][\"average\"]}\n\n        for group, score in results[\"groups\"].items():\n            if group == \"average\":\n                continue\n            metrics[f\"group/{group}\"] = score\n\n        for task, score in results[\"tasks\"].items():\n            metrics[f\"task/{task}\"] = score\n\n        return metrics\n</code></pre>"},{"location":"api/#nearai.solvers.mbpp_solver","title":"mbpp_solver","text":""},{"location":"api/#nearai.solvers.mbpp_solver.MBPPSolverStrategy","title":"MBPPSolverStrategy","text":"<p>               Bases: <code>SolverStrategy</code></p> <p>Solver strategy for the MBPP dataset.</p> Source code in <code>nearai/solvers/mbpp_solver.py</code> <pre><code>class MBPPSolverStrategy(SolverStrategy):\n    \"\"\"Solver strategy for the MBPP dataset.\"\"\"\n\n    def __init__(  # noqa: D107\n        self, dataset_ref: Union[Dataset, DatasetDict], model: str = \"\", agent: str = \"\", shots: int = 3\n    ) -&gt; None:\n        super().__init__(model, agent)\n        self.dataset_ref = dataset_ref\n        self.shots = shots\n\n    def evaluation_name(self) -&gt; str:  # noqa: D102\n        prefix = self.dataset_evaluation_name if self.dataset_evaluation_name else \"mbpp\"\n        return f\"{prefix}_{self.shots}shots\"\n\n    def compatible_datasets(self) -&gt; List[str]:  # noqa: D102\n        return [\"mbpp\"]\n\n    def solve(self, datum: dict) -&gt; bool:  # noqa: D102\n        datum = MBPPDatum(**datum).model_dump()\n\n        ## Allow LLM to think \"out loud\" for it's answer\n        function_name = get_function_name(datum[\"code\"])\n        example_problems = list(islice(self.dataset_ref[\"prompt\"], self.shots))\n        base_prompt = Template(open(PROMPTS_FOLDER / \"mbpp_verbose_answer.j2\").read(), trim_blocks=True).render(\n            function_name=function_name,\n            example_problems=example_problems,\n            challenge_problem=datum,\n        )\n        response = self.start_inference_session(str(datum[\"task_id\"])).run_task(base_prompt)\n\n        ## Extract the answer from the response\n        extract_answer_prompt = Template(\n            open(PROMPTS_FOLDER / \"mbpp_extract_answer.j2\").read(), trim_blocks=True\n        ).render(\n            function_name=function_name,\n            answer_text=response,\n        )\n        response = self.start_inference_session(str(datum[\"task_id\"])).run_task(extract_answer_prompt)\n\n        ## Parse the python code\n        python_code_blocks = parse_python_code_block(response) + parse_code_block(response)\n        code = \"\"\n        if len(python_code_blocks) == 0:\n            code = response\n        else:\n            code = python_code_blocks[0]\n\n        ## Evaluate the code\n        try:\n            for test in datum[\"test_list\"] + datum[\"challenge_test_list\"]:\n                test_code = code + \"\\n\" + test\n                if not run_with_timeout(test_code):\n                    return False\n            return True\n        except Exception:\n            return False\n</code></pre>"},{"location":"api/#nearai.solvers.mmlu_solver","title":"mmlu_solver","text":""},{"location":"api/#nearai.solvers.mmlu_solver.MMLUSolverStrategy","title":"MMLUSolverStrategy","text":"<p>               Bases: <code>SolverStrategy</code></p> <p>Solver strategy for the MMLU dataset.</p> Source code in <code>nearai/solvers/mmlu_solver.py</code> <pre><code>class MMLUSolverStrategy(SolverStrategy):\n    \"\"\"Solver strategy for the MMLU dataset.\"\"\"\n\n    def __init__(  # noqa: D107\n        self, dataset_ref: Union[Dataset, DatasetDict], model: str = \"\", agent: str = \"\", shots: int = 8\n    ) -&gt; None:\n        super().__init__(model, agent)\n        self.dataset_ref = dataset_ref\n        self.shots = shots\n\n    def evaluation_name(self) -&gt; str:  # noqa: D102\n        prefix = self.dataset_evaluation_name if self.dataset_evaluation_name else \"mmlu\"\n        return f\"{prefix}_{self.shots}shots\"\n\n    def compatible_datasets(self) -&gt; List[str]:  # noqa: D102\n        return [\"mmlu\"]\n\n    def solve(self, datum: dict) -&gt; bool:  # noqa: D102\n        datum = MMLUDatum(**datum).model_dump()\n\n        choices = [\"A\", \"B\", \"C\", \"D\"]\n        example_problems_indices = list(range(0, 5 * self.shots, 5))\n        example_problems = list(\n            map(\n                lambda d: MMLUDatum(**d).model_dump(),\n                [self.dataset_ref[\"dev\"][i] for i in example_problems_indices],\n            )\n        )\n        base_prompt = Template(open(PROMPTS_FOLDER / \"mmlu_verbose_answer.j2\").read(), trim_blocks=True).render(\n            example_problems=example_problems,\n            challenge_problem=datum,\n            choices=choices,\n        )\n\n        response = self.start_inference_session(\"\").run_task(base_prompt)\n\n        ## Extract the answer from the response\n        extract_answer_prompt = Template(\n            open(PROMPTS_FOLDER / \"mmlu_extract_answer.j2\").read(), trim_blocks=True\n        ).render(\n            challenge_problem=datum,\n            answer_text=response,\n            choices=choices,\n        )\n        response = self.start_inference_session(\"\").run_task(extract_answer_prompt)\n\n        try:\n            answer = choices.index(response)\n            return bool(answer == datum[\"answer\"])\n        except Exception:\n            print(\"Failed to parse answer\")\n            return False\n</code></pre>"},{"location":"api/#nearai.tests","title":"tests","text":""},{"location":"api/#nearai.tests.test_provider_models","title":"test_provider_models","text":""},{"location":"api/#nearai.tests.test_provider_models.TestMatchProviderModel","title":"TestMatchProviderModel","text":"<p>               Bases: <code>TestCase</code></p> <p>Unit tests for get_provider_namespaced_model.</p> Source code in <code>nearai/tests/test_provider_models.py</code> <pre><code>class TestMatchProviderModel(unittest.TestCase):\n    \"\"\"Unit tests for get_provider_namespaced_model.\"\"\"\n\n    def __init__(self, method_name=\"runTest\"):  # noqa: D107\n        super().__init__(method_name)\n        self.provider_models = ProviderModels(CONFIG.get_client_config())\n\n    def test_fireworks(self):  # noqa: D102\n        self.assertEqual(\n            self.provider_models.match_provider_model(\"fireworks::accounts/yi-01-ai/models/yi-large\"),\n            (\"fireworks\", \"fireworks::accounts/yi-01-ai/models/yi-large\"),\n        )\n        self.assertEqual(\n            self.provider_models.match_provider_model(\"accounts/yi-01-ai/models/yi-large\"),\n            (\"fireworks\", \"fireworks::accounts/yi-01-ai/models/yi-large\"),\n        )\n        self.assertEqual(\n            self.provider_models.match_provider_model(\"llama-v3-70b-instruct\"),\n            (\"fireworks\", \"fireworks::accounts/fireworks/models/llama-v3-70b-instruct\"),\n        )\n        self.assertEqual(\n            self.provider_models.match_provider_model(\"yi-01-ai/yi-large\"),\n            (\"fireworks\", \"fireworks::accounts/yi-01-ai/models/yi-large\"),\n        )\n\n    def test_hyperbolic(self):  # noqa: D102\n        self.assertEqual(\n            self.provider_models.match_provider_model(\"hyperbolic::StableDiffusion\"),\n            (\"hyperbolic\", \"hyperbolic::StableDiffusion\"),\n        )\n        self.assertEqual(\n            self.provider_models.match_provider_model(\"hyperbolic::meta-llama/Meta-Llama-3.1-70B-Instruct\"),\n            (\"hyperbolic\", \"hyperbolic::meta-llama/Meta-Llama-3.1-70B-Instruct\"),\n        )\n        self.assertEqual(\n            self.provider_models.match_provider_model(\"hyperbolic::Meta-Llama-3.1-70B-Instruct\"),\n            (\"hyperbolic\", \"hyperbolic::meta-llama/Meta-Llama-3.1-70B-Instruct\"),\n        )\n\n    def test_registry_with_multiple_providers(self):  # noqa: D102\n        self.assertEqual(\n            self.provider_models.match_provider_model(\"llama-3.1-70b-instruct\"),\n            (\"fireworks\", \"fireworks::accounts/fireworks/models/llama-v3p1-70b-instruct\"),\n        )\n        self.assertEqual(\n            self.provider_models.match_provider_model(\"llama-3.1-70b-instruct\", provider=\"hyperbolic\"),\n            (\"hyperbolic\", \"hyperbolic::meta-llama/Meta-Llama-3.1-70B-Instruct\"),\n        )\n        self.assertEqual(\n            self.provider_models.match_provider_model(\"near.ai/llama-3.1-70b-instruct\", provider=\"hyperbolic\"),\n            (\"hyperbolic\", \"hyperbolic::meta-llama/Meta-Llama-3.1-70B-Instruct\"),\n        )\n</code></pre>"},{"location":"api/#nearai.tests.test_registry_cli","title":"test_registry_cli","text":""},{"location":"api/#nearai.tests.test_registry_cli.TestRegistryCliUpload","title":"TestRegistryCliUpload","text":"<p>Tests for the RegistryCli.upload method.</p> Source code in <code>nearai/tests/test_registry_cli.py</code> <pre><code>class TestRegistryCliUpload:\n    \"\"\"Tests for the RegistryCli.upload method.\"\"\"\n\n    def test_successful_upload(self, mock_registry, mock_config, tmp_path):\n        \"\"\"Test successful upload when version doesn't exist.\"\"\"\n        # Mock the helper functions\n        with (\n            patch(\"nearai.cli.load_and_validate_metadata\") as mock_load_metadata,\n            patch(\"nearai.cli.check_version_exists\") as mock_check_version,\n        ):\n            # Setup mocks\n            mock_load_metadata.return_value = ({\"name\": \"test-agent\", \"version\": \"0.0.1\"}, None)\n            mock_check_version.return_value = (False, None)\n            mock_registry.upload.return_value = EntryLocation(namespace=\"user\", name=\"test-agent\", version=\"0.0.1\")\n\n            # Call the method\n            cli = RegistryCli()\n            result = cli.upload(str(tmp_path))\n\n            # Assertions\n            assert result is not None\n            assert result.namespace == \"user\"\n            assert result.name == \"test-agent\"\n            assert result.version == \"0.0.1\"\n            mock_registry.upload.assert_called_once()\n\n    def test_version_already_exists(self, mock_registry, mock_config, tmp_path, capsys):\n        \"\"\"Test upload failure when version already exists.\"\"\"\n        # Mock the helper functions\n        with (\n            patch(\"nearai.cli.load_and_validate_metadata\") as mock_load_metadata,\n            patch(\"nearai.cli.check_version_exists\") as mock_check_version,\n        ):\n            # Setup mocks\n            mock_load_metadata.return_value = ({\"name\": \"test-agent\", \"version\": \"0.0.1\"}, None)\n            mock_check_version.return_value = (True, None)\n\n            # Call the method\n            cli = RegistryCli()\n            result = cli.upload(str(tmp_path))\n\n            # Assertions\n            assert result is None\n            captured = capsys.readouterr()\n            # Check for the new Rich-formatted output\n            assert \"Version 0.0.1 already exists\" in captured.out\n            assert \"Version Conflict\" in captured.out\n            assert \"To upload a new version\" in captured.out\n            assert \"--bump\" in captured.out\n            assert \"--minor-bump\" in captured.out\n            assert \"--major-bump\" in captured.out\n            mock_registry.upload.assert_not_called()\n\n    def test_metadata_file_not_found(self, mock_registry, mock_config, tmp_path, capsys):\n        \"\"\"Test upload failure when metadata.json is missing.\"\"\"\n        # Mock the helper function\n        with patch(\"nearai.cli.load_and_validate_metadata\") as mock_load_metadata:\n            # Setup mock\n            mock_load_metadata.return_value = (None, \"Error: metadata.json not found\")\n\n            # Call the method\n            cli = RegistryCli()\n            result = cli.upload(str(tmp_path))\n\n            # Assertions\n            assert result is None\n            captured = capsys.readouterr()\n            assert \"Error: metadata.json not found\" in captured.out\n            mock_registry.upload.assert_not_called()\n\n    def test_invalid_json_metadata(self, mock_registry, mock_config, tmp_path, capsys):\n        \"\"\"Test upload failure when metadata.json is not valid JSON.\"\"\"\n        # Mock the helper function\n        with patch(\"nearai.cli.load_and_validate_metadata\") as mock_load_metadata:\n            # Setup mock\n            mock_load_metadata.return_value = (None, \"Error: metadata.json is not a valid JSON file\")\n\n            # Call the method\n            cli = RegistryCli()\n            result = cli.upload(str(tmp_path))\n\n            # Assertions\n            assert result is None\n            captured = capsys.readouterr()\n            assert \"Error: metadata.json is not a valid JSON file\" in captured.out\n            mock_registry.upload.assert_not_called()\n\n    def test_missing_required_fields(self, mock_registry, mock_config, tmp_path, capsys):\n        \"\"\"Test upload failure when required fields are missing in metadata.json.\"\"\"\n        # Mock the helper function\n        with patch(\"nearai.cli.load_and_validate_metadata\") as mock_load_metadata:\n            # Setup mock\n            mock_load_metadata.return_value = (None, \"Error: metadata.json must contain 'name' and 'version' fields\")\n\n            # Call the method\n            cli = RegistryCli()\n            result = cli.upload(str(tmp_path))\n\n            # Assertions\n            assert result is None\n            captured = capsys.readouterr()\n            assert \"Error: metadata.json must contain 'name' and 'version' fields\" in captured.out\n            mock_registry.upload.assert_not_called()\n\n    def test_not_logged_in(self, mock_registry, mock_config, tmp_path, capsys):\n        \"\"\"Test upload failure when user is not logged in.\"\"\"\n        # Mock the helper function\n        with patch(\"nearai.cli.load_and_validate_metadata\") as mock_load_metadata:\n            # Setup mock\n            mock_load_metadata.return_value = (None, \"Please login with `nearai login` before uploading\")\n\n            # Call the method\n            cli = RegistryCli()\n            result = cli.upload(str(tmp_path))\n\n            # Assertions\n            assert result is None\n            captured = capsys.readouterr()\n            assert \"Please login with `nearai login` before uploading\" in captured.out\n            mock_registry.upload.assert_not_called()\n\n    def test_other_registry_error(self, mock_registry, mock_config, tmp_path, capsys):\n        \"\"\"Test upload failure when an unexpected error occurs during registry info check.\"\"\"\n        # Mock the helper functions\n        with (\n            patch(\"nearai.cli.load_and_validate_metadata\") as mock_load_metadata,\n            patch(\"nearai.cli.check_version_exists\") as mock_check_version,\n        ):\n            # Setup mocks\n            mock_load_metadata.return_value = ({\"name\": \"test-agent\", \"version\": \"0.0.1\"}, None)\n            mock_check_version.return_value = (False, \"Error checking registry: Connection failed\")\n\n            # Call the method\n            cli = RegistryCli()\n            result = cli.upload(str(tmp_path))\n\n            # Assertions\n            assert result is None\n            captured = capsys.readouterr()\n            assert \"Error checking registry: Connection failed\" in captured.out\n            mock_registry.upload.assert_not_called()\n\n    def test_auto_bump_version(self, mock_registry, mock_config, tmp_path, capsys):\n        \"\"\"Test auto-increment feature when version already exists.\"\"\"\n        # Create a real metadata.json for this test\n        metadata = {\"name\": \"test-agent\", \"version\": \"0.0.1\"}\n        metadata_path = tmp_path / \"metadata.json\"\n        with open(metadata_path, \"w\") as f:\n            json.dump(metadata, f)\n\n        # Mock the helper functions\n        with (\n            patch(\"nearai.cli.load_and_validate_metadata\") as mock_load_metadata,\n            patch(\"nearai.cli.check_version_exists\") as mock_check_version,\n            patch(\"nearai.cli.increment_version_by_type\") as mock_increment,\n        ):\n            # Setup mocks for first check (version exists) and second check (new version doesn't exist)\n            mock_load_metadata.return_value = (metadata, None)\n            mock_check_version.side_effect = [(True, None), (False, None)]\n            mock_increment.return_value = \"0.0.2\"\n            mock_registry.upload.return_value = EntryLocation(namespace=\"user\", name=\"test-agent\", version=\"0.0.2\")\n\n            # Call the method with bump=True\n            cli = RegistryCli()\n            result = cli.upload(str(tmp_path), bump=True)\n\n            # Assertions\n            assert result is not None\n            assert result.version == \"0.0.2\"\n            mock_increment.assert_called_once_with(\"0.0.1\", \"patch\")\n            mock_registry.upload.assert_called_once()\n\n            # Check that metadata.json was updated\n            with open(metadata_path, \"r\") as f:\n                updated_metadata = json.load(f)\n                assert updated_metadata[\"version\"] == \"0.0.2\"\n\n            # Check console output for the new Rich-formatted panel\n            captured = capsys.readouterr()\n            assert \"Bump\" in captured.out\n            assert \"Previous version: 0.0.1\" in captured.out\n            assert \"New version:\" in captured.out\n            assert \"0.0.2\" in captured.out\n            assert \"Increment type: patch\" in captured.out\n\n    def test_minor_bump_version(self, mock_registry, mock_config, tmp_path, capsys):\n        \"\"\"Test minor bump feature when version already exists.\"\"\"\n        # Create a real metadata.json for this test\n        metadata = {\"name\": \"test-agent\", \"version\": \"0.0.1\"}\n        metadata_path = tmp_path / \"metadata.json\"\n        with open(metadata_path, \"w\") as f:\n            json.dump(metadata, f)\n\n        # Mock the helper functions\n        with (\n            patch(\"nearai.cli.load_and_validate_metadata\") as mock_load_metadata,\n            patch(\"nearai.cli.check_version_exists\") as mock_check_version,\n            patch(\"nearai.cli.increment_version_by_type\") as mock_increment,\n        ):\n            # Setup mocks for first check (version exists) and second check (new version doesn't exist)\n            mock_load_metadata.return_value = (metadata, None)\n            mock_check_version.side_effect = [(True, None), (False, None)]\n            mock_increment.return_value = \"0.1.0\"\n            mock_registry.upload.return_value = EntryLocation(namespace=\"user\", name=\"test-agent\", version=\"0.1.0\")\n\n            # Call the method with minor_bump=True\n            cli = RegistryCli()\n            result = cli.upload(str(tmp_path), minor_bump=True)\n\n            # Assertions\n            assert result is not None\n            assert result.version == \"0.1.0\"\n            mock_increment.assert_called_once_with(\"0.0.1\", \"minor\")\n            mock_registry.upload.assert_called_once()\n\n            # Check that metadata.json was updated\n            with open(metadata_path, \"r\") as f:\n                updated_metadata = json.load(f)\n                assert updated_metadata[\"version\"] == \"0.1.0\"\n\n            # Check console output for the new Rich-formatted panel\n            captured = capsys.readouterr()\n            assert \"Bump\" in captured.out\n            assert \"Previous version: 0.0.1\" in captured.out\n            assert \"New version:\" in captured.out\n            assert \"0.1.0\" in captured.out\n            assert \"Increment type: minor\" in captured.out\n\n    def test_major_bump_version(self, mock_registry, mock_config, tmp_path, capsys):\n        \"\"\"Test major bump feature when version already exists.\"\"\"\n        # Create a real metadata.json for this test\n        metadata = {\"name\": \"test-agent\", \"version\": \"0.0.1\"}\n        metadata_path = tmp_path / \"metadata.json\"\n        with open(metadata_path, \"w\") as f:\n            json.dump(metadata, f)\n\n        # Mock the helper functions\n        with (\n            patch(\"nearai.cli.load_and_validate_metadata\") as mock_load_metadata,\n            patch(\"nearai.cli.check_version_exists\") as mock_check_version,\n            patch(\"nearai.cli.increment_version_by_type\") as mock_increment,\n        ):\n            # Setup mocks for first check (version exists) and second check (new version doesn't exist)\n            mock_load_metadata.return_value = (metadata, None)\n            mock_check_version.side_effect = [(True, None), (False, None)]\n            mock_increment.return_value = \"1.0.0\"\n            mock_registry.upload.return_value = EntryLocation(namespace=\"user\", name=\"test-agent\", version=\"1.0.0\")\n\n            # Call the method with major_bump=True\n            cli = RegistryCli()\n            result = cli.upload(str(tmp_path), major_bump=True)\n\n            # Assertions\n            assert result is not None\n            assert result.version == \"1.0.0\"\n            mock_increment.assert_called_once_with(\"0.0.1\", \"major\")\n            mock_registry.upload.assert_called_once()\n\n            # Check that metadata.json was updated\n            with open(metadata_path, \"r\") as f:\n                updated_metadata = json.load(f)\n                assert updated_metadata[\"version\"] == \"1.0.0\"\n\n            # Check console output for the new Rich-formatted panel\n            captured = capsys.readouterr()\n            assert \"Bump\" in captured.out\n            assert \"Previous version: 0.0.1\" in captured.out\n            assert \"New version:\" in captured.out\n            assert \"1.0.0\" in captured.out\n            assert \"Increment type: major\" in captured.out\n\n    def test_pep440_version_validation(self, mock_registry, mock_config, tmp_path):\n        \"\"\"Test that version validation follows PEP 440 standards.\"\"\"\n        # Import the actual validation function\n        from packaging.version import InvalidVersion, Version\n\n        from nearai.cli_helpers import validate_version\n\n        # Test valid versions according to PEP 440\n        valid_versions = [\n            \"1.0.0\",\n            \"0.1.0\",\n            \"0.0.1\",  # Simple versions\n            \"1.0.0rc1\",\n            \"1.0.0a1\",\n            \"1.0.0b1\",  # Pre-releases\n            \"2.0.0.dev1\",  # Dev releases\n            \"1.0.0.post1\",  # Post releases\n            \"1!1.0.0\",  # With epoch\n            \"1.0.0+local.1\",  # Local version\n            \"1.0\",  # Implicit zero\n            \"1.0.0.0.0\",  # Many segments (valid in PEP 440)\n            \"01.02.03\",  # Leading zeros (valid in PEP 440)\n            \"1.0a\",\n            \"1.0.post\",\n            \"1.0.dev\",  # Optional numbers in pre/post/dev\n        ]\n\n        for version in valid_versions:\n            # Verify with packaging.version first\n            try:\n                Version(version)\n                is_valid_pep440 = True\n            except InvalidVersion:\n                is_valid_pep440 = False\n\n            assert is_valid_pep440, f\"Version {version} should be valid according to PEP 440\"\n\n            # Now test our validation function\n            is_valid, error_msg = validate_version(version)\n            assert is_valid, f\"Valid version {version} was rejected with error: {error_msg}\"\n\n        # Test invalid versions\n        invalid_versions = [\n            # Non-Numeric Versions\n            \"version1.0.0\",  # Arbitrary text is not allowed\n            \"hithere\",\n            \"12-212.23\",\n            \"1.0_final\",  # Underscore is not allowed in this context\n            \"1..0\",  # Empty segments are not allowed\n            \"1.0.\",  # Trailing dot is not allowed\n        ]\n\n        for version in invalid_versions:\n            # Verify with packaging.version first\n            try:\n                Version(version)\n                is_valid_pep440 = True\n            except InvalidVersion:\n                is_valid_pep440 = False\n\n            assert not is_valid_pep440, f\"Version {version} should be invalid according to PEP 440\"\n\n            # Now test our validation function\n            is_valid, error_msg = validate_version(version)\n            assert not is_valid, f\"Invalid version {version} was accepted\"\n            assert \"Invalid version format\" in error_msg or \"not a valid version\" in error_msg\n\n    def test_auto_increment_version(self, mock_registry, mock_config, tmp_path, capsys):\n        \"\"\"Test auto-increment feature when version already exists.\"\"\"\n        # Create a real metadata.json for this test\n        metadata = {\"name\": \"test-agent\", \"version\": \"0.0.1\"}\n        metadata_path = tmp_path / \"metadata.json\"\n        with open(metadata_path, \"w\") as f:\n            json.dump(metadata, f)\n\n        # Mock the helper functions\n        with (\n            patch(\"nearai.cli.load_and_validate_metadata\") as mock_load_metadata,\n            patch(\"nearai.cli.check_version_exists\") as mock_check_version,\n            patch(\"nearai.cli.increment_version_by_type\") as mock_increment,\n        ):\n            # Setup mocks for first check (version exists) and second check (new version doesn't exist)\n            mock_load_metadata.return_value = (metadata, None)\n            mock_check_version.side_effect = [(True, None), (False, None)]\n            mock_increment.return_value = \"0.0.2\"\n            mock_registry.upload.return_value = EntryLocation(namespace=\"user\", name=\"test-agent\", version=\"0.0.2\")\n\n            # Call the method with bump=True\n            cli = RegistryCli()\n            result = cli.upload(str(tmp_path), bump=True)\n\n            # Assertions\n            assert result is not None\n            assert result.version == \"0.0.2\"\n            mock_increment.assert_called_once_with(\"0.0.1\", \"patch\")\n            mock_registry.upload.assert_called_once()\n\n            # Check that metadata.json was updated\n            with open(metadata_path, \"r\") as f:\n                updated_metadata = json.load(f)\n                assert updated_metadata[\"version\"] == \"0.0.2\"\n\n            # Check console output for the new Rich-formatted panel\n            captured = capsys.readouterr()\n            assert \"Bump\" in captured.out\n            assert \"Previous version: 0.0.1\" in captured.out\n            assert \"New version:\" in captured.out\n            assert \"0.0.2\" in captured.out\n            assert \"Increment type: patch\" in captured.out\n</code></pre>"},{"location":"api/#nearai.tests.test_registry_cli.TestRegistryCliUpload.test_auto_bump_version","title":"test_auto_bump_version","text":"<pre><code>test_auto_bump_version(mock_registry, mock_config, tmp_path, capsys)\n</code></pre> <p>Test auto-increment feature when version already exists.</p> Source code in <code>nearai/tests/test_registry_cli.py</code> <pre><code>def test_auto_bump_version(self, mock_registry, mock_config, tmp_path, capsys):\n    \"\"\"Test auto-increment feature when version already exists.\"\"\"\n    # Create a real metadata.json for this test\n    metadata = {\"name\": \"test-agent\", \"version\": \"0.0.1\"}\n    metadata_path = tmp_path / \"metadata.json\"\n    with open(metadata_path, \"w\") as f:\n        json.dump(metadata, f)\n\n    # Mock the helper functions\n    with (\n        patch(\"nearai.cli.load_and_validate_metadata\") as mock_load_metadata,\n        patch(\"nearai.cli.check_version_exists\") as mock_check_version,\n        patch(\"nearai.cli.increment_version_by_type\") as mock_increment,\n    ):\n        # Setup mocks for first check (version exists) and second check (new version doesn't exist)\n        mock_load_metadata.return_value = (metadata, None)\n        mock_check_version.side_effect = [(True, None), (False, None)]\n        mock_increment.return_value = \"0.0.2\"\n        mock_registry.upload.return_value = EntryLocation(namespace=\"user\", name=\"test-agent\", version=\"0.0.2\")\n\n        # Call the method with bump=True\n        cli = RegistryCli()\n        result = cli.upload(str(tmp_path), bump=True)\n\n        # Assertions\n        assert result is not None\n        assert result.version == \"0.0.2\"\n        mock_increment.assert_called_once_with(\"0.0.1\", \"patch\")\n        mock_registry.upload.assert_called_once()\n\n        # Check that metadata.json was updated\n        with open(metadata_path, \"r\") as f:\n            updated_metadata = json.load(f)\n            assert updated_metadata[\"version\"] == \"0.0.2\"\n\n        # Check console output for the new Rich-formatted panel\n        captured = capsys.readouterr()\n        assert \"Bump\" in captured.out\n        assert \"Previous version: 0.0.1\" in captured.out\n        assert \"New version:\" in captured.out\n        assert \"0.0.2\" in captured.out\n        assert \"Increment type: patch\" in captured.out\n</code></pre>"},{"location":"api/#nearai.tests.test_registry_cli.TestRegistryCliUpload.test_auto_increment_version","title":"test_auto_increment_version","text":"<pre><code>test_auto_increment_version(mock_registry, mock_config, tmp_path, capsys)\n</code></pre> <p>Test auto-increment feature when version already exists.</p> Source code in <code>nearai/tests/test_registry_cli.py</code> <pre><code>def test_auto_increment_version(self, mock_registry, mock_config, tmp_path, capsys):\n    \"\"\"Test auto-increment feature when version already exists.\"\"\"\n    # Create a real metadata.json for this test\n    metadata = {\"name\": \"test-agent\", \"version\": \"0.0.1\"}\n    metadata_path = tmp_path / \"metadata.json\"\n    with open(metadata_path, \"w\") as f:\n        json.dump(metadata, f)\n\n    # Mock the helper functions\n    with (\n        patch(\"nearai.cli.load_and_validate_metadata\") as mock_load_metadata,\n        patch(\"nearai.cli.check_version_exists\") as mock_check_version,\n        patch(\"nearai.cli.increment_version_by_type\") as mock_increment,\n    ):\n        # Setup mocks for first check (version exists) and second check (new version doesn't exist)\n        mock_load_metadata.return_value = (metadata, None)\n        mock_check_version.side_effect = [(True, None), (False, None)]\n        mock_increment.return_value = \"0.0.2\"\n        mock_registry.upload.return_value = EntryLocation(namespace=\"user\", name=\"test-agent\", version=\"0.0.2\")\n\n        # Call the method with bump=True\n        cli = RegistryCli()\n        result = cli.upload(str(tmp_path), bump=True)\n\n        # Assertions\n        assert result is not None\n        assert result.version == \"0.0.2\"\n        mock_increment.assert_called_once_with(\"0.0.1\", \"patch\")\n        mock_registry.upload.assert_called_once()\n\n        # Check that metadata.json was updated\n        with open(metadata_path, \"r\") as f:\n            updated_metadata = json.load(f)\n            assert updated_metadata[\"version\"] == \"0.0.2\"\n\n        # Check console output for the new Rich-formatted panel\n        captured = capsys.readouterr()\n        assert \"Bump\" in captured.out\n        assert \"Previous version: 0.0.1\" in captured.out\n        assert \"New version:\" in captured.out\n        assert \"0.0.2\" in captured.out\n        assert \"Increment type: patch\" in captured.out\n</code></pre>"},{"location":"api/#nearai.tests.test_registry_cli.TestRegistryCliUpload.test_invalid_json_metadata","title":"test_invalid_json_metadata","text":"<pre><code>test_invalid_json_metadata(mock_registry, mock_config, tmp_path, capsys)\n</code></pre> <p>Test upload failure when metadata.json is not valid JSON.</p> Source code in <code>nearai/tests/test_registry_cli.py</code> <pre><code>def test_invalid_json_metadata(self, mock_registry, mock_config, tmp_path, capsys):\n    \"\"\"Test upload failure when metadata.json is not valid JSON.\"\"\"\n    # Mock the helper function\n    with patch(\"nearai.cli.load_and_validate_metadata\") as mock_load_metadata:\n        # Setup mock\n        mock_load_metadata.return_value = (None, \"Error: metadata.json is not a valid JSON file\")\n\n        # Call the method\n        cli = RegistryCli()\n        result = cli.upload(str(tmp_path))\n\n        # Assertions\n        assert result is None\n        captured = capsys.readouterr()\n        assert \"Error: metadata.json is not a valid JSON file\" in captured.out\n        mock_registry.upload.assert_not_called()\n</code></pre>"},{"location":"api/#nearai.tests.test_registry_cli.TestRegistryCliUpload.test_major_bump_version","title":"test_major_bump_version","text":"<pre><code>test_major_bump_version(mock_registry, mock_config, tmp_path, capsys)\n</code></pre> <p>Test major bump feature when version already exists.</p> Source code in <code>nearai/tests/test_registry_cli.py</code> <pre><code>def test_major_bump_version(self, mock_registry, mock_config, tmp_path, capsys):\n    \"\"\"Test major bump feature when version already exists.\"\"\"\n    # Create a real metadata.json for this test\n    metadata = {\"name\": \"test-agent\", \"version\": \"0.0.1\"}\n    metadata_path = tmp_path / \"metadata.json\"\n    with open(metadata_path, \"w\") as f:\n        json.dump(metadata, f)\n\n    # Mock the helper functions\n    with (\n        patch(\"nearai.cli.load_and_validate_metadata\") as mock_load_metadata,\n        patch(\"nearai.cli.check_version_exists\") as mock_check_version,\n        patch(\"nearai.cli.increment_version_by_type\") as mock_increment,\n    ):\n        # Setup mocks for first check (version exists) and second check (new version doesn't exist)\n        mock_load_metadata.return_value = (metadata, None)\n        mock_check_version.side_effect = [(True, None), (False, None)]\n        mock_increment.return_value = \"1.0.0\"\n        mock_registry.upload.return_value = EntryLocation(namespace=\"user\", name=\"test-agent\", version=\"1.0.0\")\n\n        # Call the method with major_bump=True\n        cli = RegistryCli()\n        result = cli.upload(str(tmp_path), major_bump=True)\n\n        # Assertions\n        assert result is not None\n        assert result.version == \"1.0.0\"\n        mock_increment.assert_called_once_with(\"0.0.1\", \"major\")\n        mock_registry.upload.assert_called_once()\n\n        # Check that metadata.json was updated\n        with open(metadata_path, \"r\") as f:\n            updated_metadata = json.load(f)\n            assert updated_metadata[\"version\"] == \"1.0.0\"\n\n        # Check console output for the new Rich-formatted panel\n        captured = capsys.readouterr()\n        assert \"Bump\" in captured.out\n        assert \"Previous version: 0.0.1\" in captured.out\n        assert \"New version:\" in captured.out\n        assert \"1.0.0\" in captured.out\n        assert \"Increment type: major\" in captured.out\n</code></pre>"},{"location":"api/#nearai.tests.test_registry_cli.TestRegistryCliUpload.test_metadata_file_not_found","title":"test_metadata_file_not_found","text":"<pre><code>test_metadata_file_not_found(mock_registry, mock_config, tmp_path, capsys)\n</code></pre> <p>Test upload failure when metadata.json is missing.</p> Source code in <code>nearai/tests/test_registry_cli.py</code> <pre><code>def test_metadata_file_not_found(self, mock_registry, mock_config, tmp_path, capsys):\n    \"\"\"Test upload failure when metadata.json is missing.\"\"\"\n    # Mock the helper function\n    with patch(\"nearai.cli.load_and_validate_metadata\") as mock_load_metadata:\n        # Setup mock\n        mock_load_metadata.return_value = (None, \"Error: metadata.json not found\")\n\n        # Call the method\n        cli = RegistryCli()\n        result = cli.upload(str(tmp_path))\n\n        # Assertions\n        assert result is None\n        captured = capsys.readouterr()\n        assert \"Error: metadata.json not found\" in captured.out\n        mock_registry.upload.assert_not_called()\n</code></pre>"},{"location":"api/#nearai.tests.test_registry_cli.TestRegistryCliUpload.test_minor_bump_version","title":"test_minor_bump_version","text":"<pre><code>test_minor_bump_version(mock_registry, mock_config, tmp_path, capsys)\n</code></pre> <p>Test minor bump feature when version already exists.</p> Source code in <code>nearai/tests/test_registry_cli.py</code> <pre><code>def test_minor_bump_version(self, mock_registry, mock_config, tmp_path, capsys):\n    \"\"\"Test minor bump feature when version already exists.\"\"\"\n    # Create a real metadata.json for this test\n    metadata = {\"name\": \"test-agent\", \"version\": \"0.0.1\"}\n    metadata_path = tmp_path / \"metadata.json\"\n    with open(metadata_path, \"w\") as f:\n        json.dump(metadata, f)\n\n    # Mock the helper functions\n    with (\n        patch(\"nearai.cli.load_and_validate_metadata\") as mock_load_metadata,\n        patch(\"nearai.cli.check_version_exists\") as mock_check_version,\n        patch(\"nearai.cli.increment_version_by_type\") as mock_increment,\n    ):\n        # Setup mocks for first check (version exists) and second check (new version doesn't exist)\n        mock_load_metadata.return_value = (metadata, None)\n        mock_check_version.side_effect = [(True, None), (False, None)]\n        mock_increment.return_value = \"0.1.0\"\n        mock_registry.upload.return_value = EntryLocation(namespace=\"user\", name=\"test-agent\", version=\"0.1.0\")\n\n        # Call the method with minor_bump=True\n        cli = RegistryCli()\n        result = cli.upload(str(tmp_path), minor_bump=True)\n\n        # Assertions\n        assert result is not None\n        assert result.version == \"0.1.0\"\n        mock_increment.assert_called_once_with(\"0.0.1\", \"minor\")\n        mock_registry.upload.assert_called_once()\n\n        # Check that metadata.json was updated\n        with open(metadata_path, \"r\") as f:\n            updated_metadata = json.load(f)\n            assert updated_metadata[\"version\"] == \"0.1.0\"\n\n        # Check console output for the new Rich-formatted panel\n        captured = capsys.readouterr()\n        assert \"Bump\" in captured.out\n        assert \"Previous version: 0.0.1\" in captured.out\n        assert \"New version:\" in captured.out\n        assert \"0.1.0\" in captured.out\n        assert \"Increment type: minor\" in captured.out\n</code></pre>"},{"location":"api/#nearai.tests.test_registry_cli.TestRegistryCliUpload.test_missing_required_fields","title":"test_missing_required_fields","text":"<pre><code>test_missing_required_fields(mock_registry, mock_config, tmp_path, capsys)\n</code></pre> <p>Test upload failure when required fields are missing in metadata.json.</p> Source code in <code>nearai/tests/test_registry_cli.py</code> <pre><code>def test_missing_required_fields(self, mock_registry, mock_config, tmp_path, capsys):\n    \"\"\"Test upload failure when required fields are missing in metadata.json.\"\"\"\n    # Mock the helper function\n    with patch(\"nearai.cli.load_and_validate_metadata\") as mock_load_metadata:\n        # Setup mock\n        mock_load_metadata.return_value = (None, \"Error: metadata.json must contain 'name' and 'version' fields\")\n\n        # Call the method\n        cli = RegistryCli()\n        result = cli.upload(str(tmp_path))\n\n        # Assertions\n        assert result is None\n        captured = capsys.readouterr()\n        assert \"Error: metadata.json must contain 'name' and 'version' fields\" in captured.out\n        mock_registry.upload.assert_not_called()\n</code></pre>"},{"location":"api/#nearai.tests.test_registry_cli.TestRegistryCliUpload.test_not_logged_in","title":"test_not_logged_in","text":"<pre><code>test_not_logged_in(mock_registry, mock_config, tmp_path, capsys)\n</code></pre> <p>Test upload failure when user is not logged in.</p> Source code in <code>nearai/tests/test_registry_cli.py</code> <pre><code>def test_not_logged_in(self, mock_registry, mock_config, tmp_path, capsys):\n    \"\"\"Test upload failure when user is not logged in.\"\"\"\n    # Mock the helper function\n    with patch(\"nearai.cli.load_and_validate_metadata\") as mock_load_metadata:\n        # Setup mock\n        mock_load_metadata.return_value = (None, \"Please login with `nearai login` before uploading\")\n\n        # Call the method\n        cli = RegistryCli()\n        result = cli.upload(str(tmp_path))\n\n        # Assertions\n        assert result is None\n        captured = capsys.readouterr()\n        assert \"Please login with `nearai login` before uploading\" in captured.out\n        mock_registry.upload.assert_not_called()\n</code></pre>"},{"location":"api/#nearai.tests.test_registry_cli.TestRegistryCliUpload.test_other_registry_error","title":"test_other_registry_error","text":"<pre><code>test_other_registry_error(mock_registry, mock_config, tmp_path, capsys)\n</code></pre> <p>Test upload failure when an unexpected error occurs during registry info check.</p> Source code in <code>nearai/tests/test_registry_cli.py</code> <pre><code>def test_other_registry_error(self, mock_registry, mock_config, tmp_path, capsys):\n    \"\"\"Test upload failure when an unexpected error occurs during registry info check.\"\"\"\n    # Mock the helper functions\n    with (\n        patch(\"nearai.cli.load_and_validate_metadata\") as mock_load_metadata,\n        patch(\"nearai.cli.check_version_exists\") as mock_check_version,\n    ):\n        # Setup mocks\n        mock_load_metadata.return_value = ({\"name\": \"test-agent\", \"version\": \"0.0.1\"}, None)\n        mock_check_version.return_value = (False, \"Error checking registry: Connection failed\")\n\n        # Call the method\n        cli = RegistryCli()\n        result = cli.upload(str(tmp_path))\n\n        # Assertions\n        assert result is None\n        captured = capsys.readouterr()\n        assert \"Error checking registry: Connection failed\" in captured.out\n        mock_registry.upload.assert_not_called()\n</code></pre>"},{"location":"api/#nearai.tests.test_registry_cli.TestRegistryCliUpload.test_pep440_version_validation","title":"test_pep440_version_validation","text":"<pre><code>test_pep440_version_validation(mock_registry, mock_config, tmp_path)\n</code></pre> <p>Test that version validation follows PEP 440 standards.</p> Source code in <code>nearai/tests/test_registry_cli.py</code> <pre><code>def test_pep440_version_validation(self, mock_registry, mock_config, tmp_path):\n    \"\"\"Test that version validation follows PEP 440 standards.\"\"\"\n    # Import the actual validation function\n    from packaging.version import InvalidVersion, Version\n\n    from nearai.cli_helpers import validate_version\n\n    # Test valid versions according to PEP 440\n    valid_versions = [\n        \"1.0.0\",\n        \"0.1.0\",\n        \"0.0.1\",  # Simple versions\n        \"1.0.0rc1\",\n        \"1.0.0a1\",\n        \"1.0.0b1\",  # Pre-releases\n        \"2.0.0.dev1\",  # Dev releases\n        \"1.0.0.post1\",  # Post releases\n        \"1!1.0.0\",  # With epoch\n        \"1.0.0+local.1\",  # Local version\n        \"1.0\",  # Implicit zero\n        \"1.0.0.0.0\",  # Many segments (valid in PEP 440)\n        \"01.02.03\",  # Leading zeros (valid in PEP 440)\n        \"1.0a\",\n        \"1.0.post\",\n        \"1.0.dev\",  # Optional numbers in pre/post/dev\n    ]\n\n    for version in valid_versions:\n        # Verify with packaging.version first\n        try:\n            Version(version)\n            is_valid_pep440 = True\n        except InvalidVersion:\n            is_valid_pep440 = False\n\n        assert is_valid_pep440, f\"Version {version} should be valid according to PEP 440\"\n\n        # Now test our validation function\n        is_valid, error_msg = validate_version(version)\n        assert is_valid, f\"Valid version {version} was rejected with error: {error_msg}\"\n\n    # Test invalid versions\n    invalid_versions = [\n        # Non-Numeric Versions\n        \"version1.0.0\",  # Arbitrary text is not allowed\n        \"hithere\",\n        \"12-212.23\",\n        \"1.0_final\",  # Underscore is not allowed in this context\n        \"1..0\",  # Empty segments are not allowed\n        \"1.0.\",  # Trailing dot is not allowed\n    ]\n\n    for version in invalid_versions:\n        # Verify with packaging.version first\n        try:\n            Version(version)\n            is_valid_pep440 = True\n        except InvalidVersion:\n            is_valid_pep440 = False\n\n        assert not is_valid_pep440, f\"Version {version} should be invalid according to PEP 440\"\n\n        # Now test our validation function\n        is_valid, error_msg = validate_version(version)\n        assert not is_valid, f\"Invalid version {version} was accepted\"\n        assert \"Invalid version format\" in error_msg or \"not a valid version\" in error_msg\n</code></pre>"},{"location":"api/#nearai.tests.test_registry_cli.TestRegistryCliUpload.test_successful_upload","title":"test_successful_upload","text":"<pre><code>test_successful_upload(mock_registry, mock_config, tmp_path)\n</code></pre> <p>Test successful upload when version doesn't exist.</p> Source code in <code>nearai/tests/test_registry_cli.py</code> <pre><code>def test_successful_upload(self, mock_registry, mock_config, tmp_path):\n    \"\"\"Test successful upload when version doesn't exist.\"\"\"\n    # Mock the helper functions\n    with (\n        patch(\"nearai.cli.load_and_validate_metadata\") as mock_load_metadata,\n        patch(\"nearai.cli.check_version_exists\") as mock_check_version,\n    ):\n        # Setup mocks\n        mock_load_metadata.return_value = ({\"name\": \"test-agent\", \"version\": \"0.0.1\"}, None)\n        mock_check_version.return_value = (False, None)\n        mock_registry.upload.return_value = EntryLocation(namespace=\"user\", name=\"test-agent\", version=\"0.0.1\")\n\n        # Call the method\n        cli = RegistryCli()\n        result = cli.upload(str(tmp_path))\n\n        # Assertions\n        assert result is not None\n        assert result.namespace == \"user\"\n        assert result.name == \"test-agent\"\n        assert result.version == \"0.0.1\"\n        mock_registry.upload.assert_called_once()\n</code></pre>"},{"location":"api/#nearai.tests.test_registry_cli.TestRegistryCliUpload.test_version_already_exists","title":"test_version_already_exists","text":"<pre><code>test_version_already_exists(mock_registry, mock_config, tmp_path, capsys)\n</code></pre> <p>Test upload failure when version already exists.</p> Source code in <code>nearai/tests/test_registry_cli.py</code> <pre><code>def test_version_already_exists(self, mock_registry, mock_config, tmp_path, capsys):\n    \"\"\"Test upload failure when version already exists.\"\"\"\n    # Mock the helper functions\n    with (\n        patch(\"nearai.cli.load_and_validate_metadata\") as mock_load_metadata,\n        patch(\"nearai.cli.check_version_exists\") as mock_check_version,\n    ):\n        # Setup mocks\n        mock_load_metadata.return_value = ({\"name\": \"test-agent\", \"version\": \"0.0.1\"}, None)\n        mock_check_version.return_value = (True, None)\n\n        # Call the method\n        cli = RegistryCli()\n        result = cli.upload(str(tmp_path))\n\n        # Assertions\n        assert result is None\n        captured = capsys.readouterr()\n        # Check for the new Rich-formatted output\n        assert \"Version 0.0.1 already exists\" in captured.out\n        assert \"Version Conflict\" in captured.out\n        assert \"To upload a new version\" in captured.out\n        assert \"--bump\" in captured.out\n        assert \"--minor-bump\" in captured.out\n        assert \"--major-bump\" in captured.out\n        mock_registry.upload.assert_not_called()\n</code></pre>"},{"location":"api/#nearai.tests.test_registry_cli.mock_config","title":"mock_config","text":"<pre><code>mock_config()\n</code></pre> <p>Mock the CONFIG with auth data.</p> Source code in <code>nearai/tests/test_registry_cli.py</code> <pre><code>@pytest.fixture\ndef mock_config():\n    \"\"\"Mock the CONFIG with auth data.\"\"\"\n    with patch(\"nearai.cli.CONFIG\") as mock_conf:\n        mock_conf.auth = MagicMock()\n        mock_conf.auth.namespace = \"test-namespace\"\n        yield mock_conf\n</code></pre>"},{"location":"api/#nearai.tests.test_registry_cli.mock_registry","title":"mock_registry","text":"<pre><code>mock_registry()\n</code></pre> <p>Mock the registry module.</p> Source code in <code>nearai/tests/test_registry_cli.py</code> <pre><code>@pytest.fixture\ndef mock_registry():\n    \"\"\"Mock the registry module.\"\"\"\n    with patch(\"nearai.cli.registry\") as mock_reg:\n        yield mock_reg\n</code></pre>"},{"location":"api/#nearai.tests.test_registry_cli.temp_agent_dir","title":"temp_agent_dir","text":"<pre><code>temp_agent_dir(tmp_path)\n</code></pre> <p>Create a temporary agent directory with metadata.json.</p> Source code in <code>nearai/tests/test_registry_cli.py</code> <pre><code>@pytest.fixture\ndef temp_agent_dir(tmp_path):\n    \"\"\"Create a temporary agent directory with metadata.json.\"\"\"\n    agent_dir = tmp_path / \"test-namespace\" / \"test-agent\" / \"0.0.1\"\n    agent_dir.mkdir(parents=True)\n\n    # Create metadata.json\n    metadata = {\"name\": \"test-agent\", \"version\": \"0.0.1\", \"description\": \"Test agent\", \"category\": \"agent\"}\n\n    metadata_path = agent_dir / \"metadata.json\"\n    with open(metadata_path, \"w\") as f:\n        json.dump(metadata, f)\n\n    # Create agent.py\n    agent_path = agent_dir / \"agent.py\"\n    with open(agent_path, \"w\") as f:\n        f.write(\"# Test agent\")\n\n    return agent_dir\n</code></pre>"},{"location":"cli/","title":"NEAR AI CLI","text":"<p>NEAR AI CLI allows you to create and deploy agents, train and test models, and more!</p>"},{"location":"cli/#quickstart","title":"Quickstart","text":""},{"location":"cli/#requirements","title":"Requirements","text":"<ul> <li>Python 3.9 - 3.11 (3.12 - 3.13 is NOT supported)</li> <li>NEAR Account</li> </ul> <p>Additionally, we recommend creating a virtual environment to avoid conflicts with other Python packages.</p> uvcondapyenv <pre><code># Create a virtual environment with python 3.11\nuv venv --python 3.11\nsource .venv/bin/activate\n</code></pre> <pre><code># Create a virtual environment with python 3.11\nconda create -n nearai python=3.11\nconda activate nearai\n</code></pre> <pre><code># Install python 3.11\npyenv install 3.11\npyenv local 3.11\n\n# Create a virtual environment\npython -m venv .venv\nsource .venv/bin/activate\n</code></pre>"},{"location":"cli/#installing-near-ai-cli","title":"Installing NEAR AI CLI","text":"piplocal <pre><code>pip install nearai  # OR: python3 -m pip install nearai\n</code></pre> <pre><code># Clone the repository:\ngit clone git@github.com:nearai/nearai.git\ncd nearai\n\n# Install dependencies:\npip install -e .  # OR: python3 -m pip install -e .\n</code></pre> <p>Python version</p> <p>NEAR AI requires python <code>3.9 - 3.11</code>. We recommend you to create a virtual environment to avoid conflicts with other Python packages or globally installing dependencies if installing locally w/ repo. </p>"},{"location":"cli/#account-creation-login","title":"Account Creation &amp; Login","text":"<p>To create a new agent, first login with a NEAR Account. If you don't have one, we recommend creating a free account with Meteor Wallet. </p> <p>After you've created an account run:</p> <pre><code>nearai login # OR nearai login --remote\n</code></pre> <p>Example:</p> <pre><code>$&gt; nearai login\n\nPlease visit the following URL to complete the login process: https://auth.near.ai?message=Welcome+to+NEAR+AI&amp;nonce=&lt;xyzxyzxyzxyzx&gt;&amp;recipient=ai.near&amp;callbackUrl=http%3A%2F%2Flocalhost%3A63130%2Fcapture\n</code></pre> <p>After successfully logging in, you will see a confirmation screen: </p> <p></p> <p>Close this window and return to your terminal. </p> <p>Setup complete! You can now proceed to Agent Quickstart and create your first AI Agent! \ud83d\ude80</p> <p>Other Login Methods</p> <p>If you have already logged in on <code>near-cli</code>, you know your account's private key, or you have the credentials on another device, you can use the following commands to login:</p> <pre><code>### Login with NEAR Account ID Only\nnearai login --accountId name.near\n\n### Login with Account ID and Private Key\nnearai login --accountId name.near --privateKey key\n\n### Login Remotely (only displays the login URL)\nnearai login --remote\n</code></pre> <p>See reference docs below for more info on account management.</p>"},{"location":"cli/#near-ai-cli-reference-docs","title":"NEAR AI CLI Reference Docs","text":"<p>Here is a complete reference guide for all NEAR AI CLI commands, their usage, and examples. Use the right navigation menu to quickly jump to an area of interest.</p> <ul> <li>Account Management</li> <li>Agent Development</li> <li>Registry Management</li> <li>Model Operations</li> <li>Configuration</li> </ul> <p>Tip</p> <p>At any time you can use the <code>--help</code> flag in NEAR AI CLI for more information about a command or group of commands along with example usage.</p> <p>Additionally, simply running <code>nearai</code> in your terminal will display a full list of all commands:</p> <p></p>"},{"location":"cli/#account-management","title":"Account Management","text":""},{"location":"cli/#login","title":"<code>login</code>","text":"<p>Authenticate with your NEAR account.</p> <p>Syntax: <pre><code>nearai login [--remote] [--auth_url=&lt;auth_url&gt;] [--accountId=&lt;accountId&gt;] [--privateKey=&lt;privateKey&gt;]\n</code></pre></p> <p>Example: <pre><code>nearai login --accountId my_near_account.near\n</code></pre></p>"},{"location":"cli/#login-status","title":"<code>login status</code>","text":"<p>Display login status and auth details.</p> <p>Syntax: <pre><code>nearai login status\n</code></pre></p>"},{"location":"cli/#login-save","title":"<code>login save</code>","text":"<p>Save NEAR account authorization data.</p> <p>Syntax: <pre><code>nearai login save --accountId=&lt;accountId&gt; --signature=&lt;signature&gt; --publicKey=&lt;publicKey&gt; --callbackUrl=&lt;callbackUrl&gt; --nonce=&lt;nonce&gt;\n</code></pre></p>"},{"location":"cli/#logout","title":"<code>logout</code>","text":"<p>Clear your NEAR account authentication data.</p> <p>Syntax: <pre><code>nearai logout\n</code></pre></p>"},{"location":"cli/#location","title":"<code>location</code>","text":"<p>Show the installation location of the <code>nearai</code> CLI.</p> <p>Syntax: <pre><code>nearai location\n</code></pre></p>"},{"location":"cli/#version","title":"<code>version</code>","text":"<p>Display the current version of the <code>nearai</code> CLI.</p> <p>Syntax: <pre><code>nearai version\n</code></pre></p>"},{"location":"cli/#agent-development","title":"Agent Development","text":"<p>Agent commands allow you to create and interact with agents by running them locally or via NEAR AI Cloud.</p>"},{"location":"cli/#agent-dev","title":"<code>agent dev</code>","text":"<p>Run a local development UI for agents that have their own UI. This launches a local server for testing and developing agent functionality in a browser-based environment.</p> <p>Syntax: <pre><code>nearai agent dev\n</code></pre></p>"},{"location":"cli/#agent-inspect","title":"<code>agent inspect</code>","text":"<p>Inspect the environment and contents of an agent at the specified path. This launches a Streamlit interface showing the agent's structure, code, and metadata.</p> <p>Syntax: <pre><code>nearai agent inspect &lt;path&gt;\n</code></pre></p> <p>Example: <pre><code>nearai agent inspect ./my_agent\n</code></pre></p>"},{"location":"cli/#agent-interactive","title":"<code>agent interactive</code>","text":"<p>Run an agent interactively in a conversational interface. If no agent is specified, you'll be presented with a list of available agents to choose from. Specifying an agent will run the agent from NEAR AI Cloud. If you want to run a specific local agent, pass the path to the agent followed by the <code>--local</code> flag.</p> <p>Syntax: <pre><code>nearai agent interactive [--agent=&lt;agent&gt;] [--thread_id=&lt;thread_id&gt;] [--tool_resources=&lt;tool_resources&gt;] [--local] [--verbose] [--env_vars=&lt;env_vars&gt;]\n</code></pre></p> <p>Example: <pre><code>nearai agent interactive --agent=namespace/agent_name/1.0.0 --local --verbose\n</code></pre></p>"},{"location":"cli/#agent-task","title":"<code>agent task</code>","text":"<p>Run a single non-interactive task with an agent. The agent will process the task and return its response. This is useful for automation or when you don't need an ongoing conversation.</p> <p>Syntax: <pre><code>nearai agent task &lt;agent&gt; &lt;task&gt; [--thread_id=&lt;thread_id&gt;] [--tool_resources=&lt;tool_resources&gt;] [--file_ids=&lt;file_ids&gt;] [--local] [--verbose] [--env_vars=&lt;env_vars&gt;]\n</code></pre></p> <p>Example: <pre><code>nearai agent task namespace/agent_name/1.0.0 \"Summarize this text\" --local --verbose\n</code></pre></p>"},{"location":"cli/#agent-create","title":"<code>agent create</code>","text":"<p>Create a new AI agent from scratch or fork existing ones.</p> <p>Syntax: <pre><code>nearai agent create [--name=&lt;name&gt;] [--description=&lt;description&gt;] [--fork=&lt;fork&gt;]\n</code></pre></p> <p>Example: <pre><code>nearai agent create --name my_new_agent --description \"A new AI agent\"\n</code></pre></p>"},{"location":"cli/#agent-upload","title":"<code>agent upload</code>","text":"<p>Upload an item to the NEAR AI registry for public use.</p> <p>Syntax: <pre><code>nearai agent upload [--local_path=&lt;path&gt;] [--bump] [--minor_bump] [--major_bump]\n</code></pre></p> <p>Example: <pre><code>nearai agent upload --local_path=./my_agent --bump\n</code></pre></p>"},{"location":"cli/#registry-management","title":"Registry Management","text":"<p>Registry Commands</p> <p>Manage items in the NEAR AI Registry including agents, models, datasets, and evaluations. These commands allow you to upload, download, update, and list available items in the NEAR AI Registry.</p> <p>Permission Commands</p> <p>Commands for managing permissions and access control for NEAR AI resources.</p> <ul> <li><code>permission grant</code></li> <li><code>permission revoke</code></li> </ul>"},{"location":"cli/#registry-info","title":"<code>registry info</code>","text":"<p>Display detailed information about a registry item. Includes metadata and available provider matches for models.</p> <p>Syntax: <pre><code>nearai registry info &lt;entry&gt;\n</code></pre></p> <p>Example: <pre><code>nearai registry info namespace/agent_name/1.0.0\n</code></pre></p>"},{"location":"cli/#registry-metadata-template","title":"<code>registry metadata-template</code>","text":"<p>This generates a properly formatted <code>metadata.json</code> file with default values that can be customized for your agent or model.</p> <p>Syntax: <pre><code>nearai registry metadata-template [--local_path=&lt;path&gt;] [--category=&lt;category&gt;] [--description=&lt;description&gt;]\n</code></pre></p> <p>Example: <pre><code>nearai registry metadata-template --local_path=./my_agent --category=agent --description=\"My new agent\"\n</code></pre></p>"},{"location":"cli/#registry-list","title":"<code>registry list</code>","text":"<p>List available items in the NEAR AI registry. You can filter the results by namespace, category, tags, and other criteria to find specific items.</p> <p>Syntax: <pre><code>nearai registry list [--namespace=&lt;namespace&gt;] [--category=&lt;category&gt;] [--tags=&lt;tags&gt;] [--total=&lt;total&gt;] [--offset=&lt;offset&gt;] [--show_all] [--show_latest_version] [--star=&lt;star&gt;]\n</code></pre></p> <p>Example: <pre><code>nearai registry list --category=agent --tags=nlp,chatbot --total=10\n</code></pre></p>"},{"location":"cli/#registry-update","title":"<code>registry update</code>","text":"<p>Update the remote metadata of an item in the NEAR AI Registry. Looks for a <code>metadata.json</code> file in the given directory and updates the remote metadata with the new values.</p> <p>Syntax: <pre><code>nearai registry update [--local_path=&lt;path&gt;]\n</code></pre></p> <p>Example: <pre><code>nearai registry update --local_path=./my_agent\n</code></pre></p>"},{"location":"cli/#registry-upload-unregistered-common-provider-models","title":"<code>registry upload-unregistered-common-provider-models</code>","text":"<p>Creates new registry items for unregistered common provider models. This command helps keep the registry up-to-date with the latest models from various providers.</p> <p>Syntax: <pre><code>nearai registry upload-unregistered-common-provider-models [--dry_run=&lt;bool&gt;]\n</code></pre></p> <p>Example: <pre><code>nearai registry upload-unregistered-common-provider-models --dry_run=False\n</code></pre></p>"},{"location":"cli/#registry-upload","title":"<code>registry upload</code>","text":"<p>Upload an item to the NEAR AI registry for public use.</p> <p>Syntax: <pre><code>nearai registry upload [--local_path=&lt;path&gt;] [--bump] [--minor_bump] [--major_bump]\n</code></pre></p> <p>Example: <pre><code>nearai registry upload --local_path=./my_agent --bump\n</code></pre></p>"},{"location":"cli/#registry-download","title":"<code>registry download</code>","text":"<p>Download an item from the NEAR AI registry to your local machine. This allows you to use or inspect agents, models, datasets, etc. that have been published by others.</p> <p>Syntax: <pre><code>nearai registry download &lt;entry_location&gt; [--force]\n</code></pre></p> <p>Example: <pre><code>nearai registry download namespace/agent_name/1.0.0 --force\n</code></pre></p>"},{"location":"cli/#permission-grant","title":"<code>permission grant</code>","text":"<p>Grant a specific permission to a NEAR account. This command allows you to grant a specific permission to a NEAR account, enabling them to access certain NEAR AI resources or perform specific actions.</p> <p>Syntax: <pre><code>nearai permission grant &lt;account_id&gt; &lt;permission&gt;\n</code></pre></p> <p>Example: <pre><code>nearai permission grant alice.near read_access\n</code></pre></p>"},{"location":"cli/#permission-revoke","title":"<code>permission revoke</code>","text":"<p>Revoke permissions from a NEAR account. If no permission is specified, all permissions will be revoked from the account.</p> <p>Syntax: <pre><code>nearai permission revoke &lt;account_id&gt; [--permission=&lt;permission&gt;]\n</code></pre></p> <p>Example: <pre><code>nearai permission revoke bob.near --permission=write_access\n</code></pre></p>"},{"location":"cli/#model-operations","title":"Model Operations","text":""},{"location":"cli/#benchmark-run","title":"<code>benchmark run</code>","text":"<p>Run benchmark on a dataset with a solver strategy. This command executes a benchmark on a specified dataset using a given solver strategy. Results are cached in the database for subsequent runs unless <code>--force</code> is used.</p> <p>Syntax: <pre><code>nearai benchmark run &lt;dataset&gt; &lt;solver_strategy&gt; [--max_concurrent=&lt;int&gt;] [--force] [--subset=&lt;subset&gt;] [--check_compatibility] [--record] [--num_inference_retries=&lt;int&gt;] [--solver_args...]\n</code></pre></p> <p>Example: <pre><code>nearai benchmark run my_dataset my_solver --max_concurrent=4 --force --record\n</code></pre></p>"},{"location":"cli/#benchmark-list","title":"<code>benchmark list</code>","text":"<p>List all executed benchmarks. This command displays a table of all executed benchmarks, with options to filter by namespace, benchmark name, solver name, and solver arguments. Results are paginated using limit and offset parameters.</p> <p>Syntax: <pre><code>nearai benchmark list [--namespace=&lt;namespace&gt;] [--benchmark=&lt;benchmark&gt;] [--solver=&lt;solver&gt;] [--args=&lt;args&gt;] [--total=&lt;total&gt;] [--offset=&lt;offset&gt;]\n</code></pre></p> <p>Example: <pre><code>nearai benchmark list --namespace=my_namespace --total=20\n</code></pre></p>"},{"location":"cli/#evaluation-table","title":"<code>evaluation table</code>","text":"<p>Displays a table of all evaluation results, with options to customize the display of columns and metrics. The table can be configured to show all key columns and metrics, or a limited subset for better readability.</p> <p>Syntax: <pre><code>nearai evaluation table [--all_key_columns] [--all_metrics] [--num_columns=&lt;int&gt;] [--metric_name_max_length=&lt;int&gt;]\n</code></pre></p> <p>Example: <pre><code>nearai evaluation table --all_metrics --num_columns=8\n</code></pre></p>"},{"location":"cli/#evaluation-read-solutions","title":"<code>evaluation read-solutions</code>","text":"<p>Reads <code>solutions.json</code> from an evaluation entry. It can filter solutions by status and show either concise or verbose output for each solution.</p> <p>Syntax: <pre><code>nearai evaluation read-solutions &lt;entry&gt; [--status=&lt;bool&gt;] [--verbose]\n</code></pre></p> <p>Example: <pre><code>nearai evaluation read-solutions namespace/evaluation_name/1.0.0 --status=True\n</code></pre></p>"},{"location":"cli/#hub-chat","title":"<code>hub chat</code>","text":"<p>Chat with a model from the NEAR AI hub. This command allows you to interact with language models hosted on the NEAR AI hub. You can specify which model to use, which provider to use, and customize the chat experience with various parameters.</p> <p>Syntax: <pre><code>nearai hub chat [--query=&lt;query&gt;] [--endpoint=&lt;endpoint&gt;] [--model=&lt;model&gt;] [--provider=&lt;provider&gt;] [--info]\n</code></pre></p> <p>Example: <pre><code>nearai hub chat --query=\"What is AI?\" --model=gpt-3.5-turbo\n</code></pre></p>"},{"location":"cli/#tensorboard-start","title":"<code>tensorboard start</code>","text":"<p>TensorBoard integration.</p> <p>Syntax: <pre><code>nearai tensorboard start &lt;logdir&gt; [--limit=&lt;limit&gt;] [--timeout=&lt;timeout&gt;]\n</code></pre></p> <p>Example: <pre><code>nearai tensorboard start /tmp/log --limit=200\n</code></pre></p>"},{"location":"cli/#vllm-run","title":"<code>vllm run</code>","text":"<p>Run a VLLM server with an OpenAI-compatible API for local inference. This command starts a VLLM server that provides an OpenAI-compatible API for running language models locally. The server supports various configuration options for optimizing performance and resource utilization.</p> <p>Syntax: <pre><code>nearai vllm run\n</code></pre></p> <p>Example: <pre><code>nearai vllm run\n</code></pre></p>"},{"location":"cli/#finetune-inspect","title":"<code>finetune inspect</code>","text":"<p>Commands for fine-tuning models.</p> <p>Syntax: <pre><code>nearai finetune inspect &lt;job_id&gt;\n</code></pre></p> <p>Example: <pre><code>nearai finetune inspect myJobId\n</code></pre></p>"},{"location":"cli/#finetune-start","title":"<code>finetune start</code>","text":"<p>Start a fine-tuning job on the current node.</p> <p>Syntax: <pre><code>nearai finetune start\n</code></pre></p> <p>Example: <pre><code>nearai finetune start\n</code></pre></p>"},{"location":"cli/#configuration","title":"Configuration","text":"<p>Configuration commands help you manage your NEAR AI CLI settings. You can view, set, and modify various configuration values that control how the CLI behaves.</p> <p>Configuration Commands</p> <ul> <li><code>config set</code></li> <li><code>config get</code></li> <li><code>config show</code></li> </ul>"},{"location":"cli/#config-set","title":"<code>config set</code>","text":"<p>Add a key-value pair to the config file.</p> <p>Syntax: <pre><code>nearai config set &lt;key&gt; &lt;value&gt; [--local]\n</code></pre></p> <p>Example: <pre><code>nearai config set api_url https://api.example.com\n</code></pre></p>"},{"location":"cli/#config-get","title":"<code>config get</code>","text":"<p>Get the value of a key from the config file.</p> <p>Syntax: <pre><code>nearai config get &lt;key&gt;\n</code></pre></p> <p>Example: <pre><code>nearai config get api_url\n</code></pre></p>"},{"location":"cli/#config-show","title":"<code>config show</code>","text":"<p>Show all configuration values.</p> <p>Syntax: <pre><code>nearai config show\n</code></pre></p>"},{"location":"cli/#miscellaneous-commands","title":"Miscellaneous Commands","text":""},{"location":"cli/#submit","title":"<code>submit</code>","text":"<p>Submit a task to be executed by a worker.</p> <p>Syntax: <pre><code>nearai submit [--path=&lt;path&gt;] [--worker_kind=&lt;worker_kind&gt;]\n</code></pre></p> <p>Example: <pre><code>nearai submit --path=./my_task --worker_kind=GPU_8_A100\n</code></pre></p>"},{"location":"contributing/","title":"Contribute to <code>nearai</code>","text":"<p>Everyone is welcome to contribute, and we value everybody's contribution. Code contributions are not the only way to help the community. Answering questions, helping others, and improving documentation are also immensely valuable.</p> <p>It also helps us if you spread the word! Reference the library in blog posts about the awesome projects it made possible, or even simply \u2b50\ufe0f the repository to say thank you.</p> <p>This guide was heavily inspired by the huggingface transformers guide to contributing.</p>"},{"location":"contributing/#ways-to-contribute","title":"Ways to contribute","text":"<p>There are several ways you can contribute to <code>nearai</code>:</p> <ul> <li>Contribute to <code>nearai</code></li> <li>Ways to contribute</li> <li>Fixing outstanding issues</li> <li>Submitting a bug-related issue or feature request<ul> <li>Did you find a bug?</li> <li>Do you want a new feature?</li> </ul> </li> <li>Contribute Documentation</li> <li>Create a Pull Request<ul> <li>Pull request checklist</li> <li>Sync a forked repository with upstream main</li> </ul> </li> </ul>"},{"location":"contributing/#fixing-outstanding-issues","title":"Fixing outstanding issues","text":"<p>If you notice an issue with the existing code and have a fix in mind, feel free to start contributing and open a Pull Request!</p>"},{"location":"contributing/#submitting-a-bug-related-issue-or-feature-request","title":"Submitting a bug-related issue or feature request","text":"<p>Do your best to follow these guidelines when submitting a bug-related issue or a feature request. It will make it easier for us to come back to you quickly and with good feedback.</p>"},{"location":"contributing/#did-you-find-a-bug","title":"Did you find a bug?","text":"<p><code>nearai</code> is alpha software. This means there is a possibility of encountering issues in the code. With help from users like you who report problems, we can make it more robust and reliable.</p> <p>Before you report an issue, we would really appreciate it if you could make sure the bug was not already reported (use the search bar on GitHub under Issues). Your issue should also be related to bugs in the library itself, and not your code.</p> <p>Once you've confirmed the bug hasn't already been reported, please include the following information in your issue so we can quickly resolve it:</p> <ul> <li>What did you do?</li> <li>What did you expect to happen?</li> <li>What happened instead?</li> <li>Your OS type and version and Python, PyTorch and versions where applicable.</li> <li>A short, self-contained, code snippet that allows us to reproduce the bug in   less than 30s.</li> <li>The full traceback if an exception is raised.</li> <li>Attach any other additional information, like screenshots, you think may help.</li> </ul> <p>To get the OS and software versions automatically, run the following command:</p> <pre><code>uname -a\n</code></pre>"},{"location":"contributing/#do-you-want-a-new-feature","title":"Do you want a new feature?","text":"<p>If there is a new feature you'd like to see in <code>nearai</code>, please open an issue and describe:</p> <ol> <li>What is the motivation behind this feature? Is it related to a problem or frustration with the library? Is it a feature related to something you need for a project? Is it something you worked on and think it could benefit the community?</li> </ol> <p>Whatever it is, we'd love to hear about it!</p> <ol> <li>Describe your requested feature in as much detail as possible. The more you can tell us about it, the better we'll be able to help you.</li> <li>Provide a code snippet that demonstrates the feature usage.</li> <li>If the feature is related to a paper, please include a link.</li> </ol>"},{"location":"contributing/#contribute-documentation","title":"Contribute Documentation","text":"<p>If you discover any errors or omissions in our documentation, please open an issue and describe:</p> <ul> <li>Which explanation or code snippet is incorrect</li> <li>What concept is not clear or missing</li> <li>If you know, what would be the correct explanation or code snippet</li> </ul> <p>If you think you can contribute a fix for the issue, please feel free to open a Pull Request.</p> <p>To preview your changes locally, you will need to install all the dependencies for the documentation which can be easily installed through <code>pip</code> or <code>uv</code>:</p> pipuv <pre><code>pip install -e \".[docs]\"\n</code></pre> <pre><code>uv sync --group docs\n</code></pre> <p>Then simply test your changes locally using <code>mkdocs serve</code></p> <p>Cairo Graphics</p> <p>If you encounter a problem with <code>cairo</code>, please follow the mkdocs-material Requirements Guide</p>"},{"location":"contributing/#create-a-pull-request","title":"Create a Pull Request","text":"<p>Before writing any code, we strongly advise you to search through the existing PRs or issues to make sure nobody is already working on the same thing. If you are unsure, it is always a good idea to open an issue to get some feedback.</p> <p>You will need basic <code>git</code> proficiency to contribute to <code>nearai</code>. While <code>git</code> is not the easiest tool to use, it has the greatest manual. Type <code>git --help</code> in a shell and enjoy! If you prefer books, Pro Git is a very good reference. We also recommend asking any available AGI to help you with <code>git</code>.</p> <p>Follow the steps below to start contributing:</p> <ol> <li> <p>Fork the repository by    clicking on the Fork button on the repository's page. This creates a copy of the code    under your GitHub user account.</p> </li> <li> <p>Clone your fork to your local disk, and add the base repository as a remote:</p> </li> </ol> <pre><code>git clone git@github.com:&lt;your Github handle&gt;/nearai.git\ncd nearai\ngit remote add upstream https://github.com/nearai/nearai.git\n</code></pre> <ol> <li>Create a new branch to hold your development changes:</li> </ol> <pre><code>git checkout -b a-descriptive-name-for-my-changes\n</code></pre> <p>\ud83d\udea8 Do not work on the <code>main</code> branch!</p> <ol> <li> <p>Set up a development environment (follow steps in the README):</p> </li> <li> <p>Develop the features in your branch.</p> </li> </ol> <p>As you work on your code, you should make sure it functions as intended.</p> <p><code>nearai</code> relies on <code>ruff</code> and <code>mypy</code> to format and type check its source code    consistently. After you make your changes and are ready to PR them, ensure that    your code is formatted and type-checked by running:</p> <pre><code>./scripts/format_check.sh\n</code></pre> <pre><code>./scripts/lint_check.sh\n</code></pre> <pre><code>./scripts/type_check.sh\n</code></pre> <p>Once you're happy with your changes, add the changed files with <code>git add</code> and    record your changes locally with <code>git commit</code>:</p> <pre><code>git add modified_file.py\ngit commit\n</code></pre> <p>Please remember to write good commit    messages to clearly communicate the changes you made!</p> <p>To keep your copy of the code up to date with the original    repository, rebase your branch on <code>upstream/branch</code> before you open a pull request or if requested by a maintainer:</p> <pre><code>git fetch upstream\ngit rebase upstream/main\n</code></pre> <p>Push your changes to your branch:</p> <pre><code>git push -u origin a-descriptive-name-for-my-changes\n</code></pre> <p>If you've already opened a pull request, you'll need to force push with the <code>--force</code> flag. Otherwise, if the pull request hasn't been opened yet, you can just push your changes normally.</p> <ol> <li> <p>Now you can go to your fork of the repository on GitHub and click on Pull Request to open a pull request. Make sure you tick off all the boxes on our checklist below. When you're ready, you can send your changes to the project maintainers for review.</p> </li> <li> <p>It's ok if maintainers request changes, it happens to our core contributors    too! So everyone can see the changes in the pull request, work in your local    branch and push the changes to your fork. They will automatically appear in    the pull request.</p> </li> </ol>"},{"location":"contributing/#pull-request-checklist","title":"Pull request checklist","text":"<ul> <li>The pull request title should summarize your contribution.</li> <li>If your pull request addresses an issue, please mention the issue number in the pull request description to make sure they are linked (and people viewing the issue know you are working on it).</li> <li>To indicate a work in progress please prefix the title with <code>[WIP]</code>. These are useful to avoid duplicated work, and to differentiate it from PRs ready to be merged.</li> <li>Don't add any images, videos and other non-text files that'll significantly weigh down the repository. Instead, reference them by URL.</li> </ul>"},{"location":"contributing/#sync-a-forked-repository-with-upstream-main","title":"Sync a forked repository with upstream main","text":"<p>When updating the main branch of a forked repository, please follow these steps to avoid pinging the upstream repository which adds reference notes to each upstream PR, and sends unnecessary notifications to the developers involved in these PRs.</p> <ol> <li>When possible, avoid syncing with the upstream using a branch and PR on the forked repository. Instead, merge directly into the forked main.</li> <li>If a PR is absolutely necessary, use the following steps after checking out your branch:</li> </ol> <pre><code>git checkout -b your-branch-for-syncing\ngit pull --squash --no-commit upstream main\ngit commit -m '&lt;your message without GitHub references&gt;'\ngit push --set-upstream origin your-branch-for-syncing\n</code></pre>"},{"location":"decentralization/","title":"Decentralization","text":"<p>This document outlines process of progressive decentralization of NEAR AI capabilities.</p> <p>NEAR AI project approaches decentralization in a progressive way, starting with delivering best researcher, developer and user experience possible and progressively decentralizing and enabling privacy and trust features over time.</p> Component Objective Registry Decentralized storage with support of private and encrypted items Training / Fine-tuning Leveraging decentralized set of nodes to train models in a provable way Agent runner Using trusted execution environment to run agents privately and Agent memory Using FHE to do retrieval from encrypted storage Inter-agent communication Peer-to-peer protocol that supports identity,  payments and dispute resolution"},{"location":"inference/","title":"NEAR AI Inference API (OpenAI Compatible)","text":"<p>NEAR AI provides an OpenAI-compatible API for inference, allowing you to easily integrate powerful language models into your applications.</p> <p>This guide covers the basic inference endpoints and how to use them. For recommended usage of NEAR AI inference in agents explore agent examples below or jump to Model Match section.</p> <p>Agent examples are available here: nearai examples, nearai_langchain examples</p>"},{"location":"inference/#getting-started","title":"Getting Started","text":"<ol> <li>Install all dependencies</li> </ol> <p>a. using <code>pip</code>:</p> <pre><code># Create a virtual environment\npython -m venv nearai_env\n\n# Activate the virtual environment\n# On Windows:\n# nearai_env\\Scripts\\activate\n# On macOS and Linux:\nsource nearai_env/bin/activate\n\n# Install the package in editable mode\npip install -e .\n</code></pre> <p>b. using uv:</p> <pre><code>uv sync --group dev\n</code></pre> <ol> <li> <p>Set up authentication:</p> </li> <li> <p>Log in to NEAR AI using the CLI: <code>nearai login</code></p> </li> <li> <p>The auth object will be saved in <code>~/.nearai/config.json</code></p> </li> <li> <p>Import the required libraries and set up the client</p> </li> </ol> <pre><code>import openai\nimport json\nimport os\nimport nearai\n\nhub_url = \"https://api.near.ai/v1\"\n\n# Login to NEAR AI Hub using nearai CLI.\n# Read the auth object from ~/.nearai/config.json\nauth = nearai.config.load_config_file()[\"auth\"]\nsignature = json.dumps(auth)\n\nclient = openai.OpenAI(base_url=hub_url, api_key=signature)\n</code></pre>"},{"location":"inference/#list-models","title":"List Models","text":"<p>To list available models, use the <code>models.list()</code> method:</p> <pre><code>models = client.models.list()\nprint(models)\n</code></pre> <p>Different providers have different models, and different model formats. For example, fireworks has a format <code>fireworks::accounts/&lt;namespace&gt;/models/&lt;model_name&gt;</code>. To get all unique providers, do:</p> <pre><code>providers = set([model.id.split(\"::\")[0] for model in models])\nprint(providers)\n</code></pre>"},{"location":"inference/#create-a-chat-completion","title":"Create a Chat Completion","text":"<p>To create a chat completion, use the <code>chat.completions.create()</code> method. Here's an example:</p> <pre><code>completion = client.chat.completions.create(\n  model=\"fireworks::accounts/fireworks/models/qwen2p5-72b-instruct\",\n  messages=[\n    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n    {\"role\": \"user\", \"content\": \"Hello, how are you?\"}\n  ]\n)\n\nprint(completion.choices[0].message.content)\n</code></pre> <p>This will send a request to the specified model with the given messages and return the model's response. The response can be accessed through the <code>choices</code> array in the returned object.</p>"},{"location":"inference/#error-handling","title":"Error Handling","text":"<p>When using the API, it's important to handle potential errors. Here's an example of how to implement basic error handling:</p> <pre><code>try:\n  completion = client.chat.completions.create(\n    model=\"fireworks::accounts/fireworks/models/qwen2p5-72b-instruct\",\n    messages=[\n      {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n      {\"role\": \"user\", \"content\": \"Hello, how are you?\"}\n    ]\n  )\n  print(completion.choices[0].message.content)\nexcept openai.APIError as e:\n  print(f\"An API error occurred: {e}\")\nexcept Exception as e:\n  print(f\"An unexpected error occurred: {e}\")\n</code></pre>"},{"location":"inference/#model-match","title":"Model Match","text":"<p>For agents written in Python, we expect the model name to be given either in <code>metadata.json</code> or <code>env.completion</code> as <code>&lt;optional_namespace&gt;/&lt;model_name&gt;</code>. Examples: - \"llama-v3-70b-instruct\" - \"yi-01-ai/yi-large\"</p> <p>\"model_provider\" may also be optionally specified in <code>metadata.json</code>.</p> <p>Existing provider models are matched against the provided model name, and provider model full path is automatically determined using <code>def match_provider_model()</code> function. This function is executed when: - <code>env.completion()</code> function is called. - (<code>nearai_langchain</code> library) <code>orchestrator.chat_model</code> inference is called.</p>"},{"location":"inference/#additional-features","title":"Additional Features","text":"<p>The NEAR AI Inference API also supports other features such as:</p> <ol> <li>Streaming responses</li> <li>Function calling</li> <li>Custom parameters (temperature, max_tokens, etc.)</li> </ol> <p>For more information on these features, please refer to the full API documentation.</p>"},{"location":"inference/#conclusion","title":"Conclusion","text":"<p>This guide covers the basics of using the NEAR AI Inference API. By following these steps, you should be able to authenticate, list models, and create chat completions. For more advanced usage and detailed information, please refer to the complete API documentation or explore the provided examples.</p>"},{"location":"private-ml-sdk/","title":"Private and Verifiable AI","text":"<p>Thanks to a combination of cryptographic techniques and secure hardware, NEAR AI is private and verifiable every step of the way. From agent interactions to model training, our infrastructure ensures data privacy and result integrity throughout the entire process.</p> <p></p> <p>NEAR AI leverages technology from Intel and NVIDIA to keep your data safe and isolated</p> <p>To execute an agent securely, NEAR AI uses Intel TDX technology to create a Confidential Virtual Machine (CVM). This CVM is a virtual machine that runs in an isolated environment within the CPU, where a Docker container containing the agent's code and user data is deployed and executed. </p> <p>For model inference, an encrypted communication channel is established using NVIDIA TEE technology, which creates a Confidential Space within the GPU. The model runs in this isolated environment, protected from external access.</p> <p>It's important to note that the CPU and GPU Confidential Spaces operate in complete isolation from the rest of the system - neither the operating system nor the hypervisor (VM manager) can access the code or data being processed.</p> Want to dive deeper? <p>All technology that powers NEAR AI is open-sourced. If you want to dive deeper and learn more about how this secure and private AI is being developed, check out the Private ML SDK on GitHub. </p>"},{"location":"private-ml-sdk/#how-can-i-trust-it","title":"How Can I Trust It?","text":"<p>Before deployment, NEAR AI verifies both proper hardware configuration and Docker container image hash integrity. This ensures a secure runtime environment and that the exact code being executed matches expectations.</p> <p>After execution, NEAR AI validates security through TEE attestations - cryptographic proofs verify both a secure environment and data/code integrity. </p> <p>You can view and verify these attestations yourself or publish them on-chain with NEAR Protocol to allow anyone to verify them. This provides a cryptographically verifiable chain of trust through the entire stack from agent to inference to hardware; all the way down to the certifications from the chip manufacturers.</p>"},{"location":"private-ml-sdk/#how-can-i-use-it","title":"How Can I Use it?","text":"<p>Right now we are beta testing this technology, but soon we will open for public use. In the near future, all agents and models will run in this secure environment, ensuring that your data is always safe and the results are correct.</p> <p>Stay tuned for more updates!</p>"},{"location":"agents/integration/","title":"Integrating NEAR AI Agents","text":"<p>Easily integrate your Agent or any NEAR AI Agent into an existing application. There are two main pathways to accomplish this:</p> <ul> <li>iFrame Agent Embedding - Simple high-level integration with just a few lines of code</li> <li>Agent API - Lower-level integration for use when orchestrating multiple agents</li> </ul>"},{"location":"agents/integration/#iframe-agent-embedding","title":"iFrame Agent Embedding","text":"<p>The quickest way to integrate a NEAR AI Agent into any existing website is with an iFrame. This allows users to interact with your agent without leaving your site.</p>"},{"location":"agents/integration/#basic-embedding","title":"Basic Embedding","text":"<p>To embed an agent, use the following iframe code replacing the <code>src</code> with the agent you want to embed.</p> <p><code>Example</code>:</p> <pre><code>&lt;iframe \n  src=\"https://app.near.ai/embed/&lt;your-account.near&gt;/&lt;your-agent-name&gt;/latest\" \n  sandbox=\"allow-scripts allow-popups allow-same-origin allow-forms\"\n  style=\"border: none; height: 100svh;\"&gt;\n&lt;/iframe&gt;\n</code></pre> <p>Info</p> <ul> <li> <p>Note that the difference with this <code>src</code> path compared to a regular link to your agent is that it is using the <code>embed</code> endpoint.</p> </li> <li> <p>Also note that you can replace the <code>latest</code> with a specific agent version number.</p> </li> </ul> <p>Tip</p> <p>You can also copy/paste the snippet from the NEAR AI Dev Platform.</p> <ul> <li>Clicking the <code>share</code> icon from your agent page and select <code>&lt;embed&gt;</code></li> </ul> <p></p>"},{"location":"agents/integration/#customizing-the-embed","title":"Customizing the Embed","text":"<p>There are three ways to customize the appearance and behavior of your embedded agent:</p> <ol> <li>Basic <code>&lt;iframe&gt;</code> attributes</li> <li>URL parameters</li> <li><code>metadata.json</code> file</li> </ol> <p>Info</p> <p>The embedded agent will inherit the styling of the NEAR AI platform while maintaining a consistent look and feel with your website.</p>"},{"location":"agents/integration/#light-or-dark-theme","title":"Light or Dark Theme","text":"<p>For light or dark themes, add a <code>theme</code> parameter to the embed src URL:</p> <p><code>src=\"https://app.near.ai/embed/&lt;your-account.near&gt;/&lt;your-agent-name&gt;/latest?theme=dark\"</code></p>"},{"location":"agents/integration/#custom-logo","title":"Custom Logo","text":"<p>You can also add a custom logo to replace the default agent name in the upper left hand corner of your agent.</p> <p>In your <code>metadata.json</code> file add an <code>embed</code> section under the agent details:</p> <pre><code>{\n  \"details\": {\n    \"agent\": {\n      \"embed\": {\n        \"logo\": \"https://near.ai/logo-white.svg\"\n      }\n    }\n  }\n}\n</code></pre>"},{"location":"agents/integration/#agent-api-integration","title":"Agent API Integration","text":"<p>NEAR AI Agents are compatible with the OpenAI Assistants API, making it easy to integrate powerful AI capabilities into your applications. The API enables NEAR AI agents to:</p> <ol> <li>Call various models with specific instructions to customize personality and capabilities</li> <li>Access multiple tools for enhanced functionality</li> <li>Maintain persistent conversation Threads</li> <li>Process files in several formats (as inputs or outputs)</li> </ol> <p>See the complete NEAR AI OpenAPI specification.</p> <p>Info</p> <p>While you can orchestrate multiple agents, in many cases you can consolidate multiple roles into a single agent. For best practices, see \"Orchestrating Agents\".</p>"},{"location":"agents/integration/#key-concepts","title":"Key Concepts","text":"Concept Description Thread A conversation session between an Assistant and a user. Threads store Messages and automatically handle truncation to fit content into a model's context. You create a Thread once and simply append Messages as users reply. Message Content created by an Assistant or user. Messages can include text, images, and other files, and are stored as a list within a Thread. Run An invocation of an Assistant on a Thread. The Assistant processes the Thread's Messages using its configuration to perform tasks by calling models and tools. Run Step Detailed record of actions the Assistant took during a Run. Examining Run Steps allows you to see how the Assistant reaches its conclusions. Service Agent A specialized Agent called by the Assistant to accomplish specific tasks (e.g., purchasing, swaps, smart contract generation)."},{"location":"agents/integration/#implementation-guide","title":"Implementation Guide","text":"<p>NEAR AI provides a powerful Assistant API that you can integrate into your applications by following these steps:</p> <ol> <li>Authentication: Login to NEAR Account</li> <li>Create a Thread</li> <li>Add Messages to the Thread</li> <li>Run the Assistant on the Thread</li> <li>Process Assistant Responses</li> </ol>"},{"location":"agents/integration/#1-authentication-login-to-near-account","title":"1. Authentication: Login to NEAR Account","text":"JavaScriptPython <p>From client side, you can use the following NEAR Wallet Selector function to sign the message and get the required NEAR AI authorization token.</p> <pre><code>async function login(wallet, message, nonce, recipient, callbackUrl) {\n    const signedMessage = await wallet.signMessage({\n        message,\n        nonce,\n        recipient,\n        callbackUrl\n    });\n    return {\n        signature: signedMessage.signature,\n        accountId: signedMessage.accountId,\n        publicKey: signedMessage.publicKey,\n        message,\n        nonce,\n        recipient,\n        callbackUrl\n    };\n}\n\n// Generate nonce based on current time in milliseconds and\n// pad it with zeros to ensure its exactly 32 bytes in length\nconst nonce = Buffer.from(Date.now().toString().padStart(32, '0'));\nconst recipient = YOUR_RECIPIENT_ADDRESS;\nconst callbackUrl = YOUR_CALLBACK_URL;\n\n// Example usage of login function\nconst auth = await login(wallet, \"Login to NEAR AI\", nonce, recipient, callbackUrl);\n</code></pre> <p>In Python, we recommend using the <code>nearai</code> CLI to login into your NEAR account. More details here.</p> <pre><code>nearai login\n</code></pre>"},{"location":"agents/integration/#2-create-a-thread","title":"2. Create a Thread","text":"<p>A Thread represents a conversation between a user and one or many Assistants. You can create a Thread when a user (or your AI application) starts a conversation with your Assistant. For more information see Threads.</p> JavaScriptPython <pre><code>import OpenAI from \"openai\";\nconst openai = new OpenAI({\n    baseURL: \"https://api.near.ai/v1\",\n    apiKey: `Bearer ${JSON.stringify(auth)}`,\n});\n\nconst thread = await openai.beta.threads.create();\n</code></pre> <pre><code>import openai\n\nclient = openai.OpenAI(\n    api_key=\"YOUR_NEARAI_SIGNATURE\",\n    base_url=\"https://api.near.ai/v1\",\n)\n\nthread = client.beta.threads.create()\n</code></pre>"},{"location":"agents/integration/#3-add-messages-to-the-thread","title":"3. Add Messages to the Thread","text":"<p>The contents of the messages your users or applications create are added as Message objects to the Thread. Messages can contain both text and files. There is a limit of 100,000 Messages per Thread and we smartly truncate any context that does not fit into the model's context window.</p> JavaScriptPython <pre><code>const message = await openai.beta.threads.messages.create(\n  thread.id,\n  {\n    role: \"user\",\n    content: \"Help me plan my trip to Tokyo\"\n  }\n);\n</code></pre> <pre><code>message = client.beta.threads.messages.create(\n    thread_id=thread.id,\n    role=\"user\",\n    content=\"Help me plan my trip to Tokyo\"\n)\n</code></pre>"},{"location":"agents/integration/#4-run-the-assistant-on-the-thread","title":"4. Run the Assistant on the Thread","text":"<p>Once all the user Messages have been added to the Thread, you can Run the Thread with any Assistant. Creating a Run uses the model and tools associated with the Assistant to generate a response. These responses are added to the Thread as assistant Messages.</p> <p>Runs are asynchronous, which means you'll want to monitor their status by polling the Run object until a terminal status is reached. For convenience, the 'create and poll' SDK helpers assist both in creating the run and then polling for its completion.</p> JavaScriptPython <pre><code>const assistant_id = \"near-ai-agents.near/assistant/latest\"\nlet run = await openai.beta.threads.runs.createAndPoll(\n  thread.id,\n  { \n    assistant_id: assistant_id,\n  }\n);\n</code></pre> <pre><code>assistant_id = \"near-ai-agents.near/assistant/latest\"\nrun = client.beta.threads.runs.create_and_poll(\n    thread_id=thread.id,\n    assistant_id=assistant_id,\n)\n</code></pre>"},{"location":"agents/integration/#5-process-assistant-responses","title":"5. Process Assistant Responses","text":"<p>Once the Run completes, you can list the Messages added to the Thread by the Assistant.</p> JavaScriptPython <pre><code>if (run.status === 'completed') {\n  const messages = await openai.beta.threads.messages.list(\n    run.thread_id\n  );\n  for (const message of messages.data.reverse()) {\n    console.log(`${message.role} &gt; ${message.content[0].text.value}`);\n  }\n} else {\n  console.log(run.status);\n}\n</code></pre> <pre><code>if run.status == 'completed':\n    messages = client.beta.threads.messages.list(run.thread_id)\n    for message in messages:\n        print(f\"{message.role} &gt; {message.content[0].text.value}\")\nelse:\n    print(run.status)\n</code></pre> <p>You may also want to list the Run Steps of this Run if you'd like to look at any tool calls made during this Run.</p>"},{"location":"agents/quickstart/","title":"Agents Quickstart","text":"<p>In this Quickstart you will learn how to setup NEAR AI and then use it to build &amp; interact with an AI agent in less than one minute. \ud83c\udfc3\u200d\u2642\ufe0f</p> <p>NEAR AI Agents are programs that can act autonomously to solve a task, while adapting and reacting to their environment.  These agents can use various AI models, store data to remember past interactions, communicate with other agents, use tools to  interact with the environment, and much more.</p>"},{"location":"agents/quickstart/#prerequisites","title":"\u26a0\ufe0f Prerequisites","text":"<p>Before creating a NEAR AI agent, please make sure you have the NEAR AI CLI installed and have logged in with your Near wallet.</p>"},{"location":"agents/quickstart/#create-an-agent","title":"Create an Agent","text":"<p>You can create a new agent by running the following command using the NEAR AI CLI:</p> <p><pre><code>nearai agent create\n</code></pre> You will then be prompted to provide a few details about your agent:</p> <ol> <li>The name of your agent.</li> <li>A short description of your agent.</li> <li>Initial instructions for the agent (which can be edited later).</li> </ol> <p></p> <p>Once you have complete these three prompts, you'll see a summary to verify the information is correct:</p> <p></p> <p>If everything looks good, press <code>y</code> to build your agent. Once complete, you should see a confirmation screen similar to this:</p> <p></p> <p>Here you will find:</p> <ol> <li> <p>Where the agent was created:</p> <p><code>/home_directory/.nearai/regisitry/&lt;your-account.near&gt;/&lt;agent-name&gt;/0.0.1</code></p> </li> <li> <p>Useful commands to get started interacting with it:</p> <pre><code># Run agent locally\nnearai agent interactive &lt;path-to-agent&gt; --local\n\n# Select from a list of agents you created to run locally\nnearai agent interactive --local\n\n# Upload agent to NEAR AI's public registry\nnearai registry upload &lt;path-to-agent&gt;\n</code></pre> </li> </ol> <p>Success! You now have a new AI Agent ready to use!  </p>"},{"location":"agents/quickstart/#agent-files","title":"Agent Files","text":"<p>During the agent creation process, <code>nearai</code> builds your agent in your local AI registry located at:</p> <p><code>/home_directory/.nearai/registry/&lt;your-account.near&gt;/&lt;agent-name&gt;/0.0.1</code> </p> <p>This folder contains two files that define your agent:</p> <ol> <li><code>metadata.json</code>: Contains information / configuration about your agent.</li> <li><code>agent.py</code>: Python code that executes each time your agent receives a prompt.</li> </ol>"},{"location":"agents/quickstart/#metadatajson","title":"<code>metadata.json</code>","text":"<p>This file contains your agent information and configuration settings.</p> metadata.json<pre><code>{\n  \"name\": \"example-agent\",\n  \"version\": \"0.0.1\",\n  \"description\": \"NEAR AI docs example agent ;)\",\n  \"category\": \"agent\",\n  \"tags\": [],\n  \"details\": {\n    \"agent\": {\n      \"defaults\": {\n        \"model\": \"llama-v3p1-70b-instruct\",\n        \"model_provider\": \"fireworks\",\n        \"model_temperature\": 1.0,\n        \"model_max_tokens\": 16384\n      }\n    }\n  },\n  \"show_entry\": true\n}\n</code></pre>"},{"location":"agents/quickstart/#ai-model","title":"AI Model","text":"<p>Llama 3.1 70B Instruct is currently the default model for agents but you can easily change it by selecting one from app.near.ai/models and updating your JSON file.</p> <p>You can also fine tune and serve a model to fit your specific needs. (See Fine Tuning)</p>"},{"location":"agents/quickstart/#python-package-support","title":"Python Package Support","text":"<p>There are different frameworks for agents that define which Python packages they support and is configured in your <code>metadata.json</code> file. </p> <pre><code>{\n  \"details\": {\n    \"agent\": {\n      \"framework\": \"standard\"  // or \"minimal\", \"ts\", \"agentkit\", etc.\n    }\n  }\n}\n</code></pre> <p>See Supported Packages for more information.</p>"},{"location":"agents/quickstart/#message-attachments","title":"Message Attachments","text":"<p>Agents can accept attached files as part of a message during an interactive session by adding <code>allow_message_attachments</code> and setting the value to <code>true</code>:</p> <pre><code>{\n  \"details\": {\n    \"agent\": {\n      \"allow_message_attachments\": true,\n      \"allow_message_attachments_accept_mime_types\": [\n        \"image/*\",\n        \"application/pdf\",\n        \"audio/mp3\",\n        \"audio/wav\",\n        \"video/mp4\"\n      ]\n    }\n  }\n}\n</code></pre> <p>Info</p> <p>If <code>allow_message_attachments_accept_mime_types</code> is not defined, all file types will be allowed, however the agent cloud may throw an error if an unsupported file type is uploaded. </p>"},{"location":"agents/quickstart/#agentpy","title":"<code>agent.py</code>","text":"<p>This file contains the code that executes each time your agent receives a prompt. By default it will use simple instructions provided by the user during the creation process.</p> <p>For more information on how to use the environment object, see The Agent Environment.</p> <p>For additional examples, see the NEAR AI Official Agents or the NEAR AI Public Registry.</p> agent.py<pre><code>from nearai.agents.environment import Environment\n\n\ndef run(env: Environment):\n    # A system message guides an agent to solve specific tasks.\n    prompt = {\"role\": \"system\", \"content\": \"You are a helpful agent that will educate users about NEAR AI.\"}\n\n    # Use the model set in the metadata to generate a response\n    result = env.completion([prompt] + env.list_messages())\n\n    # Store the result in the chat history\n    env.add_reply(result)\n\nrun(env)\n</code></pre>"},{"location":"agents/quickstart/#next-steps","title":"Next Steps","text":"<p>Now that you have the basics down, here are some key areas to focus on next that will help you better understand what is possible when building with NEAR AI:</p>"},{"location":"agents/quickstart/#ship-your-agent","title":"Ship your agent \ud83d\ude80","text":"<p>Upload your agent to the agent registry and easily embed it into your website.</p>"},{"location":"agents/quickstart/#explore-the-registry","title":"Explore the Registry \ud83d\udd0d","text":"<p>The NEAR AI Registry is your hub for agent discovery and collaboration. Browse community-created agents, learn from examples, and share your own creations with others.</p>"},{"location":"agents/quickstart/#master-threads","title":"Master Threads \ud83d\udcac","text":"<p>Threads power agent execution and interaction. Learn to structure conversations, manage file attachments, and create coordinated multi-agent interactions - all within organized conversation threads. </p>"},{"location":"agents/quickstart/#explore-the-environment","title":"Explore the Environment \ud83e\uddd1\u200d\ud83d\udcbb","text":"<p>The environment object unlocks NEAR AI's powerful features:</p> <ul> <li>Create natural conversations with advanced message handling</li> <li>Leverage AI models for intelligent decision-making</li> <li>Enable agent-to-agent communication</li> <li>Extend capabilities with custom tools</li> </ul>"},{"location":"agents/quickstart/#learn-about-secrets-and-variables","title":"Learn About Secrets and Variables \ud83d\udd11","text":"<p>Learn how to manage environment variables and secure with proper secrets management. Store API keys safely and connect to external services with confidence.</p>"},{"location":"agents/registry/","title":"Agent Registry: Finding and Publishing Agents","text":"<p>NEAR AI agents can be deployed and hosted in a common registry, allowing the community to share their creations. This registry is used by the NEAR AI Developer Hub to store and serve agents.</p> <p>Let's take a look at how we can navigate this registry, download agents, and contribute our own agents to the ecosystem.</p> <p>Note</p> <p>The agent registry is backed by an S3 bucket with metadata stored in a database.</p>"},{"location":"agents/registry/#finding-an-agent","title":"Finding an Agent","text":"<p>There are two main ways to navigate the agent registry to discover agents: </p> <ul> <li>NEAR AI Developer Hub</li> <li>NEAR AI CLI</li> </ul> <p>For the rest of this guide, we will use the CLI to find and deploy agents. </p> <p>Tip</p> <p>Refer to the Quickstart Guide to learn how to install the CLI and login to the AI Developer Hub.</p>"},{"location":"agents/registry/#view-all-agents","title":"View all agents","text":"<p>To view all agents with <code>nearai</code> CLI, run:</p> <pre><code>nearai registry list --category agent\n</code></pre> Example Output <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 entry                           \u2502 description             \u2502 tags  \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 zavodil.near/ai16z-docs/1.03    \u2502 AI agent with AI16Z ... \u2502 agent \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 flatirons.near/common-tool...   \u2502 A library of common ..  \u2502 llama \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 jayzalowitz.near/example_a...   \u2502 Example agent           \u2502       \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 ...                             \u2502 ...                     \u2502 ...   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"agents/registry/#filtering-agents","title":"Filtering Agents","text":"<p>You can further filter the agents with two flags:</p> <ul> <li><code>--namespace</code> : The developer that created it</li> <li><code>--tags</code>: Any tags that were added to the agent</li> </ul> <p>For example, to find all agents created by <code>gagdiez.near</code> with the tag <code>template</code>, run:</p> <pre><code>nearai registry list  --category agent \\\n                      --namespace gagdiez.near \\\n                      --tags template \\\n                      --show_all\n</code></pre> <p>Tip</p> <p>You can use the <code>info</code> command to get more details about a specific agent, for example:</p> <pre><code>nearai registry info gagdiez.near/hello-ai/latest\n</code></pre>"},{"location":"agents/registry/#downloading-an-agent","title":"Downloading an Agent","text":"<p>Once you find an agent that you would like to download, use the <code>download</code> command to save it locally. Agent details are passed in the following format:</p> <pre><code>nearai registry download &lt;account.near&gt;/&lt;agent_name&gt;/&lt;version&gt;\n</code></pre> <p>The <code>version</code> can be a specific version number, or <code>latest</code> to download the most recent version.</p> <p>Example: </p> <pre><code># Download a hello world agent\nnearai registry download gagdiez.near/hello-ai/latest\n</code></pre> <p>This command saves the agent locally in <code>.nearai/registry</code> under your home directory.</p> <p>The example above would save to: <code>~/.nearai/registry/gagdiez.near/hello-ai/latest</code>.</p> <p>Tip</p> <p>The <code>--force</code> flag allows you to overwrite the local agent with the version from the registry.</p>"},{"location":"agents/registry/#uploading-an-agent","title":"Uploading an Agent","text":"<p>If you created an agent and would like to share it with others, you can upload it to the registry. To upload an agent, you must be logged in.</p> <p>The <code>upload</code> command requires the path to the agent folder stored locally, for example:</p> <pre><code>nearai registry upload ~/.nearai/registry/&lt;your-account.near&gt;/&lt;agent_folder&gt;\n</code></pre> <p>The folder must contain:</p> <ul> <li><code>agent.py</code>: Agent logic</li> <li><code>metadata.json</code>: Agent information (ex: description, tags, and model, etc.)</li> </ul> Example <code>metadata.json</code> file metadata.json<pre><code>{\n\"name\": \"hello-ai\",\n\"version\": \"0.0.1\",\n\"description\": \"A friendly agent\",\n\"category\": \"agent\",\n\"tags\": [],\n\"details\": {\n  \"agent\": {\n    \"defaults\": {\n      \"model\": \"llama-v3p1-70b-instruct\",\n      \"model_provider\": \"fireworks\",\n      \"model_temperature\": 1.0,\n      \"model_max_tokens\": 16384\n    }\n  }\n},\n\"show_entry\": true\n}\n</code></pre> <p>Tags</p> <p>Remember to add tags to your agent to make it easier for others to find it in the registry, for example:</p> <pre><code>{ \"tags\": [\"travel\", \"assistant\", \"vacation\"] }\n</code></pre> <p>Danger</p> <p>All files in this folder will be uploaded to the registry which is PUBLIC! Make sure you are not including any sensitive data.</p> <p>Warning</p> <p>You can't remove or overwrite a file once it's uploaded, but you can hide the entire agent by setting the <code>\"show_entry\": false</code> field in the <code>metadata.json</code> file</p>"},{"location":"agents/running/","title":"Running an Agent","text":"<p>Agents can be run locally or remotely. When running locally, you can run them interactively or as a task. When running remotely, you can use the NEAR AI API to run them.</p>"},{"location":"agents/running/#running-the-agent-locally","title":"Running the Agent Locally","text":"<p>You can execute it in two different ways: interactively or as a task.</p> <p>Know that you can also run agents directly on the Web AI Hub, you don't need to download an agent if you just want to see how they work.</p> <p>Always Review the Code</p> <p>Agents can execute arbitrary code on your machine, so please always review the agent's code before running it!</p> <p>By default, you will find the agent's code on the local directory <code>~/.nearai/registry</code>, there, check the agent's <code>agent.py</code> file either by using the <code>cat</code> command or opening it in a text editor.</p> <pre><code># Checking gagdiez.near/hello-ai/latest code\ncd ~/.nearai/registry/gagdiez.near/hello-ai/latest\ncat agent.py\n</code></pre>"},{"location":"agents/running/#interactive-run","title":"Interactive Run","text":"<p>Interactive runs execute the agent on a loop, allowing you to chat with it interactively until you decide to exit (using the <code>exit</code> command), or quit the session using <code>ctrl+c</code>.</p> <pre><code># Download the agent from the registry\nnearai registry download gagdiez.near/hello-ai/latest\n\n# Running the agent by absolute path\nnearai agent interactive ~/.nearai/registry/gagdiez.near/hello-ai/latest --local\n</code></pre>"},{"location":"agents/running/#running-as-a-task","title":"Running as a Task","text":"<p>When running an agent as a task, we simply provide an input and let the agent execute it without any user interaction.</p> <pre><code>nearai agent task ~/.nearai/registry/gagdiez.near/hello-ai/latest \"write a poem about the sorrow of losing oneself, but end on a positive note\" --local\n</code></pre>"},{"location":"agents/running/#running-the-agent-remotely","title":"Running the Agent Remotely","text":"<p>Agents can be run through the <code>/thread/runs</code>, <code>/thread/{thread_id}/runs</code> or  <code>/agent/runs</code> endpoints. The /thread syntax matches the OpenAI / LangGraph API. The /agent syntax is NEAR AI specific.</p> <p>You will need to pass a signed message to authenticate. This example uses the credentials written by <code>nearai login</code> to your <code>~/.nearai/config.json</code> file.</p> <pre><code>auth_json=$(jq -c '.auth' ~/.nearai/config.json);\n\ncurl \"https://api.near.ai/v1/threads/runs\" \\\n      -X POST \\\n      --header 'Content-Type: application/json' \\\n      --header \"Authorization: Bearer $auth_json\" \\\n-d @- &lt;&lt;'EOF'\n  {\n    \"agent_id\": \"flatirons.near/xela-agent/5.0.1\",\n    \"new_message\":\"Build a backgammon game\"\n  }\nEOF\n</code></pre> <p>The full message will look like this. A <code>thread_id</code> param can also be passed to continue a previous conversation.  <pre><code>curl \"https://api.near.ai/v1/threads/runs\" \\\n      -X POST \\\n      --header 'Content-Type: application/json' \\\n      --header 'Authorization: Bearer {\"account_id\":\"your_account.near\",\"public_key\":\"ed25519:YOUR_PUBLIC_KEY\",\"signature\":\"A_REAL_SIGNATURE\",\"callback_url\":\"https://app.near.ai/\",\"message\":\"Welcome to NEAR AI Hub!\",\"recipient\":\"ai.near\",\"nonce\":\"A_UNIQUE_NONCE_FOR_THIS_SIGNATURE\"}' \\\n-d @- &lt;&lt;'EOF'\n  {\n    \"agent_id\": \"flatirons.near/xela-agent/5.0.1\",\n    \"thread_id\": \"a_previous_thread_id\",\n    \"new_message\":\"Build a backgammon game\"\n  }\nEOF\n</code></pre></p> Remote results <p>The results of the /agent/runs endpoints are either an error or the resulting thread_id.</p> <p>\"thread_579e1cf3f42742c785218106\"</p> <p>Threads follow the OpenAI / LangGraph api standard. <code>/threads/{thread_id}/messages</code> will return the messages on the thread. See the full NEAR AI OpenAPI spec here: https://api.near.ai/openapi.json</p>"},{"location":"agents/streaming/","title":"Agent Streaming","text":""},{"location":"agents/streaming/#what-is-agent-streaming","title":"What is Agent Streaming?","text":"<p>Agent streaming enables output from your agent in a continuous, incremental stream, rather than waiting for a complete one-time response. This improves the user experience by providing immediate feedback and preventing long wait times especially for complex tasks such as multi-step reasoning, data analysis, or tool-based interactions.</p>"},{"location":"agents/streaming/#getting-started","title":"Getting Started","text":"<p>To use agent streaming, first implement streaming completions in your agent's code by passing <code>stream=True</code> to your <code>completion</code> function:</p> <pre><code>    result = self.env.completion([prompt] + messages, stream=True)\n</code></pre> <p>( See streaming completions for more details.)</p> <p>Once complete, you can now view these streams in the UI or CLI, each having their own unique way to enable.</p>"},{"location":"agents/streaming/#enable-ui-streaming","title":"Enable UI Streaming","text":"<p>To enable agent streaming in the UI, add <code>\"show_streaming_message\": true</code> to the <code>agent</code> details within your agent's <code>metadata.json</code> file:</p> <pre><code>  \"details\": {\n    \"agent\": {\n      \"show_streaming_message\": true\n    }\n  }\n</code></pre> <p>See full example below</p> <p>The UI automatically receives the same stream as the agent. A counter of deltas received and optionally the stream of text itself are shown to the user.</p> <p></p> <p>Note</p> <p>If your agent produces intermediate, non-user-facing completions (e.g., during tool calls or complex reasoning steps), you might set <code>\"show_streaming_message\": false</code> to prevent partial or internal messages from being displayed directly to the user in the UI.</p> <p>It's also important to note that the streaming text is immediately replaced upon the next message, so some applications might still prefer <code>true</code> even with intermediate steps.</p>"},{"location":"agents/streaming/#enable-cli-streaming","title":"Enable CLI Streaming","text":"<p>To enable agent streaming in the CLI use the <code>--stream=True</code> flag when running <code>nearai agent interactive</code>.</p> <ul> <li>This method only has display mode (show each chunk of text) and only shows the final messages.</li> <li>Use this flag each time you run an agent as it does not use the <code>show_streaming_message</code> setting from your <code>metadata.json</code></li> </ul> <p>Note</p> <p>As with UI streaming, you must first implement streaming completions in your agent's code before using this command. When working with different agent types (streaming &amp; non-streaming) the following behavior is expected:</p> <ul> <li>An agent that does not stream completions that is run in <code>nearai agent interactive</code> mode with <code>--stream=True</code> will show no output.</li> <li>An agent that does stream completions will by default show the final message only. When passed <code>--stream=True</code> it will show the text as it is received.</li> </ul>"},{"location":"agents/streaming/#streaming-completions","title":"Streaming Completions","text":"<p>To stream completion chunks to your agent, pass <code>stream=True</code> when using the <code>completion</code> function:</p> <p>Example:</p> <pre><code>    result = self.env.completion([prompt] + messages, stream=True) # stream the completions!\n    self.env.add_reply(result) # write the full message to the thread as normal\n</code></pre> <p>These chunks are also persisted temporarily on the Agent Cloud (Hub) and made available to clients through  the <code>/threads/{thread_id}/stream/{run_id}</code> endpoint.</p> <ul> <li>Chunk: One or more tokens streamed from the LLM to the agent.</li> <li>Delta: An SSE event that contains a chunk, streamed to clients.</li> </ul> <p>See full example below for more</p>"},{"location":"agents/streaming/#agent-usage","title":"Agent Usage","text":"<p>Within the agent the completion function returns a <code>StreamHandler</code> object when <code>stream=True</code>.  This can be iterated over or passed to streaming libraries that accept a <code>StreamHandler</code>.</p> <pre><code>    response_stream = self.env.completion([prompt] + messages, stream=True)\n\n    for event in response_stream:\n        for _idx, chunk in enumerate(response_stream):\n            c = json.dumps(chunk.model_dump())\n            print(c)\n            # do something with the chunk\n\n            # if part of a chain of async calls you could yield the chunk or evaluate or modify it and yield\n            # yield chunk\n    self.env.add_reply(response_stream) # write the full message to the thread\n</code></pre> <p>See full example below for more</p>"},{"location":"agents/streaming/#multiple-streaming-invocations","title":"Multiple Streaming Invocations","text":"<p>A single agent run can contain multiple streaming completion calls!</p> <p>Here is an example where two different personas are passed the same conversation history:</p> <pre><code>        prompt = {\"role\": \"system\", \"content\": \"respond as though you were Socrates\"}\n        messages = self.env.list_messages()\n\n        result = self.env.completion([prompt] + messages, stream=True)\n        self.env.add_reply(result)\n\n        prompt2 = {\"role\": \"system\", \"content\": \"Now, respond as though you were Plato\"}\n        result2 = self.env.completion([prompt2] + messages, stream=True)\n        self.env.add_reply(result2)\n</code></pre> <p>See full example below for more</p>"},{"location":"agents/streaming/#streaming-api","title":"Streaming API","text":"<p>Calls to the <code>/threads/{thread_id}/stream/{run_id}</code> endpoint return an SSE EventStream of deltas and thread events. These events are compatible with OpenAI Thread streaming events, https://platform.openai.com/docs/api-reference/assistants-streaming/events.</p> <p>Here is an example of how a React client might use this API endpoint:</p> <pre><code>  const startStreaming = (threadId: string, runId: string) =&gt; {\n    if (!threadId) return;\n    if (!runId) return;\n\n    setIsStreaming(true);\n\n    const eventSource = new EventSource(\n      `/api/v1/threads/${threadId}/stream/${runId}`,\n    );\n\n    eventSource.addEventListener('message', (event) =&gt; {\n      const data = JSON.parse(event.data);\n      const eventType = data.event;\n\n      switch (eventType) {\n        case 'thread.message.delta':\n          if (!data?.data?.delta?.content) return;\n          const content = data.data.delta.content;\n          if (content.length === 0 || !content[0]?.text?.value) return;\n          const latestChunk = data.data.delta.content[0].text.value;\n          setStreamingText((prevText) =&gt; prevText + latestChunk);\n          setStreamingTextLatestChunk(latestChunk);\n\n          // Force React to rerender immediately rather than batching\n          setTimeout(() =&gt; {}, 0);\n          break;\n\n        case 'thread.message.completed':\n          setStreamingText('');\n          setStreamingTextLatestChunk('');\n          break;\n        case 'thread.run.completed':\n        case 'thread.run.error':\n        case 'thread.run.canceled':\n        case 'thread.run.expired':\n        case 'thread.run.requires_action':\n          stopStreaming(eventSource);\n          break;\n      }\n    });\n\n    eventSource.onerror = (error) =&gt; {\n      console.log('SSE error:', error);\n      eventSource.close();\n      setIsStreaming(false);\n    };\n\n    setStream(eventSource);\n\n    return () =&gt; {\n      eventSource.close();\n    };\n  };\n</code></pre>"},{"location":"agents/streaming/#advanced-streaming-faqs","title":"Advanced Streaming / FAQs","text":"<ul> <li>Child threads do not currently support streaming thus invoking another agent on a child thread will not stream.</li> <li>Agent initiated Deltas<ul> <li>Writing delta events from the agent to the agent stream is not currently supported.</li> </ul> </li> <li>Tools: <ul> <li>There is not currently any special handling of tool calls.</li> <li>Tool call responses vary between providers and models, thus some models will have tool calls in the main response     and others will not.</li> <li>Depending on your use case you may want to separate tool calls from the main response rather than requesting      both in the same completion call.</li> </ul> </li> </ul>"},{"location":"agents/streaming/#full-agent-streaming-example","title":"Full Agent Streaming Example","text":"agent.pymetadata.json <pre><code>from nearai.agents.environment import Environment\n\nclass Agent:\n    def __init__(self, env: Environment):\n        self.env = env\n\n    def run(self):\n        prompt = {\"role\": \"system\", \"content\": \"respond as though you were Socrates\"}\n        messages = self.env.list_messages()\n\n        # Pass stream=True to enable streaming of deltas\n        # They will then show automatically in the UI or can be fetched at /threads/{thread_id}/stream/{run_id}\n        result = self.env.completion([prompt] + messages, stream=True)\n        self.env.add_reply(result)\n\n        prompt2 = {\"role\": \"system\", \"content\": \"Now, respond as though you were Plato\"}\n        result2 = self.env.completion([prompt2] + messages, stream=True)\n        self.env.add_reply(result2)\n\nif globals().get('env', None):\n    agent = Agent(globals().get('env'))\n    agent.run()\n</code></pre> <pre><code>  \"name\": \"streaming-example\",\n  \"version\": \"0.0.3\",\n  \"category\": \"agent\",\n  \"description\": \"Demonstrates streaming agent runs.\",\n  \"tags\": [\"streaming\"],\n  \"details\": {\n    \"display_name\": \"Streaming Example\",\n    \"icon\": \"https://static.thenounproject.com/png/1677760-200.png\",\n    \"agent\": {\n      \"show_streaming_message\": true,\n      \"welcome\": {\n        \"title\": \"Example of streaming agent runs\",\n        \"description\": \"I respond as Socrates then as Plato.\"\n      },\n      \"defaults\": {\n        \"model\": \"llama-v3p3-70b-instruct\",\n        \"model_max_tokens\": 4000,\n        \"model_provider\": \"fireworks\"\n      }\n    },\n    \"capabilities\": []\n  },\n  \"show_entry\": true\n}\n</code></pre>"},{"location":"agents/threads/","title":"Threads","text":"<p>Every agent execution happens within a conversation thread, which is isolated from other threads. Threads allow agents to maintain a message history and persist files in time so the user can continue the conversation later.</p> <p>Info</p> <p>You can find how agents persist messages and files in the Environment: Messages &amp; Files section.</p>"},{"location":"agents/threads/#starting-a-thread","title":"Starting a Thread","text":"<p>If we start an agent without specifying an existing thread, a new thread is created. Let's try this by executing an agent using the interactive mode:</p> <pre><code>nearai agent interactive ~/.nearai/registry/&lt;your-account.near&gt;/hello-ai/0.0.1 --local\n\n&gt; Hello, my name is Guille, please remember it\n\n# Example Output:\n# ...\n# thread_id: thread_43c64803c0a948bc9a8eb8e8\n\n# Assistant: Nice to meet you, Guille! I've made a note of your name, so feel free to ask me anything or start a conversation, and I'll be sure to address you by your name throughout our chat. How's your day going so far, Guillermo?\n</code></pre> <p>We can see in the output that a new <code>thread_id</code> - <code>thread_43c64803c0a948bc9a8eb8e8</code> - was created for this conversation.</p>"},{"location":"agents/threads/#resuming-a-thread","title":"Resuming a Thread","text":"<p>If we want to resume a conversation thread with an agent, we can specify the thread ID when running the agent:</p> <pre><code>nearai agent interactive ~/.nearai/registry/&lt;your-account.near&gt;/hello-ai/0.0.1 --local --thread_id thread_43c64803c0a948bc9a8eb8e8\n\n&gt; What is my name?\n\n# Assistant: Your name is Guille\n</code></pre>"},{"location":"agents/threads/#messages-and-files","title":"Messages and Files","text":"<p>Agents can access and add messages and files on each thread, learn more about it in the Environment: Messages &amp; Files section.</p>"},{"location":"agents/vector-stores/","title":"Vector Stores","text":"<p>Vector Stores are special databases that allow retrieving documents with natural language. They are very useful to provide agents with knowledge on which they have not been trained.</p> <p>Internally, they use AI models to convert text into low-dimensional vectors known as <code>embeddings</code>. Similar text will shield similar <code>embeddings</code>, allowing us to find relevant documents for a query by comparing the query's <code>embedding</code> with the documents' <code>embeddings</code> in the store.</p> <p>This page describes how to create, upload, query, and manage a Vector Store in NEAR AI, and how to integrate it on agents to generate context-aware responses.</p> <p>Tip</p> <p>Read our tutorial on Vector Stores to discover how you can build an agent that answers questions based on your custom data</p>"},{"location":"agents/vector-stores/#1-import-and-configure","title":"1. Import and Configure","text":"<p>To use Vector Stores we need to import the NEAR AI module, load configuration settings, and initialize an OpenAI-compatible client which we will connect to the NEAR AI endpoint.</p> <pre><code>import json\nfrom glob import glob\n\nimport openai\nimport nearai\n\n# Load NEAR AI Hub configuration\nconfig = nearai.config.load_config_file()\nbase_url = config.get(\"api_url\", \"https://api.near.ai/\") + \"v1\"\nauth = config[\"auth\"]\n\nclient = openai.OpenAI(base_url=base_url, api_key=json.dumps(auth))\n</code></pre>"},{"location":"agents/vector-stores/#2-creating-a-vector-store","title":"2. Creating a Vector Store","text":"<p>Assuming you have all the files you want to add to the vector store in a local directory containing your files, you will need to upload them to NEAR AI, and assign them to a Vector Store.</p> <p>Once assigned, the files will be processed by the NEAR AI platform to automatically generate the embeddings </p> <pre><code># Load all .md files in the current directory\nmd_files = list(glob(\"./**/*.md\", recursive=True))\n\nfile_ids = []\nfor file_path in md_files:\n    print(f\"Processing {file_path}\")\n\n    with open(file_path, 'r', encoding='utf-8') as file:\n        uploaded_file = client.files.create(\n            file=(file_path, file.read(), \"text/markdown\"),\n            purpose=\"assistants\"\n        )\n        file_ids.append(uploaded_file.id)\n\nvs = client.vector_stores.create(\n    name=\"my-vector-store\",\n    file_ids=file_ids,\n    # chunking_strategy={\n    #     \"chunk_overlap_tokens\": 400,\n    #     \"max_chunk_size_tokens\": 800\n    # }\n)\n\nprint(f\"Vector store created: {vs.id}\")\n</code></pre> <p>Chunking Strategy</p> <p>Before converting documentation into vector embeddings, the NEAR AI platform segments the text into smaller, manageable chunks. The optional <code>chunking_strategy</code> parameter allows you to customize this process.</p> <p>By default, the platform will split the text into chunks of 800 tokens, with an overlap of 400 tokens. It is important to notice that chunks are not created based on the document's structure, but merely on counting tokens, which can lead to a loss of context. Feel free to adjust these values to better suit your data.</p> Processing single files <p>While less efficient for handling multiple files, you can upload and attach single files to the vector store.</p> <pre><code># Upload file\nuploaded_file = client.files.create(\n    file=(file_path, file.read(), \"text/markdown\"),\n    purpose=\"assistants\"\n)\n\n# Attach file to the vector store\nclient.vector_stores.files.create(\n    vector_store_id=vector_store.id,\n    file_id=uploaded_file.id,\n)\n</code></pre>"},{"location":"agents/vector-stores/#3-wait-for-processing","title":"3. Wait for Processing","text":"<p>After you attach files to a vector store they will be processed in the background. You can check the status of the vector store to see if the processing is complete.</p> <pre><code>while True:\n    status = client.vector_stores.retrieve(vector_store.id)\n    if status.file_counts.completed == 1:\n        break\n    time.sleep(1)\n\nprint(\"File processed! The vector store is ready.\")\n</code></pre>"},{"location":"agents/vector-stores/#4-delete-a-file","title":"4. Delete a File","text":"<p>You can delete a specific file from the vector store by providing both the store\u2019s and file\u2019s IDs. This operation removes the file and its embeddings, helping to manage and update your stored data.</p> <pre><code>client.vector_stores.files.delete(\n    vector_store_id=vector_store.id,\n    file_id=file_id\n)\n</code></pre>"},{"location":"agents/vector-stores/#5-query-the-vector-store","title":"5. Query the Vector Store","text":"<p>When building an agent, you can use the <code>env.query_vector_store()</code> function to retrieve documents from the vector store that are relevant to a user query.</p> agent.py<pre><code>def run(env: Environment):\n    user_query = env.list_messages()[-1][\"content\"]\n\n    # Query the Vector Store\n    vector_results = env.query_vector_store(VECTOR_STORE_ID, user_query)\n\nrun(env)\n</code></pre> <p>Agent Example</p> <p>Check our Docs AI Agent that uses a vector store to provide context-aware responses on NEAR Protocol documentation</p> Query Vector Store Result <p>The method returns a list of objects, each containing a <code>file_id</code>, <code>chunk_text</code>, and <code>distance</code> (similarity score), for example:</p> <pre><code>[\n    {\n      'file_id': 'file_278138cf53a245558766c31d',\n      'chunk_text': 'nMachine learning and data mining often employ the same methods and overlap significantly ...',\n      'distance': 0.6951444615680473\n    },\n    ...\n]\n</code></pre>"},{"location":"agents/vector-stores/#6-generate-an-llm-response-with-context","title":"6. Generate an LLM Response with Context","text":"<p>We can use the retrieved documents to provide a context for the AI model that generates the response.</p> <pre><code>import json\nfrom nearai.agents.environment import Environment\n\nMODEL = \"llama-v3p3-70b-instruct\"\nVECTOR_STORE_ID = \"vs_cb8d5537f64d4f4aa6cbc95f\"\n\ndef run(env: Environment):\n    user_query = env.list_messages()[-1][\"content\"]\n\n    # Query the Vector Store\n    vector_results = env.query_vector_store(VECTOR_STORE_ID, user_query)\n    docs = [{\"file\": res[\"chunk_text\"]} for res in vector_results[:6]]\n\n    prompt = [\n        {\n            \"role\": \"user query\",\n            \"content\": user_query,\n        },\n        {\n            \"role\": \"documentation\",\n            \"content\": json.dumps(docs),\n        },\n        {\n            \"role\": \"system\",\n            \"content\": \"Give a brief but complete answer to the user's query, staying as true as possible to the documentation SPECIALLY when dealing with code.\"\n        }\n    ]\n\n    answer = env.completion(model=MODEL, messages=prompt)\n    env.add_reply(answer)\n\nrun(env)\n</code></pre> <p>Note that we are embedding the retrieved documents directly into the model's prompt, so the AI can use them to generate a more context-aware response. Furthermore, notice that we are not using all the documents retrieved, but only the first six, to reduce the amount of tokens in the prompt, and filter out less relevant documents.</p>"},{"location":"agents/env/frameworks/","title":"Supported Packages","text":"<p>NEAR AI supports several framework configurations, each with its own set of Python packages. Here is an overview of available frameworks, the setting value, and descriptions:</p> Framework Setting Description Minimal <code>minimal</code> Basic essential packages - <code>DEFAULT</code> Standard <code>standard</code> More robust Agent Framework TypeScript <code>ts</code> For creating agents with TypeScript AgentKit <code>agentkit</code> For use with LangChain, LangGraph, or Coinbase's Agentkit <p>Need a package that is not currently supported?</p> <p>If you have a particular package that is not currently supported, you can reach out to the team to have it added:</p> <ul> <li>Open a PR -&gt; (Example)</li> <li>File an issue</li> <li>Ask in Telegram</li> </ul>"},{"location":"agents/env/frameworks/#framework-usage","title":"Framework Usage","text":"<p>To use a specific framework, specify it in your agent's <code>metadata.json</code>:</p> <pre><code>{\n  \"details\": {\n    \"agent\": {\n      \"framework\": \"standard\"  // or \"minimal\", \"ts\", \"agentkit\", etc.\n    }\n  }\n}\n</code></pre>"},{"location":"agents/env/frameworks/#framework-types","title":"Framework Types","text":"<p>Below are up-to-date package support for each framework as defined in NEAR AI's AWS Runner Frameworks settings.</p>"},{"location":"agents/env/frameworks/#minimal-framework","title":"Minimal Framework","text":"<pre><code># Minimal Base Framework\n# Default for NEAR AI Agents\n\n# AWS SDK for Python\n# https://github.com/boto/boto3\nboto3 &gt;= 1.35.3\n\n# Base58 encoding/decoding library\n# https://github.com/keis/base58\nbase58 == 2.1.1\n\n# Cryptography library for secure encryption\n# https://github.com/pyca/cryptography\ncryptography==44.0.2\n\n# Datadog tracing library\n# https://github.com/DataDog/dd-trace-py\nddtrace == 2.21.0\n\n# Ed25519 cryptographic signatures\n# https://github.com/warner/python-ed25519\ned25519 == 1.5\n\n# LiteLLM for unified LLM API access\n# https://github.com/BerriAI/litellm\nlitellm &gt;= 1.60.5\n\n# Loguru for better logging\n# https://github.com/Delgan/loguru\nloguru == 0.7.2\n\n# Model Context Protocol integration for LLM apps and external data sources and tools\n# https://github.com/modelcontextprotocol\nmcp\n\n# OpenAI Python client\n# https://github.com/openai/openai-python\nopenai == 1.66.2\n\n# Process and system utilities\n# https://github.com/giampaolo/psutil\npsutil &gt;= 5.9.5\n\n# Data validation using Python type annotations\n# https://github.com/pydantic/pydantic\npydantic &gt;=2.8.2\n\n# Python binding to the Networking and Cryptography library\n# https://github.com/pyca/pynacl\npynacl &gt;= 1.5.0\n\n# NEAR Protocol Python SDK\n# https://github.com/pvolnov/py-near\npy-near &gt;= 1.1.50\n\n# Python dateutil for handling dates and times\n# https://github.com/dateutil/dateutil\npython_dateutil &gt;= 2.5.3\n\n# World timezone definitions\n# https://github.com/stub42/pytz\npytz == 2024.2\n\n# Rich library for rich text and beautiful formatting in the terminal\n# https://github.com/Textualize/rich\nrich &gt;= 13.7.1\n\n# Package development tools\n# https://github.com/pypa/setuptools\nsetuptools &gt;= 21.0.0\n\n# Retry logic with exponential backoff\n# https://github.com/jd/tenacity\ntenacity == 9.0.0\n\n# Twitter API wrapper\n# https://github.com/tweepy/tweepy\ntweepy &gt;= 4.14.0\n\n# Type hinting support\n# https://github.com/python/typing_extensions\ntyping-extensions &gt;= 4.7.1\n\n# HTTP client\n# https://github.com/urllib3/urllib3\nurllib3 &gt;= 1.25.3, &lt; 2.6.0\n</code></pre>"},{"location":"agents/env/frameworks/#standard-framework","title":"Standard Framework","text":"<pre><code># Standard Web Agent Framework\n# Extended framework for web-enabled NEAR AI Agents\n\n# Asynchronous HTTP client/server framework\n# https://github.com/aio-libs/aiohttp\naiohttp==3.11.0b0\n\n# Base58 encoding/decoding library\n# https://github.com/keis/base58\nbase58 == 2.1.1\n\n# AWS SDK for Python\n# https://github.com/boto/boto3\nboto3 &gt;= 1.35.3\n\n# Borsh serialization format implementation\n# https://github.com/near/borsh-construct-py\nborsh-construct\n\n# Beautiful Soup for HTML parsing\n# https://pypi.org/project/beautifulsoup4/\nbs4\n\n# Character encoding detection\n# https://github.com/chardet/chardet\nchardet\n\n# Cryptography library for secure encryption\n# https://github.com/pyca/cryptography\ncryptography==44.0.2\n\n# Datadog tracing library\n# https://github.com/DataDog/dd-trace-py\nddtrace == 2.21.0\n\n# Ed25519 cryptographic signatures\n# https://github.com/warner/python-ed25519\ned25519 == 1.5\n\n# Account abstraction library for web3.py\n# https://github.com/ethereum/eth-account\neth-account &gt;= 0.13.5\n\n# Google Search API wrapper\n# https://github.com/Nv7-GitHub/googlesearch\ngooglesearch-python &gt;= 1.2.5\n\n# LiteLLM for unified LLM API access\n# https://github.com/BerriAI/litellm\nlitellm &gt;= 1.60.5\n\n# Loguru for better logging\n# https://github.com/Delgan/loguru\nloguru == 0.7.2\n\n# Mutagen for audio/video metadata extraction\n# https://github.com/quodlibet/mutagen\nmutagen==1.47.0\n\n# NumPy for numerical computing\n# https://github.com/numpy/numpy\nnumpy &gt;= 1.21.0\n\n# OpenAI Python client\n# https://github.com/openai/openai-python\nopenai == 1.66.2\n\n# Pillow for image processing\n# https://github.com/python-pillow/Pillow\nPillow==11.2.1\n\n# Process and system utilities\n# https://github.com/giampaolo/psutil\npsutil &gt;= 5.9.5\n\n# Data validation using Python type hints\n# https://github.com/pydantic/pydantic\npydantic==2.9.2\n\n# Python binding to the Networking and Cryptography library\n# https://github.com/pyca/pynacl\npynacl &gt;= 1.5.0\n\n# NEAR Protocol Python SDK\n# https://github.com/pvolnov/py-near\npy-near &gt;= 1.1.50\n\n# Multibase encoding\n# https://github.com/multiformats/py-multibase\npy-multibase==1.0.3\n\n# Multicodec encoding\n# https://github.com/multiformats/py-multicodec\npy-multicodec==0.2.1\n\n# Python dateutil for handling dates and times\n# https://github.com/dateutil/dateutil\npython_dateutil &gt;= 2.5.3\n\n# World timezone definitions\n# https://github.com/stub42/pytz\npytz == 2024.2\n\n# Rich library for rich text and beautiful formatting in the terminal\n# https://github.com/Textualize/rich\nrich &gt;= 13.7.1\n\n# PDF file manipulation\n# https://github.com/py-pdf/pypdf\nPyPDF2\n\n# PDF file creation and manipulation\n# https://docs.reportlab.com/\nreportlab\n\n# Package development tools\n# https://github.com/pypa/setuptools\nsetuptools &gt;= 21.0.0\n\n# Retry logic with exponential backoff\n# https://github.com/jd/tenacity\ntenacity == 9.0.0\n\n# Twitter API wrapper\n# https://github.com/tweepy/tweepy\ntweepy &gt;= 4.14.0\n\n# Type hinting support\n# https://github.com/python/typing_extensions\ntyping-extensions &gt;= 4.7.1\n\n# HTTP client\n# https://github.com/urllib3/urllib3\nurllib3 &gt;= 1.25.3, &lt; 2.6.0\n\n# Web3.py for Ethereum interaction\n# https://github.com/ethereum/web3.py\nweb3 &gt;= 7.8.0\n\n# YouTube transcript API\n# https://github.com/jdepoix/youtube-transcript-api\nyoutube_transcript_api &gt;= 0.6.2\n</code></pre>"},{"location":"agents/env/frameworks/#typescript-framework","title":"TypeScript Framework","text":"<p>For use when creating TypeScript agents.</p> <pre><code># Minimal Base Framework\n# Default for NEAR AI Agents\n\n# AWS SDK for Python\n# https://github.com/boto/boto3\nboto3 &gt;= 1.35.3\n\n# Base58 encoding/decoding library\n# https://github.com/keis/base58\nbase58 == 2.1.1\n\n# Cryptography library for secure encryption\n# https://github.com/pyca/cryptography\ncryptography==44.0.2\n\n# Datadog tracing library\n# https://github.com/DataDog/dd-trace-py\nddtrace == 2.21.0\n\n# Ed25519 cryptographic signatures\n# https://github.com/warner/python-ed25519\ned25519 == 1.5\n\n# LiteLLM for unified LLM API access\n# https://github.com/BerriAI/litellm\nlitellm &gt;= 1.60.5\n\n# Loguru for better logging\n# https://github.com/Delgan/loguru\nloguru == 0.7.2\n\n# Model Context Protocol integration for LLM apps and external data sources and tools\n# https://github.com/modelcontextprotocol\nmcp\n\n# OpenAI Python client\n# https://github.com/openai/openai-python\nopenai == 1.66.2\n\n# Process and system utilities\n# https://github.com/giampaolo/psutil\npsutil &gt;= 5.9.5\n\n# Data validation using Python type annotations\n# https://github.com/pydantic/pydantic\npydantic &gt;=2.8.2\n\n# Python binding to the Networking and Cryptography library\n# https://github.com/pyca/pynacl\npynacl &gt;= 1.5.0\n\n# NEAR Protocol Python SDK\n# https://github.com/pvolnov/py-near\npy-near &gt;= 1.1.50\n\n# Python dateutil for handling dates and times\n# https://github.com/dateutil/dateutil\npython_dateutil &gt;= 2.5.3\n\n# World timezone definitions\n# https://github.com/stub42/pytz\npytz == 2024.2\n\n# Rich library for rich text and beautiful formatting in the terminal\n# https://github.com/Textualize/rich\nrich &gt;= 13.7.1\n\n# Package development tools\n# https://github.com/pypa/setuptools\nsetuptools &gt;= 21.0.0\n\n# Retry logic with exponential backoff\n# https://github.com/jd/tenacity\ntenacity == 9.0.0\n\n# Twitter API wrapper\n# https://github.com/tweepy/tweepy\ntweepy &gt;= 4.14.0\n\n# Type hinting support\n# https://github.com/python/typing_extensions\ntyping-extensions &gt;= 4.7.1\n\n# HTTP client\n# https://github.com/urllib3/urllib3\nurllib3 &gt;= 1.25.3, &lt; 2.6.0\n</code></pre>"},{"location":"agents/env/frameworks/#agentkit-framework","title":"AgentKit Framework","text":"<p>For use with LangChain, LangGraph, or Coinbase's Agentkit</p> <pre><code># AgentKit Framework\n# For use with LangChain, LangGraph, or Coinbase's Agentkit\n\n# Base58 encoding/decoding library\n# https://github.com/keis/base58\nbase58 == 2.1.1\n\n# AWS SDK for Python\n# https://github.com/boto/boto3\nboto3 &gt;= 1.35.3\n\n# Coinbase SDK for crypto integration\n# https://github.com/coinbase/cdp-sdk-python\ncdp-sdk &gt;= 0.15.0\n\n# Coinbase AgentKit Integration\n# https://github.com/coinbase/agentkit\ncoinbase-agentkit &gt;= 0.4.0\n\n# LangChain integration for Coinbase AgentKit\n# https://github.com/coinbase/agentkit\ncoinbase-agentkit-langchain &gt;= 0.3.0\n\n# Cryptography library for secure encryption\n# https://github.com/pyca/cryptography\ncryptography==44.0.2\n\n# Datadog tracing library\n# https://github.com/DataDog/dd-trace-py\nddtrace == 2.21.0\n\n# DuckDuckGo search API\n# https://github.com/deedy5/duckduckgo_search\nduckduckgo-search &gt;= 7.3.0\n\n# Ed25519 cryptographic signatures\n# https://github.com/warner/python-ed25519\ned25519 == 1.5\n\n# LangChain Agent Development Framework\n# https://github.com/langchain-ai/langchain\nlangchain &gt;= 0.3.4\nlangchain-community &gt;= 0.3.16\nlangchain-openai &gt;= 0.2.4\n\n# LangGraph for agent orchestration\n# https://github.com/langchain-ai/langgraph\nlanggraph &gt;= 0.2.39\n\n# LiteLLM for unified LLM API access\n# https://github.com/BerriAI/litellm\nlitellm &gt;= 1.41.0\n\n# Loguru for better logging\n# https://github.com/Delgan/loguru\nloguru == 0.7.2\n\n# NEAR AI LangChain integration\n# https://github.com/nearai/nearai_langchain\nnearai-langchain &gt;= 0.0.13\n\n# Process and system utilities\n# https://github.com/giampaolo/psutil\npsutil &gt;= 5.9.5\n\n# Data validation using Python type annotations\n# https://github.com/pydantic/pydantic\npydantic &gt;=2.8.2\n\n# Python binding to the Networking and Cryptography library\n# https://github.com/pyca/pynacl\npynacl &gt;= 1.5.0\n\n# Python dateutil for handling dates and times\n# https://github.com/dateutil/dateutil\npython_dateutil &gt;= 2.5.3\n\n# World timezone definitions\n# https://github.com/stub42/pytz\npytz == 2024.2\n\n# Rich library for rich text and beautiful formatting in the terminal\n# https://github.com/Textualize/rich\nrich &gt;= 13.7.1\n\n# NEAR Protocol Python SDK\n# https://github.com/pvolnov/py-near\npy-near &gt;= 1.1.50\n\n# Package development tools\n# https://github.com/pypa/setuptools\nsetuptools &gt;= 21.0.0\n\n# Twitter API\n# https://github.com/tweepy/tweepy\ntweepy &gt;= 4.14.0\n\n# Type hinting support\n# https://github.com/python/typing_extensions\ntyping-extensions &gt;= 4.7.1\n\n# HTTP client\n# https://github.com/urllib3/urllib3\nurllib3 &gt;= 1.25.3\n\n# Web3.py for Ethereum interaction\n# https://github.com/ethereum/web3.py\nweb3 &gt;= 7.6.0\n</code></pre>"},{"location":"agents/env/inference/","title":"Inference","text":"<p>The <code>completion</code> method is used to run a prompt on a specific model, using a specific provider.</p> <p>If only the prompt is provided, the inference will be run on the model and provider specified in the agent's metadata.</p> <pre><code>messages = env.list_messages()\nresult = env.completion(messages)\n\nprint(\"Messages:\", messages)\nprint(\"Result:\", result)\n</code></pre> Example Output <pre><code>Messages: [{'id': 'msg_1149aa85884b4fe8abc7d859', 'content': 'Hello', 'role': 'user'}]\n\nResult: Hello! It's nice to meet you. Is there something I can help you with or would you like to chat?\n</code></pre>"},{"location":"agents/env/inference/#generating-an-image","title":"Generating an Image","text":"<p>The <code>generate_image</code> method is used to generate an image based on the provided description.</p> <pre><code>description = \"A puppy in the garden\"\nimage = env.generate_image(description)\n\n# Extract the base64 data from the first item in `data`\nb64_data = image.data[0].b64_json\n\n# Decode the base64 image data\nimage_data = base64.b64decode(b64_data)\n\n# Write the image file\nimage_file = env.write_file(\"puppy_image.png\", image_data)\n</code></pre> <p>Agent Example</p> <p>Check out this Agent example to learn how to use <code>generate_image</code> in your AI agent logic.</p>"},{"location":"agents/env/inference/#overriding-the-default-model","title":"Overriding the Default Model","text":"<p>To run the inference on a model different from the default one, you can pass the <code>MODEL</code> or <code>PROVIDER::MODEL</code> as second argument:</p> <pre><code>messages = env.list_messages()\nresult = env.completion([prompt] + messages, \"fireworks::qwen2p5-72b-instruct\")\n</code></pre> Example Output <pre><code>Messages: [{'id': 'msg_1149aa85884b4fe8abc7d859', 'content': 'Hello', 'role': 'user'}]\n\nResult: Hello! How can I assist you today? Is there something specific you'd like to talk about or any questions you have?\n</code></pre> <p>Tip</p> <p><code>completions</code>: returns the full llm response for more control.</p> <p>Using Models Locally: LangChain / LangGraph</p> <p>The example agent langgraph-min-example has metadata that specifies the <code>langgraph-0-1-4</code> framework to run on langgraph version 1.4. In addition, the agent.py code contains an adaptor class, <code>AgentChatModel</code> that maps LangChain inference operations to <code>env.completions</code> calls.</p>"},{"location":"agents/env/messages_files/","title":"Messages and Files","text":"<p>Agents interact with the users through messages, and can also access and create files. This page provides an overview of how agents can work with messages and files.</p> Quick Overview <ul> <li>Each run of an agent is executed in a separate thread, which contains messages and files.</li> <li>The agent can access the messages in the current thread using <code>env.list_messages()</code>.</li> <li>The agent can save temporary files to track the progress of a task.</li> <li>By default, the entire message history is stored in a file named <code>chat.txt</code>. The agent can add messages there by using <code>env.add_reply()</code>.</li> <li>During its operation, the agent creates a file named <code>.next_agent</code>, which stores the role of the next participant expected in the dialogue (either <code>user</code> or <code>agent</code>) during the next iteration of the loop. The agent can control this value using <code>env.set_next_actor()</code>.</li> <li>The agent can use local imports from the home folder or its subfolders. It is executed from a temporary folder within a temporary environment.</li> </ul>"},{"location":"agents/env/messages_files/#thread-messages","title":"Thread Messages","text":"<p>The environment provides methods for agents to access and interact with the messages in the current thread. Messages are stored in a list, with each message containing an <code>id</code>, <code>content</code>, and <code>role</code> field.</p>"},{"location":"agents/env/messages_files/#accessing-messages","title":"Accessing Messages","text":"<p>Agents can access the messages from the current thread using the <code>list_messages</code> method:</p> agent.py<pre><code>def run(env: Environment):\n  messages = env.list_messages()\n  print(messages)\n</code></pre> Example Output <pre><code>[{'id': 'msg_9b676ae4ad324ca58794739d', 'content': 'Hi', 'role': 'user'},\n  {'id': 'msg_58693367bcee42669a85cb69', 'content': \"Hello! It's nice to meet you. Is there something I can help you with or would you like to chat?\", 'role': 'assistant'},\n  {'id': 'msg_16acda223c294213bc3c814e', 'content': 'help me decide how to decorate my house!', 'role': 'user'}]\n</code></pre>"},{"location":"agents/env/messages_files/#adding-messages","title":"Adding Messages","text":"<p>Agents can add new messages to the thread using the <code>add_reply</code> method:</p> agent.py<pre><code>def run(env: Environment):\n  env.add_reply(\"I have finished\")\n</code></pre>"},{"location":"agents/env/messages_files/#files","title":"Files","text":"<p>Agents have access to two types of files through the environment:</p> <ol> <li>Those created within the current conversation thread</li> <li>Those uploaded with the agent to the registry</li> </ol>"},{"location":"agents/env/messages_files/#storing-data","title":"Storing Data","text":"<p>To create a new file in the thread we can use the <code>write_file</code> method from the environment:</p> agent.py<pre><code>def run(env: Environment):\n  env.write_file('file.txt', 'hello thread')\n</code></pre> Where is the file stored? <p>When running the agent locally, a temporary folder will be created to store each thread data. We can check exactly where the file is stored by using the <code>python debugger</code>:</p> agent.py<pre><code>def run(env: Environment):\n  env.write_file('file.txt', 'hello thread')\n  import ipdb; ipdb.set_trace()  # Call the ipdb debugger\n</code></pre> <p>After running the agent, we will be dropped into the debugger, where we can check the current working directory:</p> <pre><code>nearai agent interactive ~/.nearai/registry/&lt;your-account.near&gt;/hello-ai/0.0.1 --local --thread_id thread_43c64803c0a948bc9a8eb8e8\n\nipdb&gt; import os; os.getcwd() # Check the current working directory\n'/private/var/folders/v6/pw4e3e3r5t6h8i9oihtd9d7d1234df/T/agent_7e312s678b987sa4vc4s2zxs2s1w1345'\n</code></pre> <p>We can see that the current working directory is a temporary folder. Go ahead and start the <code>agent</code> again without the <code>--thread_id</code> parameter, you will see that the working directory changes.</p>"},{"location":"agents/env/messages_files/#accessing-files","title":"Accessing Files","text":"<p>To list the files in the thread storage, we can use the <code>list_files_from_thread</code> method from the environment:</p> agent.py<pre><code>def run(env: Environment):\n  files = env.list_files_from_thread()\n  content = env.read_file('file.txt')\n\n  print('Files:', files)\n  print('Content of file.txt:', content)\n</code></pre> Example Output <pre><code>Files [FileObject(id='file_31aab645e3214a13b402e321', bytes=12, created_at=1734733634, filename='file.txt', object='file', purpose='assistants', status='uploaded', status_details='File information retrieved successfully')]\n\nContent of file.txt hello thread\n</code></pre>"},{"location":"agents/env/messages_files/#logging","title":"Logging","text":"<p>Logging is enabled by default. When running locally, a <code>.threads</code> folder is created inside an agent directory,  containing any files an agent creates, including logs.</p> <p>Example: <pre><code>Temp run folder created: /Users/antonlomonos/.nearai/registry/alomonos.near/example-logging-agent/0.0.1/.threads/agent_5782e02da54c4365a3c9ee27a64fc81c\nLogging system_log.txt at: /Users/antonlomonos/.nearai/registry/alomonos.near/example-logging-agent/0.0.1/.threads/agent_5782e02da54c4365a3c9ee27a64fc81c/system_log.txt\nLogging agent_log.txt at: /Users/antonlomonos/.nearai/registry/alomonos.near/example-logging-agent/0.0.1/.threads/agent_5782e02da54c4365a3c9ee27a64fc81c/agent_log.txt\nLogging chat_history_log.txt at: /Users/antonlomonos/.nearai/registry/alomonos.near/example-logging-agent/0.0.1/.threads/agent_5782e02da54c4365a3c9ee27a64fc81c/chat_history_log.txt\n</code></pre> These are temporary files and are deleted when exiting an interactive agent session. To keep these files, copy them into another folder before exiting the interactive session. Here are some options for adding more logs:</p> <ul> <li><code>add_system_log</code>: adds a system or environment log that is then saved into \"system_log.txt\".</li> <li><code>add_agent_log</code>: any agent logs may go here. Saved into \"agent_log.txt\".</li> </ul> <p>In the Agent Cloud (hub), every log file update appears at the bottom of the OUTPUT section. You can turn off agent logging by passing an environment variable of <code>DEBUG</code> with a value of <code>false</code>.  The 'show logs' button (next to send message) toggles all logs to be shown in the thread.</p>"},{"location":"agents/env/near/","title":"NEAR Blockchain","text":""},{"location":"agents/env/near/#interacting-with-near-blockchain","title":"Interacting with NEAR Blockchain","text":"<p>The NEAR AI toolkit provides environment methods for interacting with the NEAR blockchain. This framework is designed for seamless integration using a private RPC when running an agent on a NEAR AI hosted runner.</p>"},{"location":"agents/env/near/#features","title":"Features","text":"<ul> <li>Private RPC Integration: NEAR AI provides a private RPC for agents to interact with the NEAR blockchain. This private RPC is optimized for both read operations (e.g., querying contract states) and write operations (e.g., sending transactions or modifying contract state). It ensures secure and reliable communication between hosted agents and the NEAR network, reducing latency and improving overall performance.</li> <li>Retry Mechanism: Both view and call methods include a robust retry mechanism to handle transient network or RPC errors.</li> </ul>"},{"location":"agents/env/near/#asyncio-and-setting-up-the-near-account","title":"Asyncio and setting Up the Near Account","text":"<p>In order to interact with the blockchain we need to run our agent (or execute our blockchain interactions) in a <code>asyncio</code> event loop and set up an Account object from the <code>py-near</code> Python library.  More details: py-near Account</p> <pre><code>from nearai.agents.environment import Environment\nimport asyncio\n\nasync def run(env: Environment):\n    near = env.set_near(account_id, private_key)\n\n    prompt = {\"role\": \"system\", \"content\": \"\"}\n    result = env.completion([prompt] + env.list_messages())\n\n    balance = await near.get_balance(\"example.near\")\n\n    env.add_reply(f\"The NEAR balance of example.near is: {balance}\")\n    env.add_reply(result)\n\nasyncio.run(run(env))\n</code></pre> <p>Important</p> <p>Ensure that the <code>account_id</code>and <code>private_key</code> are never exposed in plain text within the agent's code. We recommend using secrets to handle these credentials securely.</p> <p>Parameters:</p> <ul> <li><code>account_id</code>: The NEAR account ID (e.g., \"example.near\") that will act as the account for interactions</li> <li><code>private_key</code>: The private key associated with the account_id</li> <li><code>rpc_addr</code>: (Optional) A custom RPC address for connecting to the NEAR network. If not provided, the default NEAR Mainnet RPC address is used.</li> </ul> <p>Example: <pre><code>near = env.set_near(\"account.near\", \"ed25519:3ABCD...XYZ\")\n</code></pre></p> <p>Once called, the <code>near</code> object is ready for use. </p>"},{"location":"agents/env/near/#near-view-method","title":"NEAR VIEW Method","text":"<p>Performs a read-only operation on the NEAR blockchain. This is used to query the state of a contract without modifying it. Examples include retrieving contract states, or querying other read-only data.</p> <p>The result object contains the transaction details, including the logs and block hash, and any returned values. For more details on the format of the result object, refer to the py-near documentation.</p> <p>Info</p> <p><code>near.view</code> can be used without providing <code>account_id</code> or <code>private_key</code> to <code>env.set_near()</code>.</p> <pre><code>near = env.set_near()\n\nawait near.view(\n    contract_id: str,\n    method_name: str,\n    args: dict,\n    block_id: Optional[int] = None,\n    threshold: Optional[int] = None,\n    max_retries: int = 3\n))\n</code></pre> <p>Parameters:</p> <ul> <li><code>contract_id</code>: The NEAR account ID of the smart contract you want to query.</li> <li><code>method_name</code>: The name of the view method to call on the contract.</li> <li><code>args</code>: A dictionary of arguments to pass to the view method.</li> <li><code>block_id</code>: (Optional) The block ID to query. Defaults to the latest block.</li> <li><code>threshold</code>: (Optional) A threshold parameter for advanced queries.</li> <li><code>max_retries</code>: (Optional) The maximum number of retry attempts in case of transient errors (default is 3, max is 10).</li> </ul> <p>Returns:</p> <ul> <li>The result of the view method call, typically containing the queried data.</li> </ul> <p>Example: <pre><code>near = env.set_near()\n\nresult = await near.view(\n    contract_id=\"wrap.near\",\n    method_name=\"ft_balance_of\",\n    args={\n        \"account_id\": \"user.near\"\n    }\n)\n\nprint(result)\nprint(\"Wrap.NEAR Balance:\", result.result)\n</code></pre></p> Output <p>{'block_hash': 'GT3RNuDyAf88GJKEuedNKAkEqZanfTkt2BhhQ11PXAoJ', 'block_height': 144201901, 'logs': [], 'result': '0'}</p> <p>'Wrap.NEAR Balance: 0'</p>"},{"location":"agents/env/near/#near-call-method","title":"NEAR CALL Method","text":"<p>Executes a state-changing operation on the NEAR blockchain. This is used to call methods on contracts that can modify state, transfer tokens, or perform other operations requiring gas and/or attached tokens.</p> <p>The result object contains the transaction details, including the status, transaction hash, and any returned values. For more details on the format of the result object, refer to the py-near documentation.</p> <pre><code>near = env.set_near(\"user.near\", \"ed25519:3ABCD...XYZ\")\n\nawait near.call(\n    contract_id: str,\n    method_name: str,\n    args: dict,\n    gas: int = DEFAULT_ATTACHED_GAS,\n    amount: int = 0,\n    nowait: bool = False,\n    included: bool = False,\n    max_retries: int = 3\n))\n</code></pre> <p>Parameters:</p> <ul> <li><code>contract_id</code>: The NEAR account ID of the smart contract you want to call.</li> <li><code>method_name</code>: The name of the method to invoke on the contract.</li> <li><code>args</code>: A dictionary of arguments to pass to the method.</li> <li><code>gas</code>: (Optional) The amount of gas to attach for execution (default: DEFAULT_ATTACHED_GAS). </li> <li><code>amount</code>: (Optional) The amount of NEAR tokens to attach to the transaction (default: 0). </li> <li><code>nowait</code>: (Optional) If True, the call will not wait for transaction confirmation. </li> <li><code>included</code>: (Optional) If True, ensures the transaction is included in the block before returning.</li> <li><code>max_retries</code>: (Optional) The maximum number of retry attempts in case of transient errors (default is 3, max is 10). Use this parameter only if necessary, as there is a risk that the transaction might be sent multiple times.</li> </ul> <p>Returns:</p> <ul> <li>The result of the contract method call, including transaction details and status.</li> </ul> <p>Example: <pre><code>near = env.set_near(\"user.near\", \"ed25519:3ABCD...XYZ\")\n\nresult = await near.call(\n    contract_id=\"wrap.near\",\n    method_name=\"ft_transfer\",\n    args={\n        \"receiver_id\": \"example.near\",\n        \"amount\": \"1000000\"\n    },\n    gas=30000000000000,\n    amount=1\n)\n\nif \"SuccessValue\" in result.status:\n    print(\"tx\", result.transaction.hash)\n</code></pre></p>"},{"location":"agents/env/near/#near-get-balance","title":"NEAR GET BALANCE","text":"<p><code>get_balance</code> retrieves the NEAR token balance of a given account. </p> <p>Parameters:</p> <ul> <li><code>account_id</code> : (Optional) The ID of the account to retrieve the balance for. If not provided, the balance of the current account is retrieved.</li> </ul> <p>Example: <pre><code>near = env.set_near(\"alice.near\")\n\nprint(await near.get_balance())\nprint(await near.get_balance(\"bob.near\"))\n</code></pre></p>"},{"location":"agents/env/overview/","title":"The Environment Object","text":"<p>Each time an agents executes it receives an environment, which gives it access to features such as:</p> <ul> <li>Retrieve messages in the conversation, both from the user and the agent</li> <li>Request input from the user</li> <li>Read and write files on the agent's storage</li> <li>Call other agents</li> </ul>"},{"location":"agents/env/overview/#available-variables","title":"Available variables","text":"<ul> <li><code>signer_account_id</code>: get the NEAR Account ID of the signer </li> </ul>"},{"location":"agents/env/tools/","title":"Tools &amp; Commands","text":"<p>NEAR AI supports function based tool calling where the LLM can decide to call one of the functions (Tools) that you pass it. You can register your own function or use any of the built-in tools (list_files, read_file, write_file, query_vector_store).</p> <p>The tool registry supports OpenAI style tool calling and Llama style. When a llama model is explicitly passed to completion(s)_and_run_tools a system message is added to the conversation. This system message contains the tool definitions and instructions on how to invoke them  using <code>&lt;function&gt;</code> tags.</p> <p>To tell the LLM about your tools and automatically execute them when selected by the LLM, call one of these environment methods:</p> <ul> <li><code>completion_and_run_tools</code>: Allows tools to be passed and processes any returned tool_calls by running the tool</li> <li><code>completions_and_run_tools</code>: Handles tool calls and returns the full llm response.</li> <li><code>completion_and_get_tools_calls</code>: Returns completion message and/or tool calls from OpenAI or Llama tool formats.</li> </ul> <p>By default, these methods will add both the LLM response and tool invocation responses to the message list.  You do not need to call <code>env.add_message</code> for these responses. This behavior allows the LLM to see its call then tool responses in the message list on the next run.  This can be disabled by passing <code>add_to_messages=False</code> to the method.</p>"},{"location":"agents/env/tools/#registering-tools","title":"Registering Tools","text":"<ul> <li><code>get_tool_registry</code>: returns the  tool registry, a dictionary of tools that can be called by the agent. By default it is populated with the tools listed above for working with files. To register a function as a new tool, call <code>register_tool</code> on the tool registry, passing it your function.</li> </ul> <p>The tool registry provides two ways to register tools:</p> <ol> <li> <p>Using <code>register_tool</code> for regular Python functions/callables: <pre><code>def my_tool():\n    \"\"\"A simple tool that returns a string. This docstring helps the LLM know when to call the tool.\"\"\"\n    return \"Hello from my tool\"\n\ntool_registry = env.get_tool_registry()\ntool_registry.register_tool(my_tool)\ntool_def = tool_registry.get_tool_definition('my_tool')\nresponse = env.completions_and_run_tools(messages, tools=[tool_def])\n</code></pre></p> </li> <li> <p>Using <code>register_mcp_tool</code> for MCP tools: <pre><code>from nearai.agents.models.tool_definition import MCPTool\n\nmcp_tool = MCPTool(\n    name=\"weather\",\n    description=\"Get the current weather in a location\",\n    inputSchema={\n        \"type\": \"object\",\n        \"properties\": {\n            \"location\": {\n                \"type\": \"string\",\n                \"description\": \"The city and state, e.g. San Francisco, CA\"\n            }\n        },\n        \"required\": [\"location\"]\n    }\n)\n\nasync def call_weather_api(name: str, args: dict):\n    # Implementation of the weather API call\n    return f\"Weather in {args['location']}: Sunny\"\n\ntool_registry.register_mcp_tool(mcp_tool, call_weather_api)\n</code></pre></p> </li> </ol> <p>To pass all the built in tools plus any you have registered use the <code>get_all_tool_definitions</code> method. <pre><code>all_tools = env.get_tool_registry().get_all_tool_definitions()\nresponse = env.completions_and_run_tools(messages, tools=all_tools)\n</code></pre> If you do not want to use the built-in tools, use <code>get_tool_registry(new=True)</code> <pre><code>    tool_registry = env.get_tool_registry(new=True)\n    tool_registry.register_tool(my_tool)\n    tool_registry.register_tool(my_tool2)\n    response = env.completions_and_run_tools(messages, tools=tool_registry.get_all_tool_definitions())\n</code></pre></p>"},{"location":"agents/env/tools/#terminal-commands","title":"Terminal Commands","text":"<p>Agents have access to the local terminal through the environment, the following methods are available:</p> Method Description <code>list_terminal_commands()</code> Lists the history of terminal commands executed by the agent"},{"location":"agents/env/variables/","title":"Secrets &amp; Environment Variables","text":"<p>NEAR AI's environment variable system lets you securely manage configuration and sensitive data for your agents. Both agent developers and users can easily configure variables without modifying the agent's code. </p>"},{"location":"agents/env/variables/#key-features","title":"Key Features","text":"<ul> <li>Flexible variable management: Set and manage variables as agent authors or users</li> <li>Hierarchical variable resolution: Users can override agent variables and secrets</li> <li>NEAR wallet-based authentication: Only authorized users can set and get secrets</li> </ul>"},{"location":"agents/env/variables/#public-variables","title":"Public Variables","text":"<p>Public variables are configuration values that are visible in code and metadata. </p>"},{"location":"agents/env/variables/#agent-public-variables","title":"Agent Public Variables","text":"<p>Agent public variables are defined by by the agent author in the agent's <code>metadata.json</code> file:</p> <pre><code>{\n  \"details\": {\n    \"env_vars\": {\n      \"API_ENDPOINT\": \"https://api.example.com\",\n    }\n  }\n}\n</code></pre>"},{"location":"agents/env/variables/#user-public-variables","title":"User Public Variables","text":"<p>User public variables are set by users via the CLI or URL parameters.</p> <pre><code># Via CLI\nnearai agent run my-agent --env_vars='{\"CUSTOM_ENDPOINT\":\"https://api.custom.com\"}'\n\n# Via URL parameters\nhttps://app.near.ai/agents/casino.near/game/1?refId=ad.near\n</code></pre>"},{"location":"agents/env/variables/#private-variables-secrets","title":"Private Variables (Secrets)","text":"<p>These are private variables that are securely stored and never exposed in agent code.</p>"},{"location":"agents/env/variables/#agent-secrets","title":"Agent Secrets","text":"<ul> <li>Set by agent authors</li> <li>Scoped to specific agent versions (ex: <code>v0.0.1</code> vs <code>v0.0.2</code>)</li> </ul>"},{"location":"agents/env/variables/#user-secrets","title":"User Secrets","text":"<ul> <li>Set by users for specific agents</li> <li>Can override agent secrets</li> <li>Accessible only to authorized runners</li> </ul>"},{"location":"agents/env/variables/#how-variables-are-merged","title":"How Variables are Merged","text":"<p>It's important to note that at runtime both agent author and user environment variables and secrets are merged into a single <code>env_vars</code> object. </p> <p>If there are conflicting variables with the same name, user variables will take precedence. This enables agent users to easily override any default value or secret set by the agent author such as an API key or API endpoint.</p> <p>Priority Order</p> <p>Variables are merged with this priority (highest to lowest), ultimately user variables take precedence:</p> <ol> <li>User Public Variables (CLI/URL Params)</li> <li>User Secrets</li> <li>Agent Secrets</li> <li>Agent Public Variables (metadata.json)</li> </ol> <p><code>Example</code>:</p> <pre><code># Given these variables:\nagent_secrets = {\"API_KEY\": \"agent-key\"}\nagent_public = {\"API_KEY\": \"metadata-key\"}\nuser_secrets = {\"API_KEY\": \"user-key\"}\nuser_public = {\"API_KEY\": \"cli-key\"}\n\n# Your agent sees:\nenv.env_vars[\"API_KEY\"] == \"cli-key\"  # Highest priority wins\n</code></pre>"},{"location":"agents/env/variables/#using-variables-in-agent-code","title":"Using Variables in Agent Code","text":"<p>Using variables in your agent is straightforward. You can access any variable in your agent code  using Python\u2019s os module or by accessing the <code>env_vars</code> dictionary directly using the <code>env.env_vars</code> object.</p> <p><code>Examples</code>:</p> <pre><code># Access any variable\napi_key = env.env_vars.get('VARIABLE_NAME', 'default-value')\n\n# Using env.env_vars\nvalue = env.env_vars.get('VARIABLE_NAME', 'default_value')\n\n# Using os.environ\nimport os\nvalue = os.environ.get('VARIABLE_NAME', 'default_value')\n\n# Or using globals()\nvalue = globals()['env'].env_vars.get('VARIABLE_NAME', 'default_value')\n</code></pre> <p>You can also check if a variable exists:</p> <pre><code>if 'VARIABLE_NAME' in env.env_vars:\n    # Use API key\n    api_key = env.env_vars['VARIABLE_NAME']\n</code></pre>"},{"location":"agents/env/variables/#managing-secrets","title":"Managing Secrets","text":"<p>All secret variables are managed through the NEAR AI platform and require a NEAR account to access. This ensures that only authorized users can access sensitive information and that the variables are stored securely. </p> <p>You can manage these secrets at a lower level by using the secrets API, but the NEAR AI Developer Hub provides a more user-friendly interface for managing these variables.</p>"},{"location":"agents/env/variables/#using-developer-hub","title":"Using Developer Hub","text":"<p>The easiest way to manage variables is through app.near.ai:</p> <ol> <li>Select your agent</li> <li>Click the \"Run\" tab</li> <li>Under \"Environment Variables\":<ul> <li>Click \"+\" to add new variable</li> <li>Enter key-value pair</li> <li>Choose visibility (public/private)</li> </ul> </li> </ol> <p></p>"},{"location":"agents/env/variables/#using-cli","title":"Using CLI","text":"<p>Local Development Only</p> <p>For local development and testing only as the CLI does not upload secret variables to the NEAR AI platform. </p> <p>Also note that using <code>--env_vars</code> flag only works for a single run.</p> <pre><code># Set variables for a single run\nnearai agent interactive &lt;AGENT-PATH&gt; --env_vars='{\"API_KEY\":\"sk-...\",\"ENDPOINT\":\"https://api.custom.com\"}'\n</code></pre>"},{"location":"agents/env/variables/#secrets-api","title":"Secrets API","text":"<p>For programmatic management of secrets, you can use the following API endpoints:</p> Endpoint Method Description <code>/v1/get_user_secrets</code> GET Retrieve user secrets <code>/v1/create_hub_secret</code> POST Create a new secret <code>/v1/remove_hub_secret</code> POST Delete an existing secret"},{"location":"agents/env/variables/#get-v1get_user_secrets","title":"<code>GET /v1/get_user_secrets</code>","text":"<p>Retrieves secrets belonging to the authenticated user. (via <code>owner_namespace</code>)</p> <p>Params:</p> <ul> <li><code>limit</code>: Optional (default: 100)</li> <li><code>offset</code>: Optional (default: 0)</li> </ul> <p>Example Request:</p> <pre><code>curl -X GET \"https://&lt;api-url&gt;/v1/get_user_secrets?limit=10&amp;offset=0\" \\\n  -H \"Authorization: Bearer &lt;YOUR-NEAR-AUTH-TOKEN&gt;\"\n</code></pre> Example response <pre><code>[\n  {\n    \"id\": 123,\n    \"owner_namespace\": \"your_account.near\",\n    \"namespace\": \"example_agent\",\n    \"name\": \"my_secret_name\",\n    \"version\": \"1.0\",\n    \"key\": \"GITHUB_API_TOKEN\",\n    \"category\": \"agent\",\n    \"created_at\": \"2025-02-19T12:34:56.789Z\",\n    \"updated_at\": \"2025-02-19T12:34:56.789Z\"\n  }\n]\n</code></pre>"},{"location":"agents/env/variables/#post-v1create_hub_secret","title":"<code>POST /v1/create_hub_secret</code>","text":"<p>Creates a new secret for the authenticated user.</p> <p>Secrets are tied to:</p> <ul> <li>A specific version</li> <li>A category, such as \"agent\" or \"user\" (default is \"agent\")</li> </ul> <p>Example Request Body:</p> <pre><code>{\n  \"namespace\": \"string\",      // Required\n  \"name\": \"string\",           // Required\n  \"version\": \"string\",        // Optional\n  \"description\": \"string\",    // Optional\n  \"key\": \"string\",            // Required\n  \"value\": \"string\",          // Required\n  \"category\": \"string\"        // Optional (default: \"agent\")\n}\n</code></pre> <p>Example Request:</p> <pre><code>curl -X POST \"https://&lt;api-url&gt;/v1/create_hub_secret\" \\\n  -H \"Authorization: Bearer &lt;YOUR-NEAR-AUTH-TOKEN&gt;\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"namespace\": \"example_agent\",\n    \"name\": \"my_secret_name\",\n    \"version\": \"0.0.1\",\n    \"description\": \"GitHub token for my agent\",\n    \"key\": \"GITHUB_API_TOKEN\",\n    \"value\": \"ghp_abc123\",\n    \"category\": \"agent\"\n  }'\n</code></pre> <p>Example Response:</p> <pre><code>true\n</code></pre>"},{"location":"agents/env/variables/#post-v1remove_hub_secret","title":"<code>POST /v1/remove_hub_secret</code>","text":"<p>Deletes an existing secret.</p> <p>Example Request Body:</p> <pre><code>{\n  \"namespace\": \"string\",  // Required\n  \"name\": \"string\",       // Required\n  \"version\": \"string\",    // Optional\n  \"key\": \"string\",        // Required\n  \"category\": \"string\"    // Optional (default: \"agent\")\n}\n</code></pre> <p>Example Request:</p> <pre><code>curl -X POST \"https://&lt;api-url&gt;/v1/remove_hub_secret\" \\\n  -H \"Authorization: Bearer &lt;YOUR-NEAR-AUTH-TOKEN&gt;\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"namespace\": \"example_agent\",\n    \"name\": \"my_secret_name\",\n    \"version\": \"1.0\",\n    \"key\": \"GITHUB_API_TOKEN\",\n    \"category\": \"agent\"\n  }'\n</code></pre> <p>Example Response:</p> <pre><code>true\n</code></pre>"},{"location":"agents/env/variables/#security-authentication","title":"Security &amp; Authentication","text":"<p>The management of secret variables on the NEAR AI platform requires authentication via a NEAR account to ensure that only authorized users can access sensitive information.</p> <p>Both the NEAR AI CLI &amp; Developer Hub require logging in with a NEAR account and then abstract away the auth token creation process by signing a message with your NEAR wallet. At a low level this authentication is handled by creating an <code>auth_token</code> from a signed message which is then passed to all Secret API requests.</p> <p><code>Example</code>:</p> <pre><code>// Authentication flow\nconst signature = await wallet.signMessage({\n    message: messageToSign,\n    recipient: \"ai.near\",\n    nonce: generateNonce(),  // 32-byte random nonce\n});\n\nconst authToken = {\n    account_id: wallet.accountId,\n    public_key: wallet.publicKey,\n    signature: signature,\n    message: messageToSign,\n    nonce: nonce,\n    recipient: \"ai.near\"\n};\n</code></pre>"},{"location":"agents/env/variables/#security-model","title":"Security Model","text":"<ol> <li> <p>Access Control</p> <ul> <li>Variables are scoped to specific agents/users</li> <li>Each agent only sees its own variables</li> <li>User variables override agent variables</li> </ul> </li> <li> <p>Request Security</p> <ul> <li>All requests require NEAR wallet signature</li> <li>Nonces prevent replay attacks</li> <li>Rate limiting on failed attempts</li> <li>Request logging and monitoring</li> </ul> </li> </ol> <p>Security Best Practices</p> <ul> <li>Never commit secrets to source code</li> <li>Use descriptive key names</li> <li>Validate required variables exist</li> <li>Handle API errors gracefully</li> <li>Cache secret existence checks</li> <li>For use cases that need higher security secret handling, run your agent in a TEE and verify the TEE quote attestation (docs coming soon).</li> </ul>"},{"location":"agents/examples/overview/","title":"NEAR AI Agent Examples","text":"<p>Below you will find a list of easy-to-use agent templates as well as more comprehensive agent examples you can review, modify, and deploy.</p>"},{"location":"agents/examples/overview/#agent-templates","title":"Agent Templates","text":"<p>Ready-to-use Agent Templates to quickly get started.</p> Template Description Link NEAR Blockchain Agent that can view and perform transactions on the NEAR Blockchain. View Twitter / X Agent that listens for and responds to twitter mentions. View AgentKit For use with Langchain, LangGraph, &amp; Coinbase's Agentkit View TypeScript Create an AI agent using TypeScript. View <p>Info</p> <p></p> <p>You can deploy these templates right from the NEAR AI App. </p> <p>Just click the plus sign in the upper right hand corner. </p>"},{"location":"agents/examples/overview/#agent-examples","title":"Agent Examples","text":"<p>Community created agent examples to help you understand how to build on NEAR AI and showcase what's possible.</p>"},{"location":"agents/examples/overview/#general-purpose-agents","title":"General Purpose Agents","text":"Example Description Link Category AI Product Manager Agent that helps with product management tasks View Productivity Image Processing Agent for handling and processing images View Images Agent Builder Agent that can build other agents View Coding Shopping Agent Agent for shopping and product recommendations View E-commerce"},{"location":"agents/examples/overview/#blockchain-enabled-agents","title":"Blockchain Enabled Agents","text":"Example Description Link Linkdrop Agent Agent for creating and managing linkdrops View AI AMM AI-powered Automated Market Maker Agent View AMM Agent Automated Market Maker Agent View Memecoin Creation Agent for creating and managing memecoins View <ul> <li>For more examples, see NEAR AI Official Agents.</li> </ul> <p>Getting Started</p> <p>New to NEAR AI? Check out our quickstart guide to get up and running with your first agent.</p>"},{"location":"agents/patterns/agent_to_agent/","title":"Agent to Agent Communication","text":"<p>Tip</p> <p>You may not need to call other agents. See Orchestrating multiple agents for more information.</p> <p>Agents can call other agents to interact with them using the <code>run_agent</code> method. To call an agent, provide the agent's id. Optionally, a query can be passed to the agent.</p> <p>Example:</p> <pre><code>result = env.run_agent(\"travel.near/travel-agent/latest\", query=\"Plan a two-day trip to Buenos Aires\")\nprint(result)\n\n# result\n$ thread_312f2ea5e42742c785218106\n</code></pre> <p>The result of <code>run_agent</code> is a string containing the <code>thread_id</code> where the external agent executed.</p> <p>Shared Environment</p> <p>With the <code>SAME</code> and <code>FORK</code> thread modes below the agent being called will receive the thread,  meaning it can access all the messages and files from the current conversation.  Moreover, the called agent will be able to add messages and files to the current thread.</p>"},{"location":"agents/patterns/agent_to_agent/#thread-mode","title":"Thread Mode","text":"<p>The <code>run_agent</code> method has an optional <code>thread_mode</code> parameter to control whether the called agent should  write to: the same thread (ThreadMode.SAME), a forked thread (ThreadMode.FORK), or a child thread (ThreadMode.CHILD). By default, <code>thread_mode</code> is set to <code>ThreadMode.FORK</code>. <code>ThreadMode.SAME</code> causes the called agent to write to the current thread. <code>ThreadMode.FORK</code> creates a new thread for the called agent and copies all messages and files from the current thread. <code>ThreadMode.CHILD</code> creates a new thread for the called agent and copies only the value of the <code>query</code> parameter to the new thread.</p>"},{"location":"agents/patterns/agent_to_agent/#run-mode-experimental","title":"Run Mode (Experimental)","text":"<p>The <code>run_agent</code> method has an optional <code>run_mode</code> parameter to control whether the calling agent should be called again after the called agent finishes. By default, <code>run_mode</code> is set to <code>RunMode.SIMPLE</code>. <code>RunMode.SIMPLE</code> calls the agent once. <code>RunMode.WITH_CALLBACK</code> calls the calling agent again after the called agent finishes. This is designed to be used with  <code>ThreadMode.CHILD</code>.</p> <p>To use RunMode.WITH_CALLBACK, the calling agent MUST check whether it was called on the parent thread or the child thread. This can be done by fetching the current thread with <code>env.get_thread()</code> and checking whether it has a <code>parent_thread_id</code>.</p> <p></p> <p>When a thread is forked, the agent we are calling will work on a copy of the thread, meaning that they have access to all files and messages created so far, but any message or file they create will be part of their own thread.</p> <p>With ThreadMode.SAME, the called agent will work in the same thread as the current agent, meaning that they have access to all files and messages created so far, and any message or file they create will be part of the current thread.</p>"},{"location":"agents/patterns/agent_to_agent/#schedule-a-run","title":"Schedule a run","text":"<p>Agents can schedule a run for a later time using the <code>schedule_run</code> method. The scheduled run will be executed  by the scheduler using hub credentials.</p>"},{"location":"agents/patterns/near_events/","title":"Running Agents Based on Events from the NEAR Blockchain","text":""},{"location":"agents/patterns/near_events/#overview","title":"Overview","text":"<p>The NEAR AI HUB monitors the latest blocks from the NEAR blockchain and can trigger agents when it detects <code>EVENT_JSON</code> entries following the <code>nearai</code> standard in transactions.</p>"},{"location":"agents/patterns/near_events/#example-of-an-event-log-entry","title":"Example of an Event Log Entry","text":"<pre><code>{\n  \"standard\": \"nearai\",\n  \"version\": \"0.1.0\",\n  \"event\": \"run_agent\",\n  \"data\": [\n    {\n      \"message\": \"Hello from NEAR Blockchain\",\n      \"agent\": \"user.near/agent-name/latest\",\n      \"env_vars\": null,\n      \"signer_id\": \"account.near\",\n      \"referral_id\": null,\n      \"request_id\": null,\n      \"amount\": \"0\"\n    }\n  ]\n}\n</code></pre> <p>Example Transaction.</p> <p>When such an event is detected, the agent specified in the <code>agent</code> field (e.g., <code>user.near/agent-name/latest</code>) will be automatically triggered. The agent will receive a JSON string containing the following object as its input:</p> <pre><code>{\n  \"event\": \"run_agent\",\n  \"message\": \"...\",\n  \"receipt_id\": \"...\",\n  // Other fields from the `data` object in the logs.\n}\n</code></pre> <p>To allow your agent to be invoked in this way, add a function that parses the incoming user message as a JSON string. If the required values for <code>event</code> and <code>message</code> are present, it should take the appropriate actions. The agent is not required to trust the data sent by the NEAR AI HUB and can independently verify the blockchain by reading the necessary block based on the <code>receipt_id</code>.</p>"},{"location":"agents/patterns/orchestration/","title":"Orchestrating multiple agent","text":"<p>NEAR AI provides flexible architecture for orchestrating multiple agents to work together effectively.</p>"},{"location":"agents/patterns/orchestration/#one-agent-one-trust-boundary","title":"One Agent - One Trust Boundary","text":"<p>Usually, a swarm of multiple agent roles can be all orchestrated within a single deployed NEAR AI agent. If your organization is the author of all the agents, combining these roles into a single agent is the most straightforward and often recommended approach which keeps things simple, efficient, and within one trust boundary.</p> <p>Examples:</p> <ul> <li>common-tool-library-agent - contains      over a hundred prompts for tackling specific problems</li> <li>langchain-reflection-agent -      contains separate code generation and reviews sub-agents that hand off work to each other</li> </ul> <p>To track turns or which sub-agent to invoke there are two common patterns:</p> <ul> <li>Router - the initial agent logic reviews the thread messages and decides which sub-agent to call.</li> <li>State file - a file is written to the thread that contains the current programmatic state of the conversation.      The agent reads this file to determine what to do next. See messages_files.md.</li> </ul>"},{"location":"agents/patterns/orchestration/#agent-to-agent-multiple-trust-boundaries","title":"Agent to Agent - Multiple trust boundaries","text":"<p>Agents can call other agents to interact with them using the <code>run_agent</code> method. This can be on the same thread or a sub-thread. This introduces multiple trust boundaries which should be considered before implementation.</p> <p>For more information on this, see Agent to Agent Communication.</p>"},{"location":"agents/patterns/orchestration/#api-integration","title":"API integration","text":"<p>External applications can call one or more NEAR AI Agents using the NEAR AI Assistants API. For more on this, see Integrating Agents.</p>"},{"location":"agents/patterns/twitter/twitter_agent/","title":"Mention Twitter(X) Agent","text":"<p>NEAR AI allows anyone to easily create an <code>X</code> (Previously <code>Twitter</code>) agent that controls and account and answers to mentions.</p> <p></p> <p>In this tutorial, you'll learn how the Twitter jokester agent works, and how you can change it to create your own X AI agent in less than five minutes.</p>"},{"location":"agents/patterns/twitter/twitter_agent/#the-x-agent","title":"The X Agent","text":"<p>Let's start by exploring the code of the agent. The agent is a simple bot that replies to mentions with a joke, using the <code>tweepy</code> library to interact with the X API.</p> <p>Try it out by tweeting a mention to the agent, and see how it replies with a joke or you can try it out by clicking here</p> <pre><code>Hey @maguila_bot, tell me a joke!\n</code></pre>"},{"location":"agents/patterns/twitter/twitter_agent/#invoking-the-agent-mentions","title":"Invoking the Agent: Mentions","text":"<p>The agent works by listening to mentions of the account <code>@maguila_bot</code> and replying to them. The account that the agent listens to is configured in the <code>metadata.json</code> file, specifically in the section <code>x_mentions</code>.</p> metadata.json<pre><code>{\n  \"category\": \"agent\",\n  \"description\": \"\",\n  \"tags\": [],\n  \"details\": {\n    \"agent\": {\n      \"welcome\": {\n        \"description\": \"To use tweet a message and mention @maguila_bot.\",\n        \"title\": \"Jokester\"\n      },\n      \"framework\": \"standard\"\n    },\n    \"triggers\": {\n      \"events\" : {\n        \"x_mentions\": [\"@maguila_bot\"]\n      }\n    }\n  },\n  \"show_entry\": true,\n  \"name\": \"jokester\",\n  \"version\": \"0.0.1\"\n}\n</code></pre> <p>Info</p> <p>Technically speaking, the agent is not listening to mentions, but is executed when a specific event happens. In this case, when the account <code>@maguila_bot</code> is mentioned in <code>X</code> (denoted by the <code>event</code> <code>x_mentions</code>)</p> <p>Events are automatically handled by the NEAR AI platform, so you don't need to worry about them.</p>"},{"location":"agents/patterns/twitter/twitter_agent/#processing-the-tweet","title":"Processing the Tweet","text":"<p>The agent receives the <code>tweet</code> that mentions it as an input object, which contains the following data:</p> <pre><code> {\n   \"author_id\": \"...\",\n   \"tweet_id\": \"...\",\n   \"text\": \"...\",\n   \"author\": {\n      \"username\": \"...\",\n      \"name\": \"...\",\n      \"profile_image_url\": \"...\"\n    }\n }\n</code></pre> <p>Since the <code>tweet</code> object contains all the needed information, we can process it through a model to generate a response.</p> agent.py<pre><code># Get the last message, which is the tweet\nlast_message = env.list_messages()[-1]\n\n# Check if the last message is None\nif last_message is None:\n    env.add_reply(\"No message found\")\n    print(\"No message found\")\n    return\n\n# Check if the last message is empty\nif not last_message[\"content\"]:\n    env.add_reply(\"Message content was empty\")\n    print(\"Message content was empty\")\n    return\n\n# Get the content of the message\nevent = json.loads(last_message[\"content\"])\n\n# Get the text of the tweet\ninput_user_text = event[\"text\"]\n# Get the id author of the tweet\ninput_tweet_id = event[\"tweet_id\"]\n\n# Generate a prompt\nprompt = [\n    {\"role\": \"system\", \"content\": PROMPT},\n    {\"role\": \"user\", \"content\": input_user_text},\n]\n\n# Generate a response\njoke = env.completion(messages=prompt, model=MODEL)\n\n# Limit the length of the joke to 280 characters\nif len(joke) &gt; 280:\n    joke = joke[:277] + \"...\"\n</code></pre>"},{"location":"agents/patterns/twitter/twitter_agent/#answering","title":"Answering","text":"<p>The agent uses the <code>tweepy</code> library to send a reply to the tweet.</p> <p>First, it creates a <code>tweepy</code> client, which is done using the <code>__init__</code> method of the agent. The client is created using the API keys stored in the environment variables.</p> agent.py<pre><code>self.x_client = tweepy.Client(\n          consumer_key=self.x_consumer_key,\n          consumer_secret=self.x_consumer_secret,\n          access_token=self.x_access_token,\n          access_token_secret=self.x_access_token_secret\n      )\n</code></pre> <p>Then, it uses the <code>send_tweet</code> method - which takes the <code>env</code>, <code>tweet</code>, and <code>tweet_id</code> - to send a reply to the tweet.</p> agent.py<pre><code>def send_tweet(self, env, tweet, tweet_id):\n    env.add_reply(f\"Sending tweet: {tweet}\")\n    response = self.x_client.create_tweet(text=tweet, in_reply_to_tweet_id=tweet_id)\n    print(f\"Tweet published! ID: {response.data['id']}\")\n\n    # Send the tweet\n    self.send_tweet(env, joke, input_tweet_id)\n</code></pre>"},{"location":"agents/patterns/twitter/twitter_agent/#modifying-the-agent","title":"Modifying the Agent","text":"<p>If you want to create your own agent, you will need to start by forking Jokester, and setting up the right Twitter API keys, so the agent can control the account you want.</p>"},{"location":"agents/patterns/twitter/twitter_agent/#forking-an-agent","title":"Forking an Agent","text":"<p>To fork the Twitter agent, open this link in your browser and click the <code>Fork</code> button. This will create a copy of the agent in your NEAR AI registry.</p> <p></p> <p>You can see the next windows where you can change the name and version to create the agent.</p> <p></p>"},{"location":"agents/patterns/twitter/twitter_agent/#generate-your-api-keys","title":"Generate your API keys","text":"<p>To allow your agent to post to X you will need to get API keys from X and setup them in the agent. To generate your API keys, follow these steps:  </p> <ol> <li>Go to the Twitter Developer Portal</li> <li>Click on the \"Projects &amp; Apps\" tab.</li> <li>Click on the \"Create App\" button.</li> <li>Fill in the required fields, such as the app name and description.</li> <li>Click on the \"Create\" button.</li> <li>Once the app is created, go to the \"Keys and tokens\" tab.</li> <li>You must create an API Key and Secret, and an Access Token and Secret.</li> </ol> <p>Free X Developer Account</p> <p>Please know that free X developer accounts are limited on the amount of tweets they can produce.</p> <p>Permissions</p> <p>Remember to set the permissions for the Access Token and Secret to \"Read and Write\".</p> <p>To change the permissions, go to the Settings tab, scroll down to the User authentication settings section, and select \"Read and Write\" under App permissions.</p>"},{"location":"agents/patterns/twitter/twitter_agent/#setting-up-the-keys","title":"Setting up the Keys","text":"<p>There are two ways to set your keys to environment variables: through the web interface or through the CLI.</p>"},{"location":"agents/patterns/twitter/twitter_agent/#web-interface","title":"Web interface","text":"<ol> <li>Go to the agent.</li> <li>Click on the <code>Run</code> tab.</li> <li>On the left side, click on the <code>Add</code> Button next to <code>Environment Variables</code>.</li> <li>Add the following variables:<ul> <li><code>X_ACCESS_TOKEN</code></li> <li><code>X_ACCESS_TOKEN_SECRET</code></li> <li><code>X_CONSUMER_KEY</code></li> <li><code>X_CONSUMER_SECRET</code></li> </ul> </li> </ol> <p>Here is how it would look like:</p> <p></p>"},{"location":"agents/patterns/twitter/twitter_agent/#cli","title":"CLI","text":"<p>You can also set your keys to environment variables using the command-line interface (CLI). For example, if you want to set a variable, you can run the following command in your terminal (remember to replace the values with your own keys):</p> <pre><code>curl -X POST \"https://api.near.ai/v1/create_hub_secret\" \\\n  -H \"Authorization: Bearer {\"account_id\":\"maguila.near\",\"public_key\":\"ed25519:aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa\",\"signature\":\"aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa\",\"callback_url\":\"https://app.near.ai/sign-in/callback\",\"message\":\"Welcome to NEAR AI Hub!\",\"recipient\":\"ai.near\",\"nonce\":\"00000000000000000000000000000000\"}\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"namespace\": \"maguila.near\",\n    \"name\": \"jokester\",\n    \"version\": \"0.0.1\",\n    \"description\": \"access token for the X API\",\n    \"key\": \"X_ACCESS_TOKEN\",\n    \"value\": \"AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\",\n    \"category\": \"agent\"\n  }'\n</code></pre> <pre><code>curl -X POST \"https://api.near.ai/v1/create_hub_secret\" \\\n  -H \"Authorization: Bearer {\"account_id\":\"maguila.near\",\"public_key\":\"ed25519:aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa\",\"signature\":\"aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa\",\"callback_url\":\"https://app.near.ai/sign-in/callback\",\"message\":\"Welcome to NEAR AI Hub!\",\"recipient\":\"ai.near\",\"nonce\":\"00000000000000000000000000000000\"}\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"namespace\": \"maguila.near\",\n    \"name\": \"jokester\",\n    \"version\": \"0.0.1\",\n    \"description\": \"access token secret for the X API\",\n    \"key\": \"X_ACCESS_TOKEN_SECRET\",\n    \"value\": \"AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\",\n    \"category\": \"agent\"\n  }'\n</code></pre> <pre><code>curl -X POST \"https://api.near.ai/v1/create_hub_secret\" \\\n  -H \"Authorization: Bearer {\"account_id\":\"maguila.near\",\"public_key\":\"ed25519:aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa\",\"signature\":\"aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa\",\"callback_url\":\"https://app.near.ai/sign-in/callback\",\"message\":\"Welcome to NEAR AI Hub!\",\"recipient\":\"ai.near\",\"nonce\":\"00000000000000000000000000000000\"}\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"namespace\": \"maguila.near\",\n    \"name\": \"jokester\",\n    \"version\": \"0.0.1\",\n    \"description\": \"consumer key for the X API\",\n    \"key\": \"X_CONSUMER_KEY\",\n    \"value\": \"AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\",\n    \"category\": \"agent\"\n  }'\n</code></pre> <pre><code>curl -X POST \"https://api.near.ai/v1/create_hub_secret\" \\\n  -H \"Authorization: Bearer {\"account_id\":\"maguila.near\",\"public_key\":\"ed25519:aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa\",\"signature\":\"aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa\",\"callback_url\":\"https://app.near.ai/sign-in/callback\",\"message\":\"Welcome to NEAR AI Hub!\",\"recipient\":\"ai.near\",\"nonce\":\"00000000000000000000000000000000\"}\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"namespace\": \"maguila.near\",\n    \"name\": \"jokester\",\n    \"version\": \"0.0.1\",\n    \"description\": \"consumer secret for the X API\",\n    \"key\": \"X_CONSUMER_SECRET\",\n    \"value\": \"AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\",\n    \"category\": \"agent\"\n  }'\n</code></pre> <p>Bearer Token</p> <p>You need to generate a bearer token to use the NEAR AI API. you can read more about it here.</p>"},{"location":"agents/patterns/twitter/twitter_agent/#modifying-the-agent_1","title":"Modifying the Agent","text":"<p>Let's change the agent into a charming 19<sup>th</sup>-century scholar who answers history questions with poetic grace and Victorian elegance. You can also adjust the prompt and model accordingly, for example, like this:</p> agent.py<pre><code>PROMPT = \"\"\"\n\"You are a charming and well-spoken individual from the 18th or 19th century, tasked with replying to modern tweets in a poetic, archaic, or Victorian manner. Your tone should be sophisticated, witty, and slightly formal, using old-fashioned phrases, Shakespearean flourishes, or genteel expressions. You may sprinkle in light humor, proverbs, or dramatic phrasing\u2014but always keep it engaging and understandable for a modern audience. Avoid modern slang, and instead respond as if you\u2019ve just stepped out of a Jane Austen novel or a Dickensian parlor.\"\n\nExample Responses:\n\nTweet: \"Ugh, Mondays are the worst.\"\nYour Reply: \"Verily, dear sufferer of this most grievous day, Monday doth test the patience of even the noblest souls. Take heart, for the week shall soon improve\u2014or else we must blame the stars!\"\n\nTweet: \"Just got ghosted. Again.\"\nYour Reply: \"Alas! To be cast aside without so much as a farewell is a cruelty most ungentlemanly (or unladylike). Pray, dry thine eyes\u2014a worthier suitor shall surely appear anon.\"\n\nTweet: \"Why is life so hard?\"\nYour Reply: \"My dear troubled soul, life hath ever been a labyrinth of trials and triumphs. Yet remember: even the mightiest oak was once a nut that held its ground. Persevere!\"\n\"\"\"\n\nMODEL = \"llama-v3p1-70b-instruct\"\n</code></pre> <p>You can also change the name of the agent in the <code>metadata.json</code> file, and update version so it can be something like this: metadata.json<pre><code>{\n  \"category\": \"agent\",\n  \"description\": \"\",\n  \"tags\": [],\n  \"details\": {\n    \"agent\": {\n      \"welcome\": {\n        \"description\": \"To use tweet a message and mention @maguila_bot.\",\n        \"title\": \"Scholar\"\n      },\n      \"framework\": \"standard\"\n    },\n    \"triggers\": {\n      \"events\" : {\n        \"x_mentions\": [\"@maguila_bot\"]\n      }\n    }\n  },\n  \"show_entry\": true,\n  \"name\": \"scholar\",\n  \"version\": \"0.0.2\"\n}\n</code></pre></p> <p>Version</p> <p>Remember to change the version of the agent, so it can be something like <code>0.0.2</code> or <code>1.0.0</code>, but not <code>0.0.1</code> again. This is important because if you don't change the version, the agent will not be updated in the NEAR AI registry.</p> <p>Now you must publish the agent, You must run the following command in your terminal: <pre><code>nearai registry upload .\n</code></pre></p>"},{"location":"agents/patterns/twitter/twitter_agent/#other-examples","title":"Other Examples","text":"<p>Check out more examples, including an advanced one that answers questions like a teacher:</p> <ul> <li>Teacher</li> </ul>"},{"location":"agents/patterns/twitter/twitter_agent/#conclusion","title":"Conclusion","text":"<p>Congratulations! You have created your own X agent. You can now tweet to the agent and see how it replies with a joke or a charming response. You can also modify the agent to do whatever you want, and use it as a template for your own agents.</p>"},{"location":"agents/templates/coinbase/","title":"Coinbase AgentKit Example","text":"<p>The Coinbase Developer Platform (CDP) AgentKit is a powerful tool that allows developers to create AI agents that can interact with multiple blockchains. This allows agents to perform actions on the blockchain, such as transferring tokens or interacting with smart contracts.</p> <p>In this page, we will showcase a NEAR AI agent that uses the CDP AgentKit to perform actions such as:</p> <ul> <li>\"Transfer a portion of your ETH to a random address\"</li> <li>\"What is the price of BTC?\"</li> <li>\"Deploy an NFT that will go super viral!\"</li> <li>\"Deploy an ERC-20 token with total supply 1 billion\"</li> </ul>"},{"location":"agents/templates/coinbase/#prerequisites","title":"Prerequisites","text":"<p>Before creating a NEAR AI agent, please make sure you have the met the following requisites:</p> <ul> <li>You have the NEAR AI CLI installed and have logged in with your Near wallet.</li> <li>Obtain a CDP API Key</li> <li>Install extra dependencies: <code>pip install coinbase_agentkit coinbase_agentkit_langchain dotenv langgraph nearai_langchain</code></li> </ul>"},{"location":"agents/templates/coinbase/#using-the-agent","title":"Using the Agent","text":"<p>Lets start by downloading the agent from the NEAR AI registry and running it locally.</p> <pre><code>nearai registry download another_one_test_account.near/first-try-agent/latest\n</code></pre> <p>Navigate to the <code>registry folder</code> (by default, <code>~/.nearai/registry/another_one_test_account.near/first-try-agent/latest</code>).</p> <p>For the agent to work, you need to set up a few environment variables:</p> <ul> <li>\"CDP_API_KEY_NAME\"</li> <li>\"CDP_API_KEY_PRIVATE_KEY\"</li> <li>\"NETWORK_ID\" (Defaults to <code>base-sepolia</code>)</li> </ul> <p>You can obtain Coinbase Developer Platform (CDP) Api Key in your CDP settings here.</p> <p>Now you can run the agent locally and pass all environment variables using NEAR AI CLI:</p> <p><code>nearai agent interactive ~/.nearai/registry/another_one_test_account.near/first-try-agent/latest --local --env_vars='{\"CDP_API_KEY_NAME\":\"YOUR_API_KEY\", \"CDP_API_KEY_PRIVATE_KEY\":\"YOUR_SECRET_KEY\", \"NETWORK_ID\":\"base-sepolia\"}'</code></p> <p>You can create <code>.env</code> file locally and put your variables there, but DON'T FORGET TO DELETE IT before uploading your code on Near AI Hub. There is no way to ignore it without deleting yet.</p> <p>Tip</p> <p>Refer to the Secrets &amp; Environment Variables to learn how to work with your variables locally and on Near AI Hub.</p>"},{"location":"agents/templates/coinbase/#how-it-works","title":"How it works","text":"<p>Lets take a look at the imports at the beginning of the agent code:</p> <pre><code>import json\nimport os\n\nfrom coinbase_agentkit import (  # type: ignore\n    AgentKit,\n    AgentKitConfig,\n    CdpWalletProvider,\n    CdpWalletProviderConfig,\n    cdp_api_action_provider,\n    cdp_wallet_action_provider,\n    erc20_action_provider,\n    pyth_action_provider,\n    wallet_action_provider,\n    weth_action_provider,\n)\nfrom coinbase_agentkit_langchain import get_langchain_tools  # type: ignore\nfrom dotenv import load_dotenv\nfrom langgraph.prebuilt import create_react_agent\nfrom nearai_langchain.orchestrator import NearAILangchainOrchestrator, RunMode\n</code></pre> <p>General packages:</p> <ul> <li><code>json</code> is a built-in package, which can be used to work with JSON data.</li> <li><code>os</code> provides a portable way of using operating system dependent functionality.</li> <li><code>dotenv</code> allows using environment variables in the project.</li> </ul> <p>AI related packages:</p> <ul> <li><code>coinbase_agentkit</code> is a framework for easily enabling AI agents to take actions onchain. It is designed to be framework-agnostic, so you can use it with any AI framework, and wallet-agnostic, so you can use it with any wallet.</li> <li><code>coinbase_agentkit_langchain</code> is the LangChain extension of AgentKit. Enables agentic workflows to interact with onchain actions.</li> <li><code>nearai_langchain</code> provides seamless integration between NearAI and LangChain, allowing developers to use NearAI's capabilities within their LangChain applications.</li> <li><code>langgraph</code> is a low-level orchestration framework for building controllable agents. While langchain provides integrations and composable components to streamline LLM application development, the LangGraph library enables agent orchestration \u2014 offering customizable architectures, long-term memory, and human-in-the-loop to reliably handle complex tasks.</li> </ul>"},{"location":"agents/templates/coinbase/#wallet-setup","title":"Wallet Setup","text":"<p>The agent will control a wallet to perform onchain actions. A local file <code>wallet_data.txt</code> is used to persist the MPC wallet data for reuse between sessions.</p> <ul> <li>If the file exists, it loads the wallet data for reinitialization</li> <li>If the file doesn't exists it's initiated using <code>CdpWalletProvider</code> from <code>coinbase_agentkit</code> module.</li> </ul> <pre><code># Initialize CDP Wallet Provider\nwallet_data = None\nif os.path.exists(wallet_data_file):\n    with open(wallet_data_file) as f:\n        wallet_data = f.read()\n\ncdp_config = None\nif wallet_data is not None:\n    cdp_config = CdpWalletProviderConfig(wallet_data=wallet_data)\n\nwallet_provider = CdpWalletProvider(cdp_config)\n</code></pre>"},{"location":"agents/templates/coinbase/#agentkit-setup","title":"AgentKit Setup","text":"<p>To initiate AgentKit, in addition to passing <code>wallet_provider</code>, it is necessary to pass the action providers it is going to work with. Action providers define the actions that an agent can take.</p> <p>In this example, we're using the following ones:</p> <ul> <li><code>cdp_api_action_provider</code> retrieves the current price of a cryptocurrency from the CDP API and fetches on-chain data from Base blockchain explorer via CDP API.</li> <li><code>cdp_wallet_action_provider</code> deploys contracts (custom, NFT, FT) and executes trades.</li> <li><code>erc20_action_provider</code> retrieves the token balance for a specified address and ERC-20 contract and transfers a specified amount of ERC-20 tokens to a destination address.</li> <li><code>pyth_action_provider</code> retrieves current price data from a specified Pyth price feed and the unique price feed identifier for a given token symbol.</li> <li><code>wallet_action_provider</code> retrieves wallet address, network info, balances, gets the native currency balance of the connected wallet, transfers native blockchain tokens (e.g., ETH) to a destination address.</li> <li><code>weth_action_provider</code> converts native ETH to Wrapped ETH (WETH) on supported networks.</li> </ul> <p>The list of all available action providers and their actions you can find here. You can also create a custom action provider.</p>"},{"location":"agents/templates/coinbase/#llm-setup","title":"LLM Setup","text":"<p>LLM is initiatized by NearAILangchainOrchestrator (<code>nearai_langchain</code> module). This example leverages a ChatOpenAI model.</p> <pre><code>orchestrator = NearAILangchainOrchestrator(globals())\n\n...\n\ndef initialize_agent():\n    \"\"\"Initialize the agent with CDP AgentKit.\"\"\"\n    # Get ChatOpenAI model.\n    llm = orchestrator.chat_model.chat_open_ai_model\n\n...\n</code></pre>"},{"location":"agents/templates/coinbase/#langchain-setup","title":"Langchain Setup","text":"<p>In order to use AgentKit blockchain tools we have to wrap these providers into usable tools for the agent using <code>get_langchain_tools</code> method from <code>coinbase_agentkit_langchain</code> module.</p> <pre><code>tools = get_langchain_tools(agentkit)\n</code></pre>"},{"location":"agents/templates/coinbase/#agent-setup","title":"Agent Setup","text":"<p>Eventually we create and return ReAct Agent:</p> <pre><code># Create ReAct Agent using the LLM and CDP AgentKit wrappet tools.\nreturn create_react_agent(\n    llm,\n    tools=tools,\n    state_modifier=(\n        \"You are a helpful agent that can interact onchain using the Coinbase Developer Platform AgentKit. \"\n        \"You are empowered to interact onchain using your tools. If you ever need funds, you can request \"\n        \"them from the faucet if you are on network ID 'base-sepolia'. If not, you can provide your wallet \"\n        \"details and request funds from the user. Before executing your first action, get the wallet details \"\n        \"to see what network you're on. If there is a 5XX (internal) HTTP error code, ask the user to try \"\n        \"again later. If someone asks you to do something you can't do with your currently available tools, \"\n        \"you must say so, and encourage them to implement it themselves using the CDP SDK + AgentKit, \"\n        \"recommend they go to docs.cdp.coinbase.com for more information. Be concise and helpful with your \"\n        \"responses. Refrain from restating your tools' descriptions unless it is explicitly requested.\"\n    ),\n)\n</code></pre>"},{"location":"agents/templates/coinbase/#nearai-environment-setup","title":"NearAI Environment Setup","text":"<ul> <li>In remote mode thread is assigned, user messages are given, and an agent is run at least once per user message.</li> <li>In local mode an agent is responsible to get and upload user messages.</li> </ul> <pre><code>env = orchestrator.env\n\nif orchestrator.run_mode == RunMode.LOCAL:\n    print(\"Entering chat mode...\")\n    user_input = input(\"\\nPrompt: \")\n    env.add_user_message(user_input)\n\nmessages = env.list_messages()\nfor chunk in executor.stream({\"messages\": messages}):\n    if \"agent\" in chunk:\n        result = chunk[\"agent\"][\"messages\"][0].content\n    elif \"tools\" in chunk:\n        result = chunk[\"tools\"][\"messages\"][0].content\n    env.add_reply(result)\n\n    if orchestrator.run_mode == RunMode.LOCAL:\n        print(result)\n        print(\"-------------------\")\n</code></pre>"},{"location":"api/guide/","title":"API Basics","text":"<p>NEAR AI provides a standard Web API based on OpenAPI to interact with the Developer Hub, allowing powerful backend integrations.</p> <p>In this article you'll find information on how to generate a valid <code>auth</code> token to use the API, and links to the API reference documentation:</p> <ul> <li>Using the API</li> <li>API documentation</li> <li>Interactive API</li> </ul>"},{"location":"api/guide/#using-the-api","title":"Using the API","text":"<p>To use the Near AI Web API methods, you'll need an authorization token generated from your NEAR account.</p> <p>In the next section, you can find detailed information about the <code>auth</code> bearer token, and how it can be generated in a programmatic way.</p>"},{"location":"api/guide/#auth-token","title":"Auth token","text":"<p>The <code>auth</code> bearer token is a stringyfied JSON that must contain <code>account_id</code>, <code>public_key</code>, <code>signature</code>, <code>nonce</code>, and <code>message</code> (it can also include some optional fields).</p> <p>For example, this object:</p> <pre><code>{\n  \"account_id\": \"demoaccount.near\",\n  \"public_key\": \"ed25519:G59i7tou.....QGUrfSkeJ\",\n  \"signature\": \"P6fZ5bp9j.....Z7BvtqDQ==\",\n  \"callback_url\": \"https://app.near.ai/sign-in/callback\",\n  \"message\": \"Welcome to NEAR AI Hub!\",\n  \"recipient\": \"ai.near\",\n  \"nonce\": \"00000000000000000001741287395592\"\n}\n</code></pre> <p>Becomes a string token:</p> <pre><code>auth:'{\"account_id\":\"demoaccount.near\",\"public_key\":\"ed25519:G59i7touKk.....QGUrfSkeJ\",\"signature\":\"P6fZ5bp9j.....BVTZ7BvtqDQ==\",\"callback_url\":\"https://app.near.ai/sign-in/callback\",\"message\":\"Welcome to NEAR AI Hub!\",\"recipient\":\"ai.near\",\"nonce\":\"00000000000000000001741287395592\"}'\n</code></pre> <p>Tip</p> <p>If you're logged into app.near.ai, you can grab the token out of the <code>auth</code> cookie. For example:</p> <pre><code>auth:\"%7B%22account_id%22%3A%22demoaccount.near%22%2C%22public_key%22%3A%22ed25519%3AG59i7tou.....QGUrfSkeJ%22%2C%22signature%22%3A%22P6fZ5bp9j.....Z7BvtqDQ%3D%3D%22%2C%22callback_url%22%3A%22https%3A%2F%2Fapp.near.ai%2Fsign-in%2Fcallback%22%2C%22message%22%3A%22Welcome%20to%20NEAR%20AI%20Hub!%22%2C%22recipient%22%3A%22ai.near%22%2C%22nonce%22%3A%2200000000000000000001741287395592%22%7D\"\n</code></pre>"},{"location":"api/guide/#generate-the-token","title":"Generate the token","text":"<p>You can also generate the <code>auth</code> bearer token using your preferred programming language. In this section you can find a few code samples using Python, Rust, and JavaScript.</p> PythonJavaScriptRust <p>This code example uses the <code>nearai</code> CLI to complete the authentication process, and then generates the <code>auth</code> token. For additional details, check the full implementation.</p> <pre><code>import shlex\nimport sys\nimport fire  # type: ignore\n\nfrom nearai.cli import LoginCLI  # type: ignore\nfrom nearai.config import load_config  # type: ignore\nfrom nearai.shared.client_config import ClientConfig  # type: ignore\n\ndef get_nearai_client_config() -&gt; ClientConfig:\n  config = load_config()\n  if self.config.auth is None:\n    print(\"Attempt to get local client config, nearai_langchain v ^0.0.11\")\n    print(\"NearAI authentication required. Running web-based login...\")\n    command = \"nearai login --remote\"\n    sys.argv = shlex.split(command)\n    fire.Fire(CLI)\n\n    # Note: At this point, the user needs to:\n    # 1. Follow the auth URL that will be printed\n    # 2. Complete the authentication process\n    # 3. Get the login save command with parameters\n\n    save_command = input(\"Please enter the login save command: \")\n    sys.argv = shlex.split(save_command)  # Properly splits command respecting quotes\n    fire.Fire(CLI)\n\n    config = load_config()\n  return config.get_client_config()\n\n\ndef generate_nearai_auth_token(config: ClientConfig):\n  auth_bearer_token = config.auth.generate_bearer_token()\n  new_token = json.loads(auth_bearer_token)\n  new_token[\"runner_data\"] = json.dumps(\n    {\"agent\": \"\", \"runner_api_key\": \"\"}\n  )\n  auth_bearer_token = json.dumps(new_token)\n  return auth_bearer_token\n\n\napi_key = generate_nearai_auth_token(get_nearai_client_config())\n</code></pre> <p>This JS code example uses the Wallet selector to complete the authentication process, and then generates the <code>auth</code> token. For additional information, you can check the complete Next.js implementation.</p> <pre><code>export const NEAR_AI_AUTH_OBJECT_STORAGE_KEY = \"NearAIAuthObject\";\n\nexport async function handleNearAILoginCallback() {\n  const callbackparams = new URLSearchParams(location.hash);\n  const accountId = callbackparams.get(\"#accountId\");\n\n  if (accountId) {\n    const nearaiauthobject = JSON.parse(\n      localStorage?.getItem(NEAR_AI_AUTH_OBJECT_STORAGE_KEY),\n    );\n    nearaiauthobject.account_id = accountId;\n    nearaiauthobject.signature = callbackparams.get(\"signature\");\n    nearaiauthobject.public_key = callbackparams.get(\"publicKey\");\n    localStorage.setItem(\n      NEAR_AI_AUTH_OBJECT_STORAGE_KEY,\n      JSON.stringify(nearaiauthobject),\n    );\n    location.hash = \"\";\n  }\n}\n\nexport async function nearAIlogin(wallet, message) {\n  // Generate nonce based on current time in milliseconds\n  const nonce = new String(Date.now());\n  const nonceBuffer = Buffer.from(\n    new TextEncoder().encode(nonce.padStart(32, \"0\")),\n  );\n\n  const recipient = \"ai.near\";\n  const callbackUrl = location.href;\n\n  const authObject = {\n    message,\n    nonce,\n    recipient,\n    callback_url: callbackUrl,\n    signature:\"\",\n    account_id: \"\",\n    public_key: \"\"\n  };\n\n  localStorage.setItem(\n    NEAR_AI_AUTH_OBJECT_STORAGE_KEY,\n    JSON.stringify(authObject),\n  );\n  const signedMessage = await wallet.signMessage({\n    message,\n    nonce: nonceBuffer,\n    recipient,\n    callbackUrl,\n  });\n\n  authObject.signature = signedMessage.signature;\n  authObject.account_id = signedMessage.accountId;\n  authObject.public_key = signedMessage.publicKey;\n\n  localStorage.setItem(\n    NEAR_AI_AUTH_OBJECT_STORAGE_KEY,\n    JSON.stringify(authObject),\n  );\n\n  return authObject;\n}\n</code></pre> <p>This code example builds the auth payload, signs it using the <code>sign_message_nep413()</code> method, and stringifies the JSON result to generate the <code>auth</code> token. For additional information, see full implementation.</p> <pre><code>async fn get_api_key(account: SelectedAccount) -&gt; Result&lt;String, anyhow::Error&gt; {\n    let nonce = SystemTime::now()\n        .duration_since(UNIX_EPOCH)\n        .unwrap()\n        .as_millis();\n    let payload = NEP413Payload {\n        message: \"Welcome to NEAR AI\".to_string(),\n        recipient: \"ai.near\".to_string(),\n        nonce: [\n            vec!['0' as u8; 32 - nonce.to_string().as_bytes().len()],\n            nonce.to_string().as_bytes().to_vec(),\n        ]\n        .concat()\n        .try_into()\n        .unwrap(),\n        callback_url: Some(\"https://example.callback.url\".to_string()),\n    };\n    let signature = account\n        .signer\n        .sign_message_nep413(\n            account.account_id.clone(),\n            account.public_key.clone(),\n            payload.clone(),\n        )\n        .await?;\n    Ok(serde_json::to_string(&amp;serde_json::json!({\n        \"account_id\": account.account_id,\n        \"signature\": BASE64_STANDARD.encode(match signature {\n            Signature::ED25519(sig) =&gt; sig.to_bytes(),\n            Signature::SECP256K1(_) =&gt; unreachable!(), // we're working only with ed25519 on trading bot side\n        }),\n        \"public_key\": account.public_key,\n        \"nonce\": nonce.to_string(),\n        \"recipient\": payload.recipient,\n        \"message\": payload.message,\n        \"callback_url\": payload.callback_url,\n        \"on_behalf_of\": null,\n    }))?)\n}\n</code></pre>"},{"location":"api/guide/#api-documentation","title":"API documentation","text":"<p>You can find complete and detailed technical reference documentation regarding available NEAR AI API methods in this page.</p> <p></p> <p>Reference documentation includes:</p> <ul> <li>Request schemas</li> <li>Query parameters</li> <li>Responses</li> <li>Response schemas</li> </ul> <p>API Endpoints reference documentation</p> Link Description Default Main Hub methods. Registry Registry management methods. Agents &amp; Assistants Agent management methods. Benchmark Benchmarking methods. Stars Star management methods. Jobs Jobs management methods. Permissions Permission methods. Evaluation Evaluation methods. Delegation Delegation management methods. Logs Logging methods. Vector Stores Vector Store management methods. Files File management methods. Threads Thread management methods. Hub Secrets Manage Hub secrets. Run Schedule Scheduling methods."},{"location":"api/guide/#interactive-api","title":"Interactive API","text":"<p>NEAR AI also provides an interactive Web API based on Swagger UI that allows you to visualize and interact with NEAR AI API\u2019s resources without having any of the implementation logic in place.</p> <p>The interactive API is automatically generated from the OpenAPI specification, providing a simple API tool for back end implementation and client side consumption.</p> <p>Info</p> <p>Please note that to execute API calls using this interactive tool, you need to provide a valid authentication token. Check this section to learn more about generating your <code>auth</code> token.</p> <p></p> <p>Interactive API Endpoints</p> <ul> <li>Default</li> <li>Registry</li> <li>Agents &amp; Assistants</li> <li>Benchmark</li> <li>Stars</li> <li>Jobs</li> <li>Permissions</li> <li>Evaluation</li> <li>Delegation</li> <li>Logs</li> <li>Vector Stores</li> <li>Files</li> <li>Threads</li> <li>Hub Secrets</li> <li>Run Schedule</li> </ul>"},{"location":"cloud/","title":"Introduction to NEAR AI Cloud","text":"<p>NEAR AI Cloud is the platform where AI runs on your terms \u2014 fully private, verifiable, and user-owned. Deploy and scale powerful language models on TEE-enabled GPUs with on-chain guarantees. Built for the age of agentic AI, NEAR AI Cloud gives your models the power to act, reason, and transact \u2014 while you retain complete control over the stack.</p>"},{"location":"cloud/#getting-started","title":"Getting Started","text":"<p>Ready to build the future of AI? Get started with NEAR AI Cloud in minutes:</p> <p> Quick Start Guide</p> <p>For developers looking to understand the technical architecture and security model:</p> <p> Private Inference Deep Dive</p> <p>To learn how to verify the authenticity of AI responses:</p> <p> Verification Guide</p> <p>Beta Release</p> <p>NEAR AI Cloud is currently in beta - we're rapidly building and shipping new features! Join our community to help shape the future of private, verifiable AI.</p>"},{"location":"cloud/get-started/","title":"Get Started with NEAR AI Cloud","text":"<p>NEAR AI Cloud offers developers access to private, verifiable AI models through a unified API. This guide will walk you through setting up your account, creating API keys, and making your first requests.</p>"},{"location":"cloud/get-started/#overview","title":"Overview","text":"<p>NEAR AI Cloud provides:</p> <ul> <li>Unified API for AI Models: Access leading AI models like DeepSeek, Llama, OpenAI, Qwen and more through a single API</li> <li>Private Inference: All AI computations run in Trusted Execution Environments (TEEs) ensuring end-to-end privacy and verifiability</li> <li>Flexible Payments: Top up or pay as you go</li> </ul>"},{"location":"cloud/get-started/#quick-setup","title":"Quick Setup","text":"<ol> <li>Visit NEAR AI Cloud, and connect your GitHub or Google account</li> <li>Navigate to the Credits section in your dashboard, and purchase the amount of credits you need</li> <li>Go to the API Keys section in your dashboard, and create a new API key.</li> </ol> <p>API Key Security</p> <p>Keep your API key secure and never share it publicly. You can regenerate keys at any time from your dashboard.</p>"},{"location":"cloud/get-started/#making-your-first-request","title":"Making Your First Request","text":"<p>Let's chat with one open source model, with privacy protected. Please replace the API key with the one you have created.</p> pythonjavascriptcurl <pre><code>import openai\n\nclient = openai.OpenAI(\n    base_url=\"https://cloud-api.near.ai/v1\",\n    api_key=\"your-api-key-here\"\n)\n\nresponse = client.chat.completions.create(\n    model=\"deepseek-chat-v3-0324\",\n    messages=[{\n        \"role\": \"user\", \"content\": \"Hello, how are you?\"\n    }]\n)\n\nprint(response.choices[0].message.content)\n</code></pre> <pre><code>import OpenAI from 'openai';\n\nconst openai = new OpenAI({\n    baseURL: 'https://cloud-api.near.ai/v1',\n    apiKey: 'your-api-key-here',\n});\n\nconst completion = await openai.chat.completions.create({\n    model: 'deepseek-chat-v3-0324',\n    messages: [{\n        role: 'user', content: 'Hello, how are you?'\n    }]\n});\n\nconsole.log(completion.choices[0].message.content);\n</code></pre> <pre><code>curl https://cloud-api.near.ai/v1/chat/completions \\\n-H \"Content-Type: application/json\" \\\n-H \"Authorization: Bearer your-api-key-here\" \\\n-d '{\n    \"model\": \"deepseek-chat-v3-0324\",\n    \"messages\": [{\n        \"role\": \"user\",\n        \"content\": \"Hello, how are you?\"\n    }]\n}'\n</code></pre>"},{"location":"cloud/get-started/#available-models","title":"Available Models","text":"<p>NEAR AI Cloud now supports a few open source, private and verifiable models. We'll add more models soon.</p> <p>You can find the model list from https://cloud.near.ai/models</p>"},{"location":"cloud/get-started/#next-steps","title":"Next Steps","text":"<p>Now that you're set up with NEAR AI Cloud, explore these resources:</p> <ul> <li> Private Inference Deep Dive - Learn about private inference</li> <li> Verification Guide - Understand how to verify private AI responses</li> </ul>"},{"location":"cloud/private-inference/","title":"Private Inference","text":"<p>NEAR AI Cloud's private inference capabilities leverage Trusted Execution Environments (TEEs) to provide cryptographically verifiable AI computations while ensuring complete data privacy. This guide explains how private inference works, its architecture, and security guarantees.</p> <p>Private inference ensures that your AI model interactions are completely private and verifiable, with cryptographic proofs that guarantee the integrity of every computation.</p>"},{"location":"cloud/private-inference/#what-is-private-inference","title":"What is Private Inference?","text":"<p>Private inference in NEAR AI Cloud ensures that your AI model interactions are:</p> <ul> <li>\ud83d\udd12 Private: Your prompts, model weights, and outputs are never visible to the infrastructure provider</li> <li>\ud83d\udee1\ufe0f Verifiable: Every computation is cryptographically signed and can be verified to have occurred in a secure environment</li> <li>\u26a1 Fast: Optimized for high-throughput inference with minimal latency overhead</li> </ul>"},{"location":"cloud/private-inference/#how-private-inference-works","title":"How Private Inference Works","text":""},{"location":"cloud/private-inference/#trusted-execution-environment-tee","title":"Trusted Execution Environment (TEE)","text":"<p>NEAR AI Cloud uses a combination of Intel TDX and NVIDIA TEE technologies to create isolated, secure environments for AI computation:</p> <p></p> <ol> <li>Intel TDX (Trust Domain Extensions): Creates confidential virtual machines (CVMs) that isolate your AI workloads from the host system</li> <li>NVIDIA TEE: Provides GPU-level isolation for model inference, ensuring model weights and computations remain private</li> <li>Cryptographic Attestation: Each TEE environment generates cryptographic proofs of its integrity and configuration</li> </ol>"},{"location":"cloud/private-inference/#the-private-inference-process","title":"The Private Inference Process","text":"<pre><code>graph TD\n    A[User Request] --&gt; B[Secure Request Routing]\n    B --&gt; C[Secure Inference]\n    C --&gt; D[Cryptographic Signature]\n    D --&gt; E[Verifiable Response]\n\n    C --&gt; F[Attestation Report]</code></pre> <ol> <li>Request Initiation: Users send chat completion requests to the LLM Gateway, which operates within a secure TEE environment</li> <li>Secure Request Routing: The LLM Gateway securely routes requests to appropriate Private LLM Nodes based on model availability and load balancing</li> <li>Secure Inference: AI inference computations are performed inside the Private LLM Nodes, with all data and model weights protected by TEE isolation</li> <li>Attestation Generation: CPU and GPU attestation reports are generated, providing cryptographic proof of the TEE environment's integrity</li> <li>Cryptographic Signing: The TEE cryptographically signs both the original request and the inference results to ensure authenticity and prevent tampering</li> <li>Verifiable Response: Users receive the AI response along with cryptographic signatures for complete verification</li> </ol>"},{"location":"cloud/private-inference/#architecture-overview","title":"Architecture Overview","text":"<p>NEAR AI Cloud operates LLM Gateway and a network of Private LLM Nodes:</p> <pre><code>    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n    \u2502   Private LLM   \u2502    \u2502   Private LLM   \u2502    \u2502   Private LLM   \u2502\n    \u2502   (8x H200)     \u2502    \u2502   (8x H200)     \u2502    \u2502   (8x H200)     \u2502\n    \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524    \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524    \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n    \u2502 Intel TDX CVM   \u2502    \u2502 Intel TDX CVM   \u2502    \u2502 Intel TDX CVM   \u2502\n    \u2502 NVIDIA TEE      \u2502    \u2502 NVIDIA TEE      \u2502    \u2502 NVIDIA TEE      \u2502\n    \u2502 Private-ML-SDK  \u2502    \u2502 Private-ML-SDK  \u2502    \u2502 Private-ML-SDK  \u2502 \n    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n            \u2502                       \u2502                       \u2502\n            \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                    \u2502\n                            \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                            \u2502  LLM Gateway    \u2502\n                            \u2502  Key Creation   \u2502\n                            \u2502  Usage Tracking \u2502\n                            \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"cloud/private-inference/#key-components","title":"Key Components","text":""},{"location":"cloud/private-inference/#1-private-llm-nodes","title":"1. Private LLM Nodes","text":"<ul> <li>Standardized hardware: 8x NVIDIA H200 GPUs per machine</li> <li>Intel TDX-enabled CPUs for secure virtualization</li> <li>Private-ML-SDK for secure model execution and attestation</li> <li>Automated liveness monitoring and health checks</li> </ul>"},{"location":"cloud/private-inference/#2-llm-gateway","title":"2. LLM Gateway","text":"<ul> <li>Model registration and provider management</li> <li>Request routing and load balancing across Private LLM Nodes</li> <li>Attestation verification and storage</li> <li>API key management and usage tracking</li> </ul>"},{"location":"cloud/private-inference/#security-guarantees","title":"Security Guarantees","text":""},{"location":"cloud/private-inference/#cryptographic-isolation","title":"Cryptographic Isolation","text":"<p>Private inference provides multiple layers of security:</p> <ol> <li>Hardware-Level Isolation: TEEs create isolated execution environments at the hardware level</li> <li>Secure Communication: End-to-end encryption between users and LLM</li> <li>Attestation Verification: Cryptographic proofs verify the integrity of the execution environment</li> <li>Result Signing: All AI outputs are cryptographically signed inside TEE</li> </ol>"},{"location":"cloud/private-inference/#threat-model","title":"Threat Model","text":"<p>NEAR AI Cloud's private inference protects against:</p> <ul> <li>Malicious Infrastructure Providers: TEEs prevent providers from accessing user data</li> <li>Network Attacks: End-to-end encryption protects data in transit</li> <li>Model Extraction: Model weights remain encrypted and inaccessible</li> <li>Result Tampering: Cryptographic signatures ensure result integrity</li> </ul>"},{"location":"cloud/private-inference/#next-steps","title":"Next Steps","text":"<p>Ready to implement private inference in your applications?</p> <ul> <li> Get Started - Set up your first private inference request</li> <li> Verification Guide - Learn about attestation and signature verification</li> </ul>"},{"location":"cloud/verification/","title":"Verification","text":"<p>NEAR AI Cloud operates in Trusted Execution Environments (TEEs) which use cryptographic proofs to verify that your private AI conversations actually happened in secure, isolated environments - not on compromised systems or with unauthorized access.</p> <p>This section will show you step-by-step processes for checking these proofs, validating digital signatures, and confirming that your AI interactions haven't been tampered with.</p>"},{"location":"cloud/verification/#overview","title":"Overview","text":""},{"location":"cloud/verification/#how-near-ai-cloud-verification-works","title":"How NEAR AI Cloud Verification Works:","text":"<ol> <li> <p>Secure Key Generation: When NEAR AI Cloud initializes, it generates a unique cryptographic signing key pair inside the Trusted Execution Environment (TEE). The private key never leaves the secure hardware.</p> </li> <li> <p>Hardware Attestation: The system generates attestation reports that cryptographically prove it's running on genuine NVIDIA H100/H200/B100 hardware in TEE mode within a Confidential VM.</p> </li> <li> <p>Key Binding: These attestation reports include the public key from step 1, creating a verifiable link between the secure hardware and the signing capability.</p> </li> <li> <p>Message Signing: Every AI inference request and response is digitally signed using the private key that remains secured within the TEE.</p> </li> <li> <p>End-to-End Verification: You can verify that your AI interactions were genuinely processed in the secure environment by:</p> <ul> <li>Checking the hardware attestation reports</li> <li>Validating the digital signatures on your messages</li> <li>Confirming the signing key matches the attested hardware</li> </ul> </li> </ol>"},{"location":"cloud/verification/#model-verification","title":"Model Verification","text":"<p>To verify a NEAR AI model is operating in a secure trusted environment, there are two main steps:</p> <ul> <li>Request Model Attestation report from NEAR AI Cloud</li> <li>Verify Model Attestation report using NVIDIA &amp; Intel attestation authenticators</li> </ul>"},{"location":"cloud/verification/#request-model-attestation","title":"Request Model Attestation","text":"<p>To request a model attestation from NEAR AI cloud, use the following <code>GET</code> API endpoint:</p> <pre><code>https://cloud-api.near.ai/v1/attestation/report?model={model_name}\n</code></pre> <p>Example Requests:</p> curlJavaScriptPython <pre><code>curl https://cloud-api.near.ai/v1/attestation/report?model=deepseek-chat-v3-0324 \\\n  -H 'accept: application/json' \\\n  -H 'Content-Type: application/json' \\\n  -H 'Authorization: Bearer &lt;YOUR_NEAR_AI_CLOUD_API_KEY&gt;'\n</code></pre> <pre><code>const MODEL_NAME = 'deepseek-chat-v3-0324'\n\nconst response = await fetch(\n  `https://cloud-api.near.ai/v1/attestation/report?model=${MODEL_NAME}`,\n  {\n    headers: {\n      Authorization: `Bearer ${YOUR_NEARAI_CLOUD_API_KEY}`,\n      'Content-Type': 'application/json',\n      },\n  }\n);\n</code></pre> <pre><code>import requests\n\nMODEL_NAME = 'deepseek-chat-v3-0324'\n\nresponse = requests.get(\n    f'https://cloud-api.near.ai/v1/attestation/report?model={MODEL_NAME}',\n    headers={\n        'Authorization': f'Bearer {YOUR_NEARAI_CLOUD_API_KEY}',\n        'Content-Type': 'application/json',\n    }\n)\n</code></pre> <p>Note</p> <p>This endpoint requires NEAR AI Cloud Account &amp; API Key</p> <p>Implementation: This endpoint is defined in the NEAR AI Private ML SDK.</p> <p>Example Response:</p> <pre><code>{\n\"signing_address\": \"...\",     \\\\ TEE Public Key\n\"nvidia_payload\": \"...\",      \\\\ Attestation report used to verify w/ NVIDIA\n\"intel_quote\": \"...\",         \\\\ Attestation report use to verify w/ Intel\n\"all_attestations\": [         \\\\ List of all GPU nodes in the network\n  {\n    \"signing_address\": \"...\",\n    \"nvidia_payload\": \"...\",\n    \"intel_quote\": \"...\"\n  }\n]\n}\n</code></pre> <ul> <li> <p><code>signing_address</code>: Account address generated inside TEE that will be used to sign the chat response.</p> </li> <li> <p><code>nvidia_payload</code> and <code>intel_quote</code>: Attestation report formatted for NVIDIA TEE and Intel TEE respectively. You can use them to verify the integrity of the TEE. See Verify the Attestation for more details.</p> </li> <li> <p><code>all_attestations</code>: List attestations from all GPU nodes as multiple TEE nodes may be used to serve inference requests. You can utilize the <code>signing_address</code> from <code>all_attestations</code> to select the appropriate TEE node for verifying its integrity.</p> </li> </ul>"},{"location":"cloud/verification/#verifying-model-attestation","title":"Verifying Model Attestation","text":"<p>Once you have requested a model attestation from NEAR AI Cloud, you can use the returned payload to verify its authenticity for both GPU &amp; CPU chips.</p>"},{"location":"cloud/verification/#verify-gpu-attestation","title":"Verify GPU Attestation","text":"<p>NVIDIA offers a Remote Attestation Service that allows you to verify that you are using a trusted environment with one of their GPUs. To verify this they require:</p> <ul> <li><code>nonce</code> - A randomly generated 64 character hexadecimal string</li> <li><code>arch</code> - Architecture of the GPU (HOPPER or BLACKWELL)</li> <li><code>evidence_list</code> - A list of GPU evidence items, each containing an evidence and a corresponding certificate</li> </ul> <p>The <code>evidence_list</code> contains Base64 encoded data that lists the GPU's:</p> <ul> <li>Hardware Identity</li> <li>Firmware &amp; Software measurements</li> <li>Security configuration state</li> <li>Endorsement certificates (Signed measurements from the GPU's unique key)</li> </ul> <p>The private key of this GPU is how we can securely verify the authenticity. NVIDIA burns this unique private key into each GPU during the manufacturing process and only retains the corresponding public key, which is used to verify the signature of attestations provided to them.</p> <p>All of this data is provided to you from the Model Attestation response as <code>nvidia_payload</code>.</p> <p>Simply use this JSON Object with your API call:</p> <pre><code>curl -X POST https://nras.attestation.nvidia.com/v3/attest/gpu \\\n -H \"accept: application/json\" \\\n -H \"content-type: application/json\" \\\n -d \"&lt;NVIDIA_PAYLOAD_FROM_NEARAI_MODEL_ATTESTATION&gt;\"\n</code></pre> <p>See official documentation: https://docs.api.nvidia.com/attestation/reference/attestmultigpu_1</p> <p>Example GPU Attestation Response:</p> <pre><code>[\n  [\n    \"JWT\",\n    \"eyJraWQiOiJudi1lYXQta2lkLXByb2QtMjAyNTA4MjQxNzI2MzczMzEtMGM4YzM2MzQtY2ZkMC00YmViLWFmNWYtMTE2MzliOWUxOTIyIiwiYWxnIjoiRVMzODQifQ.eyJzdWIiOiJOVklESUEtUExBVEZPUk0tQVRURVNUQVRJT04iLCJ4LW52aWRpYS12ZXIiOiIyLjAiLCJuYmYiOjE3NTYxNjg5MjYsImlzcyI6Imh0dHBzOi8vbnJhcy5hdHRlc3RhdGlvbi5udmlkaWEuY29tIiwieC1udmlkaWEtb3ZlcmFsbC1hdHQtcmVzdWx0Ijp0cnVlLCJzdWJtb2RzIjp7IkdQVS0wIjpbIkRJR0VTVCIsWyJTSEEtMjU2IiwiMDJmYzJmMTg3M2JkZjg5Y2VlNGYzZTQzYzU3ZTE3YzI0ODUxODcwMmQ4ZGZjMzcwNmE3YjdmZTgwMzZlOTNkMCJdXX0sImVhdF9ub25jZSI6IjRkNmUwYzQ5MzIxZDIyZGFhOWJkN2ZjMjIwNWUzODFmOTUwNmMyMGU3N2RkNTA4MmVjZjVlMTI0ZWMwZjQ2MTgiLCJleHAiOjE3NTYxNzI1MjYsImlhdCI6MTc1NjE2ODkyNiwianRpIjoiYzFhM2NkYzktZWUyMi00MmFkLTljZDEtNDRhMTE5OWYyZGVlIn0.199S4bah6SVZpy4lpBvRBc975tmf25gkf_mLDwR9-fwrc_kWYePNxGygTRQUzGbRdbrZOQHXWP0eALUPkJvmwGIV_MVfHRIKaBIRdr1e2_7jEP1-mqkbCmbefimiZN8t\"\n  ],\n  {\n    \"GPU-0\": \"eyJraWQiOiJudi1lYXQta2lkLXByb2QtMjAyNTA4MjQxNzI2MzczMzEtMGM4YzM2MzQtY2ZkMC00YmViLWFmNWYtMTE2MzliOWUxOTIyIiwiYWxnIjoiRVMzODQifQ.eyJ4LW52aWRpYS1ncHUtZHJpdmVyLXJpbS1zY2hlbWEtdmFsaWRhdGVkIjp0cnVlLCJpc3MiOiJodHRwczovL25yYXMuYXR0ZXN0YXRpb24ubnZpZGlhLmNvbSIsIngtbnZpZGlhLWdwdS1hdHRlc3RhdGlvbi1yZXBvcnQtY2VydC1jaGFpbi12YWxpZGF0ZWQiOnRydWUsImVhdF9ub25jZSI6IjRkNmUwYzQ5MzIxZDIyZGFhOWJkN2ZjMjIwNWUzODFmOTUwNmMyMGU3N2RkNTA4MmVjZjVlMTI0ZWMwZjQ2MTgiLCJ4LW52aWRpYS1ncHUtdmJpb3MtcmltLXNpZ25hdHVyZS12ZXJpZmllZCI6dHJ1ZSwieC1udmlkaWEtZ3B1LXZiaW9zLXJpbS1mZXRjaGVkIjp0cnVlLCJleHAiOjE3NTYxNzI1MjYsImlhdCI6MTc1NjE2ODkyNiwidWVpZCI6IjY0Mjk2MDE4OTI5ODAwNzUxMTI1MDk1ODAzMDUwMDc0OTE1MjczMDIyMTE0MjQ2OCIsImp0aSI6IjFhMzhjMTAzLWMyODAtNDQyMi1hZDc1LTRkMTA3OTkyMGI2MyIsIngtbnZpZGlhLWdwdS1hdHRlc3RhdGlvbi1yZXBvcnQtbm9uY2UtbWF0Y2giOnRydWUsIngtbnZpZGlhLWdwdS12Ymlvcy1pbmRleC1uby1jb25mbGljdCI6dHJ1ZSwieC1udmlkaWEtZ3B1LXZiaW9zLXJpbS1jZXJ0LXZhbGlkYXRlZCI6dHJ1ZSwic2VjYm9vdCI6dHJ1ZSwieC1udmlkaWEtZ3B1LWF0dGVzdGF0aW9uLXJlcG9ydC1wYXJzZWQiOnRydWUsIngtbnZpZGlhLWdwdS1kcml2ZXItcmltLXNpZ25hdHVyZS12ZXJpZmllZCI6dHJ1ZSwieC1udmlkaWEtZ3B1LWFyY2gtY2hlY2siOnRydWUsIngtbnZpZGlhLWF0dGVzdGF0aW9uLXdhcm5pbmciOm51bGwsIm5iZiI6MTc1NjE2ODkyNiwieC1udmlkaWEtZ3B1LWRyaXZlci12ZXJzaW9uIjoiNTcwLjEzMy4yMCIsIngtbnZpZGlhLWdwdS1kcml2ZXItcmltLW1lYXN1cmVtZW50cy1hdmFpbGFibGUiOnRydWUsIngtbnZpZGlhLWdwdS1hdHRlc3RhdGlvbi1yZXBvcnQtc2lnbmF0dXJlLXZlcmlmaWVkIjp0cnVlLCJod21vZGVsIjoiR0gxMDAgQTAxIEdTUCBCUk9NIiwiZGJnc3RhdCI6ImRpc2FibGVkIiwieC1udmlkaWEtZ3B1LWRyaXZlci1yaW0tZmV0Y2hlZCI6dHJ1ZSwib2VtaWQiOiI1NzAzIiwieC1udmlkaWEtZ3B1LXZiaW9zLXJpbS1zY2hlbWEtdmFsaWRhdGVkIjp0cnVlLCJtZWFzcmVzIjoic3VjY2VzcyIsIngtbnZpZGlhLWdwdS1kcml2ZXItcmltLWNlcnQtdmFsaWRhdGVkIjp0cnVlLCJ4LW52aWRpYS1ncHUtdmJpb3MtdmVyc2lvbiI6Ijk2LjAwLkNGLjAwLjAyIiwieC1udmlkaWEtZ3B1LXZiaW9zLXJpbS1tZWFzdXJlbWVudHMtYXZhaWxhYmxlIjp0cnVlfQ.Zjac1Al0OsYbrXu7lOKDAH7lLNnRU_G2R1UJBnUpvZKL1EE8mjPyy-4sqRvE_d8uZJ4GuhXoy_EonyuUIXESd3sxjY0Eohe9Rtlzatj14iLOdcVrF_eOq12ZHNIYs4Go\"\n  }\n]\n</code></pre> <p>Tip</p> <p>NVIDIA's attestation verification response returns a \"Entity Attestation Token\" (EAT) encoded as a JSON Web Token (JWT)</p> <p>To decode these values, you can use an online tool such as jwt.io or a library such as Jose.</p> <p>Example Formatted Result:</p> <pre><code>\"JWT\":\n{\n  \"sub\": \"NVIDIA-PLATFORM-ATTESTATION\",\n  \"x-nvidia-ver\": \"2.0\",\n  \"nbf\": 1756168926,\n  \"iss\": \"https://nras.attestation.nvidia.com\",\n  \"x-nvidia-overall-att-result\": true,\n  \"submods\": {\n    \"GPU-0\": [\n      \"DIGEST\",\n      [\n        \"SHA-256\",\n        \"02fc2f1873bdf89cee4f3e43c57e17c248518702d8dfc3706a7b7fe8036e93d0\"\n      ]\n    ]\n  },\n  \"eat_nonce\": \"4d6e0c49321d22daa9bd7fc2205e381f9506c20e77dd5082ecf5e124ec0f4618\",\n  \"exp\": 1756172526,\n  \"iat\": 1756168926,\n  \"jti\": \"c1a3cdc9-ee22-42ad-9cd1-44a1199f2dee\"\n}\n\n\"GPU-0\":\n{\n  \"x-nvidia-gpu-driver-rim-schema-validated\": true,\n  \"iss\": \"https://nras.attestation.nvidia.com\",\n  \"x-nvidia-gpu-attestation-report-cert-chain-validated\": true,\n  \"eat_nonce\": \"4d6e0c49321d22daa9bd7fc2205e381f9506c20e77dd5082ecf5e124ec0f4618\",\n  \"x-nvidia-gpu-vbios-rim-signature-verified\": true,\n  \"x-nvidia-gpu-vbios-rim-fetched\": true,\n  \"exp\": 1756172526,\n  \"iat\": 1756168926,\n  \"ueid\": \"642960189298007511250958030500749152730221142468\",\n  \"jti\": \"1a38c103-c280-4422-ad75-4d1079920b63\",\n  \"x-nvidia-gpu-attestation-report-nonce-match\": true,\n  \"x-nvidia-gpu-vbios-index-no-conflict\": true,\n  \"x-nvidia-gpu-vbios-rim-cert-validated\": true,\n  \"secboot\": true,\n  \"x-nvidia-gpu-attestation-report-parsed\": true,\n  \"x-nvidia-gpu-driver-rim-signature-verified\": true,\n  \"x-nvidia-gpu-arch-check\": true,\n  \"x-nvidia-attestation-warning\": null,\n  \"nbf\": 1756168926,\n  \"x-nvidia-gpu-driver-version\": \"570.133.20\",\n  \"x-nvidia-gpu-driver-rim-measurements-available\": true,\n  \"x-nvidia-gpu-attestation-report-signature-verified\": true,\n  \"hwmodel\": \"GH100 A01 GSP BROM\",\n  \"dbgstat\": \"disabled\",\n  \"x-nvidia-gpu-driver-rim-fetched\": true,\n  \"oemid\": \"5703\",\n  \"x-nvidia-gpu-vbios-rim-schema-validated\": true,\n  \"measres\": \"success\",\n  \"x-nvidia-gpu-driver-rim-cert-validated\": true,\n  \"x-nvidia-gpu-vbios-version\": \"96.00.CF.00.02\",\n  \"x-nvidia-gpu-vbios-rim-measurements-available\": true\n}\n</code></pre>"},{"location":"cloud/verification/#verify-tdx-quote","title":"Verify TDX Quote","text":"<p>You can verify the Intel TDX quote with the value of <code>intel_quote</code> at TEE Attestation Explorer.</p>"},{"location":"cloud/verification/#chat-message-verification","title":"Chat Message Verification","text":"<p>You can verify each chat message with NEAR AI Confidential Cloud. For this you will need:</p> <ol> <li>Chat Message REQUEST Hash</li> <li>Chat Message RESPONSE Hash</li> <li>Chat Message Signature</li> </ol>"},{"location":"cloud/verification/#chat-message-request-hash","title":"Chat Message Request Hash","text":"<p>The value is calculated from the exact JSON request body string.</p> <p>Example:</p> <pre><code>{\n  \"messages\": [\n    {\n      \"content\": \"Respond with only two words.\",\n      \"role\": \"user\"\n    }\n  ],\n  \"stream\": true,\n  \"model\": \"llama-3.3-70b-instruct\"\n}\n</code></pre> <p>Which hashes to:</p> <pre><code>31f46232b8ae6154e75a68256523851c1ce84f9ad53a1f8290c9d0576b95929f\n</code></pre> <p>Here is an example of how to get the sha256 hash of your message request body:</p> JavaScript <pre><code>import crypto from 'crypto';\n\nconst requestBody = JSON.stringify({\n  \"messages\": [\n    {\n      \"content\": \"Respond with only two words.\",\n      \"role\": \"user\"\n    }\n  ],\n  \"stream\": true,\n  \"model\": \"llama-3.3-70b-instruct\"\n});\n\nconst hash = crypto.createHash('sha256').update(requestBody).digest('hex');\nconsole.log(hash); //31f46232b8ae6154e75a68256523851c1ce84f9ad53a1f8290c9d0576b95929f\n</code></pre>"},{"location":"cloud/verification/#chat-message-response-hash","title":"Chat Message Response Hash","text":"<p>This value is calculated from the exact response body string.</p> <p>Example Response Body:</p> <pre><code>data: {\"id\":\"chatcmpl-13edbcd23c9e4139b796fa988a88451b\",\"created\":1756693844,\"model\":\"phala/llama-3.3-70b-instruct\",\"object\":\"chat.completion.chunk\",\"choices\":[{\"index\":0,\"delta\":{\"content\":\"I\",\"role\":\"assistant\"}}]}\n\ndata: {\"id\":\"chatcmpl-13edbcd23c9e4139b796fa988a88451b\",\"created\":1756693844,\"model\":\"phala/llama-3.3-70b-instruct\",\"object\":\"chat.completion.chunk\",\"choices\":[{\"index\":0,\"delta\":{\"content\":\" agree\"}}]}\n\ndata: {\"id\":\"chatcmpl-13edbcd23c9e4139b796fa988a88451b\",\"created\":1756693844,\"model\":\"phala/llama-3.3-70b-instruct\",\"object\":\"chat.completion.chunk\",\"choices\":[{\"finish_reason\":\"stop\",\"index\":0,\"delta\":{}}]}\n\ndata: [DONE]\n</code></pre> <p>Which hashes to:</p> <pre><code>5d679ec62b9d8e9681085814391bfef9e837b8cc08757f479302311b828284b2\n</code></pre> <p>Here is an example of how to get the sha256 hash of your message response body:</p> JavaScript <pre><code>const response = await fetch('https://cloud-api.near.ai/v1/chat/completions', {\n  method: 'POST',\n  headers: {\n    'accept': 'application/json',\n    'Content-Type': 'application/json',\n    'Authorization': `Bearer ${process.env.NEARAI_CLOUD_API_KEY}`\n  },\n  body: requestBody\n});\n\nconst responseBody = await response.text();\nconst hash = crypto.createHash('sha256').update(responseBody).digest('hex');\nconsole.log(hash); // 5d679ec62b9d8e9681085814391bfef9e837b8cc08757f479302311b828284b2\n</code></pre>"},{"location":"cloud/verification/#chat-message-signature","title":"Chat Message Signature","text":"<p>From the Chat Message Response you will get a unique chat <code>id</code> that is used to fetch the Chat Message Signature from NEAR AI Confidential Cloud.</p> <p>By default, you can query another API with the value of <code>id</code> in the response in 5 minutes after chat completion. The signature will be persistent in the LLM gateway once it's queried.</p> <p>Use the following endpoint to get this signature:</p> <pre><code>GET https://cloud-api.near.ai/v1/signature/{chat_id}?model={model_id}&amp;signing_algo=ecdsa\n</code></pre> <p>Implementation: This endpoint is defined in the NEAR AI Private ML SDK.</p> <p>For example, the response in the previous section, the <code>id</code> is:</p> <p><code>chatcmpl-13edbcd23c9e4139b796fa988a88451b</code></p> <pre><code>curl -X GET 'https://cloud-api.near.ai/signature/chatcmpl-13edbcd23c9e4139b796fa988a88451b?model=llama-3.3-70b-instruct&amp;signing_algo=ecdsa' \\\n    -H \"Content-Type: application/json\" \\\n    -H \"Authorization: Bearer &lt;YOUR-NEARAI-CLOUD-API-KEY&gt;\"\n</code></pre> <p>Example Response:</p> <pre><code>{\n  \"text\": \"31f46232b8ae6154e75a68256523851c1ce84f9ad53a1f8290c9d0576b95929f:5d679ec62b9d8e9681085814391bfef9e837b8cc08757f479302311b828284b2\",\n  \"signature\": \"0x5ed3ac0642bceb8cdd5b222cd2db36b92af2a4d427f11cd1bec0e5b732b94628015f32f2cec91865148bf9d6f56ab673645f6bc500421cd28ff120339ea7e1a01b\",\n  \"signing_address\": \"0x1d58EE32e9eB327c074294A2b8320C47E33b9316\",\n  \"signing_algo\": \"ecdsa\"\n}\n</code></pre> <p>The above response gives us all of the crucial information we need to verify that the message was executed in our trusted environment:</p> <ul> <li><code>text</code> - This is the Chat Message REQUEST Hash &amp; Chat Message RESPONSE Hash concatenated with a <code>:</code> separator.</li> <li><code>signature</code> - This is the cryptographic signature of the <code>text</code> field, generated using the TEE's private key </li> <li><code>signing_address</code> - Public key of the TEE unique to the model we used</li> <li><code>signing_algo</code> - Cryptography curve used to sign</li> </ul> <p>You can see that <code>text</code> is:</p> <p><code>31f46232b8ae6154e75a68256523851c1ce84f9ad53a1f8290c9d0576b95929f:5d679ec62b9d8e9681085814391bfef9e837b8cc08757f479302311b828284b2</code></p> <p>This exactly matches the concatenated values we calculated in the previous sections:</p> <ul> <li>Request hash: <code>31f46232b8ae6154e75a68256523851c1ce84f9ad53a1f8290c9d0576b95929f</code></li> <li>Response hash: <code>5d679ec62b9d8e9681085814391bfef9e837b8cc08757f479302311b828284b2</code></li> </ul> <p>Note</p> <p>Due to resource limitations, signatures are kept in memory for 5 minutes after the response is generated. However, once queried within this 5-minute window, the signature becomes persistent in the LLM gateway for future verification.</p>"},{"location":"cloud/verification/#verify-signature","title":"Verify Signature","text":"<p>Signature verification can be easily done with any standard ECDSA verification library such as ethers or even an online tool such as etherscan's VerifySignatures.</p> <p>These tools will require:</p> <ul> <li><code>Address</code>: What the expected address is for the signature. In our case it will be the one retrieved from your attestation API query. </li> <li><code>Message</code>: The original message before signing. In our case it will be the sha256 hash of the request and response (<code>text</code> field from Get Chat Message Signature)</li> <li><code>Signature</code>: The signed message from above</li> </ul> <p>Here is an example of how to verify the Chat Message signature using <code>ethers</code>:</p> <pre><code>  import { ethers } from 'ethers';\n\n  const text = \"65b0adb47d0450971803dfb18d0ce4af4a64d27420a43d5aad4066ebf10b81b5:e508d818744d175a62aae1a9fb3f373c075460cbe50bf962a88ac008c843dff1\";\n  const signature = \"0xf28f537325c337fd96ae6e156783c904ca708dcd38fb8a476d1280dfc72dc88e4fcb5c3941bdd4f8fe5238a2253b975c6b02ea6a0a450b5b0f9296ab54cf24181b\";\n  const expectedAddress = \"0xc51268C9b46140619CBC066A34441a6ca51F85f9\";\n\n  // Recover the address from the signature\n  const recoveredAddress = ethers.verifyMessage(text, signature);\n\n  // Compare with expected address (case-insensitive)\n  const isValid = recoveredAddress.toLowerCase() === expectedAddress.toLowerCase();\n\n  console.log(\"Text:\", text);\n  console.log(\"Expected address:\", expectedAddress);\n  console.log(\"Recovered address:\", recoveredAddress);\n  console.log(\"Signature valid:\", isValid);\n</code></pre>"},{"location":"models/benchmarks_and_evaluations/","title":"Benchmarks and Evaluations","text":"<p><code>Benchmarks</code> allow you to compare different agents and solvers on specific tasks, so you can determine which is the best fit for your needs.</p> <p><code>Evaluations</code> are the results of running benchmarks. They are stored in the registry and can be used to compare different agents and solvers.</p>"},{"location":"models/benchmarks_and_evaluations/#how-is-a-benchmark-implemented","title":"How is a benchmark implemented?","text":"<p>A benchmark is the combination of a dataset and a solver (more on this below). </p> <p>Once you have created a dataset and its solver, you can run the benchmark using the <code>benchmark</code> command.</p> <p>For example, to run the <code>mpbb</code> benchmark on the <code>llama v3</code>, you can use:</p> <pre><code>nearai benchmark run mbpp MBPPSolverStrategy \\\n    --model llama-v3-70b-instruct \\\n    --subset=train \\\n    --max_concurrent=1\n</code></pre>"},{"location":"models/benchmarks_and_evaluations/#adding-a-benchmark-dataset","title":"Adding a benchmark dataset","text":"<p><code>nearai</code> leverages huggingface datasets as the primitive when operating with datasets + benchmarks (see <code>load_dataset</code>). This means that to add a new benchmark, you need to create a new dataset and register it with the <code>nearai</code> registry (we will go over this in Implementing the \"3 digit addition\" benchmark).</p> <p>There is also a support for datasets of custom format.</p>"},{"location":"models/benchmarks_and_evaluations/#adding-a-solver","title":"Adding a solver","text":"<p>To implement a solver, you will need to implement the SolverStrategy interface under the <code>nearai.solvers</code> module. The most important method the solver should implement is the <code>solve</code> method. This method should take a datum, run your implementation specific agentic strategy / strategies, and return a result.</p>"},{"location":"models/benchmarks_and_evaluations/#implementing-the-3-digit-addition-benchmark","title":"Implementing the \"3 digit addition\" benchmark","text":"<p>In this section we will be implementing a benchmark we'll call \"3 digit addition\". The goal of the benchmark is to test an agents ability to add two 1-3 digit numbers together. The dataset will consist of 1000 examples of 3 digit addition problems and their solutions. The solver will adjudicate the agent answers and return a single accuracy score. While this benchmark is simple and can be solved with a simple program, it serves as a good example of how to implement a benchmark in <code>nearai</code>.</p>"},{"location":"models/benchmarks_and_evaluations/#step-1-creating-the-dataset","title":"Step 1: Creating the dataset","text":"<p>To create this dataset, we will first synthetically generate the data. We will then register the dataset with the <code>nearai</code> registry.</p> <pre><code>import random\nfrom itertools import product\n\nimport datasets\n\nSAMPLE_SIZE = 1000\nSEED = 42\nPATH = \"3_digit_addition\"\n\nrandom.seed(SEED)\ndatasets.Dataset.from_generator(\n    lambda: iter(\n        {\n            \"input\": f\"{a} + {b}\",\n            \"output\": str(a + b)\n        }\n        for a, b in random.sample(list(product(range(1000), range(1000))), SAMPLE_SIZE)\n    ),\n    features=datasets.Features(\n        {\n            \"input\": datasets.Value(\"string\"),\n            \"output\": datasets.Value(\"string\")\n        }\n    )\n).save_to_disk(PATH)\n</code></pre> <p>Now to upload the dataset to the registry we'll run the command:</p> <pre><code>nearai registry upload ./3_digit_addition\n</code></pre>"},{"location":"models/benchmarks_and_evaluations/#step-2-creating-the-solver","title":"Step 2: Creating the solver","text":"<p>To create the solver, we will implement the <code>SolverStrategy</code> interface. The solver will take in a datum, parse the input, execute any setup for the agent, run the agent, and return the correctness of the agents result.</p> Remember <p>To ensure this solver is registered with <code>nearai</code>:</p> <ol> <li>Write this implementation in the <code>nearai.solvers</code> module.</li> <li>Import it in the <code>__init__.py</code> file in the <code>nearai.solvers</code> module.</li> </ol> <pre><code># ... other imports ...\nfrom pydantic import BaseModel\nfrom huggingface import Dataset\nfrom nearai.solvers import SolverStrategy\n\nfrom typing import Dict, List\n\nclass ThreeDigitAdditionDatum(BaseModel):\n    input: str\n    output: str\n\nclass ThreeDigitAdditionSolver(SolverStrategy):\n    \"\"\"Solver for the 3 digit addition benchmark.\"\"\"\n\n    def __init__(self, dataset_ref: Dataset, model: str = \"\", agent: str = \"\"):\n        super().__init__(model, agent)\n        self.dataset_ref = dataset_ref\n\n    def evaluation_name(self) -&gt; str:\n        return \"3_digit_addition\"\n\n    def compatible_datasets(self) -&gt; List[str]:\n        return [\"3_digit_addition\"]\n\n    def solve(self, datum: Dict[str, str]) -&gt; bool:\n        datum = ThreeDigitAdditionDatum(**datum)\n        label = datum.input.replace(\" + \", \"+\")\n        session = self.start_inference_session(label)\n\n        goal = f\"\"\"Please add the following numbers together: {datum.input}\\n\\nOutput the result only.\"\"\"\n        result = session.run_task(goal).strip()\n        return result == datum.output\n</code></pre> <p>The code above can run for both models and agents. If both <code>model</code> and <code>agent</code> are given, the <code>model</code> value will be inserted into <code>agent</code> metadata.</p> <p>To check agent functionality to write files: <pre><code>    def solve(self, datum: Dict[str, str]) -&gt; bool:\n        datum = ThreeDigitAdditionDatum(**datum)\n        label = datum.input.replace(\" + \", \"+\")\n        session = self.start_inference_session(label)\n\n        goal = f\"\"\"Please add the following numbers together: {datum.input}\\n\\nOutput the result in a file called 'result.txt'.\"\"\"\n        session.run_task(goal)\n        with open(os.path.join(session.path, \"result.txt\"), \"r\") as f:\n            result = f.read().strip()\n        return result == datum.output\n</code></pre></p>"},{"location":"models/benchmarks_and_evaluations/#step-3-running-the-benchmark","title":"Step 3: Running the benchmark","text":"<p>To run the benchmark, we will use the <code>nearai</code> CLI. We will specify the dataset and solver we want to use.</p> <pre><code>nearai benchmark run near.ai/3_digit_addition/1.00 ThreeDigitAdditionSolver --agent ~/.nearai/registry/&lt;my_agent&gt;\n</code></pre>"},{"location":"models/benchmarks_and_evaluations/#benchmarks-cache","title":"Benchmarks Cache","text":"<p>Benchmark individual tasks and completion are cached in registry or locally. To see registry benchmark completion caches:</p> <pre><code>nearai benchmark list\n</code></pre> <p>To force execution and overwrite cache pass <code>--force</code> flag.</p> <pre><code>nearai benchmark run near.ai/mbpp/1.0.0 MBPPSolverStrategy --model 'llama-3p2-1b-instruct' --subset test --force\n</code></pre>"},{"location":"models/benchmarks_and_evaluations/#example-runs","title":"Example runs","text":"<pre><code>$ nearai benchmark run near.ai/mbpp/1.0.0 MBPPSolverStrategy --model 'llama-3p2-1b-instruct' --subset test\n$ nearai benchmark run near.ai/mmlu/1.0.0 MMLUSolverStrategy --model 'llama-v3p1-405b-instruct' --subset test\n$ nearai benchmark run near.ai/mbpp/1.0.0 MBPPSolverStrategy --model 'qwen2p5-72b-instruct' --subset test --agent ~/.nearai/registry/flatirons.near/example-travel-agent/1\n$ nearai benchmark run near.ai/live_bench/1.0.0 LiveBenchSolverStrategy --model 'qwen2p5-72b-instruct' --agent ~/.nearai/registry/flatirons.near/example-travel-agent/1\n</code></pre>"},{"location":"models/benchmarks_and_evaluations/#evaluations","title":"Evaluations","text":""},{"location":"models/benchmarks_and_evaluations/#recording-benchmark-result-as-an-evaluation","title":"Recording benchmark result as an evaluation","text":"<p>To record benchmark results as an evaluation, pass <code>--record</code>. It is strongly recommended to pass this flag after verifying successful run of the benchmark.</p> <pre><code>$ nearai benchmark run near.ai/mbpp/1.0.0 MBPPSolverStrategy --model 'llama-3p2-1b-instruct' --subset test\nFinal score: 131/500 - 26.20%\n$ nearai benchmark run near.ai/mbpp/1.0.0 MBPPSolverStrategy --model 'llama-3p2-1b-instruct' --subset test --record\n</code></pre> <p>That creates new evaluation entry in the registry: <pre><code>$ nearai registry list --category=evaluation\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 entry                                                                  \u2502 category   \u2502 description   \u2502 tags   \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 alomonos.near/evaluation_mbpp_model_llama-v3p2-1b-                     \u2502 evaluation \u2502               \u2502        \u2502\n\u2502 instruct_provider_fireworks/0.0.1                                      \u2502            \u2502               \u2502        \u2502\n</code></pre></p>"},{"location":"models/benchmarks_and_evaluations/#view-evaluation-table","title":"View evaluation table","text":"<p>To view evaluation table in CLI: <pre><code>$ nearai evaluation table --num_columns=8\n$ nearai evaluation table --all_key_columns --num_columns=8\n$ nearai evaluation table --all_metrics\n</code></pre></p> <p>https://app.near.ai/evaluations has a functionality to choose any columns.</p>"},{"location":"models/benchmarks_and_evaluations/#submit-an-experiment","title":"Submit an experiment","text":"<p>To submit a new experiment run:</p> <pre><code>nearai submit --command &lt;COMMAND&gt; --name &lt;EXPERIMENT_NAME&gt; [--nodes &lt;NUMBER_OF_NODES&gt;] [--cluster &lt;CLUSTER&gt;]\n</code></pre> <p>This will submit a new experiment. The command must be executed from a folder that is a git repository (public github repositories, and private github repositories on the same organization as nearai are supported). The current commit will be used for running the command so make sure it is already available online. The diff with respect to the current commit will be applied remotely (new files are not included in the diff).</p> <p>On each node the environment variable <code>ASSIGNED_SUPERVISORS</code> will be available with a comma separated list of supervisors that are running the experiment. The current supervisor can be accessed via <code>nearai.CONFIG.supervisor_id</code>. See examples/prepare_data.py for an example.</p>"},{"location":"models/benchmarks_and_evaluations/#issues","title":"Issues","text":"<ul> <li>Overwriting existing evaluation entry is currently not supported</li> <li>litellm.Timeout errors when running benchmark</li> <li>Feature request: tag individual evaluation metrics</li> <li>Feature request: add view for a metric</li> <li>Feature request: add cost of running benchmark to evaluation results as a separate metric</li> <li>Feature request: hide evaluation results for hidden agents and models</li> <li>Capabilities Benchmarks Tracking: list of benchmarks we want to add</li> </ul>"},{"location":"models/fine_tuning/","title":"<code>nearai</code> fine-tuning guide","text":"<p>As a part of the <code>nearai</code> project, we provide a collection of tools to fine-tune and evaluate models. Fine-tuning is a set of techniques to tune model parameters in a parameter-efficient way to improve model performance on specific tasks. More commonly, fine-tuning is used to modify the behavior of a pre-trained model. Some examples of this are to produce structured output (JSON, XLM, etc), to produce stylized output (poetic, neutral, etc), or to respond properly to instruction based prompts.</p> <p>In this guide, we will walk through the process of fine-tuning <code>llama-3-8b-instruct</code> on the orca-math-word-problems-200k dataset to improve its performance on the gsm8k benchmark.</p>"},{"location":"models/fine_tuning/#step-1-create-the-dataset","title":"Step 1: Create the dataset","text":"<p>The two datasets we will be using are orca-math-word-problems-200k and gsm8k. Both datasets are a collection of word based math problems + answers. For convenience, we will download the datasets from huggingface, save it to disk, and then upload it into the <code>nearai</code> registry.</p> <pre><code>import re\nfrom textwrap import dedent\nfrom datasets import load_dataset, concatenate_datasets, DatasetDict\nfrom transformers import AutoTokenizer\ntokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3-8B-Instruct\")\n\nds_math_word_problems = load_dataset(\"HuggingFaceH4/orca-math-word-problems-200k\")\nds_gsm8k = load_dataset(\"openai/gsm8k\", \"main\")['train']\n\n## create new column by concatenating the 'question' and 'answer' columns\ndef to_q_a(x):\n    q_n_a = tokenizer.apply_chat_template(\n        [\n            {\n                \"role\": \"system\",\n                \"content\": \"You are a helpful assistant. Please answer the math question.\"\n            },\n            {\n                \"role\": \"user\",\n                \"content\": x[\"question\"]\n            },\n            {\n                \"role\": \"assistant\",\n                \"content\": x[\"answer\"]\n            }\n        ],\n        tokenize=False\n    )\n    return {\n        \"question_and_answer\": q_n_a\n    }\nds_math_word_problems = ds_math_word_problems.map(to_q_a)\n\ndef to_q_a_gsm8k(x):\n    q_n_a = tokenizer.apply_chat_template(\n        [\n            {\n                \"role\": \"system\",\n                \"content\": \"You are a helpful assistant. Please answer the math question.\"\n            },\n            {\n                \"role\": \"user\",\n                \"content\": x[\"question\"]\n            },\n            {\n                \"role\": \"assistant\",\n                \"content\": re.sub(r\"&lt;&lt;.*?&gt;&gt;\", \"\", x[\"answer\"])\n            }\n        ],\n        tokenize=False\n    )\n    return {\n        \"question_and_answer\": q_n_a\n    }\nds_gsm8k = ds_gsm8k.map(to_q_a_gsm8k)\n\n## combine the datasets\nds_combined = concatenate_datasets([ds_math_word_problems['train_sft'], ds_gsm8k])\nds_combined = ds_combined.remove_columns([col for col in ds_combined.column_names if col != \"question_and_answer\"])\n\n# Add a split on 'train'\nds_combined = ds_combined.train_test_split(test_size=0.0001, seed=42)\nds_dict = DatasetDict({\n    'train': ds_combined['train'],\n    'validation': ds_combined['test']\n})\nds_dict.save_to_disk(\"orca_math_gsm8k_train\")\n</code></pre> <pre><code>nearai registry upload ./orca_math_gsm8k_train\n</code></pre>"},{"location":"models/fine_tuning/#step-2-fine-tune-the-model","title":"Step 2: Fine-tune the model","text":"<p>Under the hood, <code>nearai</code> uses torchtune to manage the fine-tuning process. To launch a fine-tuning job you can use <code>nearai finetune</code>.</p> <p>Here is the command we used to fine-tune <code>llama-3-8b-instruct</code> on our combined <code>orca-math-word-problems-200k</code> and <code>gsm8k</code> dataset on an 8-GPU machine:</p> <pre><code>uv run python3 -m nearai finetune start \\\n    --model llama-3-8b-instruct \\\n    --format llama3-8b \\\n    --tokenizer llama-3-8b-instruct/tokenizer.model \\\n    --dataset ./orca_math_gsm8k_train \\\n    --method nearai.finetune.text_completion.dataset \\\n    --column question_and_answer \\\n    --split train \\\n    --num_procs 8\n</code></pre> <p>To change the configuration of the fine-tuning job, edit <code>etc/finetune/llama3-8b.yml</code>.</p> <p>Included in the output of the command is the path to the fine-tuned model checkpoint. In this case, the path was <code>~.nearai/finetune/job-2024-08-29_20-58-08-207756662/checkpoint_output</code>. The path may/will be different based on the time you run the command.</p>"},{"location":"models/fine_tuning/#step-3-serve-the-fine-tuned-model","title":"Step 3: Serve the fine-tuned model","text":"<p>To serve fine-tuned models, we use vllm. Once we serve the fine-tuned model + the baseline model, we will benchmark it against both.</p> <pre><code>uv run python3 -m vllm.entrypoints.openai.api_server \\\n    --model meta-llama/Meta-Llama-3-8B-Instruct \\\n    --enable-lora \\\n    --lora-modules mynewlora=&lt;path_to_checkpoint&gt; \\\n    --tensor-parallel 8\n</code></pre> <p>Now we will run the <code>gsm8k</code> benchmark on both the baseline model and the fine-tuned model using <code>nearai benchmark</code>. The solvers will call our fine-tuned model and the baseline model through the vllm server.</p> <pre><code>python3 -m nearai benchmark run \\\n    cmrfrd.near/gsm8k/0.0.2 \\\n    GSM8KSolverStrategy \\\n    --subset test \\\n    --model local::meta-llama/Meta-Llama-3-8B-Instruct\n\npython3 -m nearai benchmark run \\\n    cmrfrd.near/gsm8k/0.0.2 \\\n    GSM8KSolverStrategy \\\n    --subset test \\\n    --model local::mynewlora\n</code></pre> <p>And we can see the results of the benchmark. And we can see that the fine-tuned model performs better than the baseline model.</p> <pre><code># meta-llama/Meta-Llama-3-8B-Instruct\nCorrect/Seen - 1061/1319 - 80.44%\n\n# fine tuned llama3-8b-instruct\nCorrect/Seen - 966/1319 - 73.24%\n</code></pre> <p>From these results, we can see that our fine-tuned model needs improvement to perform better than the baseline model.</p>"},{"location":"models/home/","title":"Home","text":"<p>NEAR AI provides access to powerful open-source AI models including Llama, Qwen, Deepseek and Stable Diffusion, leveraging various AI providers to deliver high-quality responses.</p> <p>Seamlessly find, tune and integrate models into your applications for chat completions, image creation, and function calling.</p> <p>Tip</p> <p>Are you looking to use the model in an agent? Check out the Agents Guide.</p>"},{"location":"models/home/#prerequisites","title":"\u26a0\ufe0f Prerequisites","text":"<p>Before you start using NEAR AI models, please make sure you have the NEAR AI CLI installed and have logged in with your Near wallet.</p>"},{"location":"models/home/#listing-available-models","title":"Listing Available Models","text":"<p>You can find a non-exhaustive list of models available on NEAR AI in the Web Hub. If you want to find all supported models on NEAR AI, run the following Python script:</p> <pre><code>import openai\nimport json\nimport nearai\n\nhub_url = \"https://api.near.ai/v1\"\nauth = nearai.config.load_config_file()[\"auth\"]\nsignature = json.dumps(auth)\n\nclient = openai.OpenAI(base_url=hub_url, api_key=signature)\n\nmodels = client.models.list()\nprint(models)\n</code></pre> <p>Warning</p> <p>Remember to use the NEAR CLI to login using your NEAR wallet</p> <pre><code>$&gt; nearai login\n</code></pre> Output <pre><code>{'fireworks': 'fireworks::accounts/yi-01-ai/models/yi-large'}, NamespacedName(name='aiderlm0', namespace='melse0299e'): {'fireworks': 'fireworks::accounts/mels-e0299e/models/aiderlm-v0'}, NamespacedName(name='flux1schnellfp8', namespace=''): {'fireworks': 'fireworks::accounts/fireworks/models/flux-1-schnell-fp8'}, NamespacedName(name='flux1devfp8', namespace=''): {'fireworks': 'fireworks::accounts/fireworks/models/flux-1-dev-fp8'}, NamespacedName(name='mistralsmall24binstruct2501', namespace=''): {'fireworks': 'fireworks::accounts/fireworks/models/mistral-small-24b-instruct-2501'}, NamespacedName(name='deepseekr1', namespace=''): {'fireworks': 'fireworks::accounts/fireworks/models/deepseek-r1'}, NamespacedName(name='mixtral8x22binstruct', namespace=''): {'fireworks': 'fireworks::accounts/fireworks/models/mixtral-8x22b-instruct'}, NamespacedName(name='llama3_8binstruct', namespace=''): {'fireworks': 'fireworks::accounts/fireworks/models/llama-v3-8b-instruct'}, NamespacedName(name='deepseek3', namespace=''): {'fireworks': 'fireworks::accounts/fireworks/models/deepseek-v3'}, NamespacedName(name='qwq32bpreview', namespace=''): {'fireworks': 'fireworks::accounts/fireworks/models/qwen-qwq-32b-preview'}, NamespacedName(name='llama3p1_405binstruct', namespace=''): {'fireworks': 'fireworks::accounts/fireworks/models/llama-v3p1-405b-instruct'}, NamespacedName(name='llama3p1_8binstruct', namespace=''): {'fireworks': 'fireworks::accounts/fireworks/models/llama-v3p1-8b-instruct'}, NamespacedName(name='llama3_8binstructhf', namespace=''): {'fireworks': 'fireworks::accounts/fireworks/models/llama-v3-8b-instruct-hf'}, NamespacedName(name='llama3p2_3binstruct', namespace=''): {'fireworks': 'fireworks::accounts/fireworks/models/llama-v3p2-3b-instruct'}, NamespacedName(name='qwen2p5_72binstruct', namespace=''): {'fireworks': 'fireworks::accounts/fireworks/models/qwen2p5-72b-instruct'}, NamespacedName(name='llama3p1_405binstructlong', namespace=''): {'fireworks': 'fireworks::accounts/fireworks/models/llama-v3p1-405b-instruct-long'}, NamespacedName(name='llama3p1_70binstruct', namespace=''): {'fireworks': 'fireworks::accounts/fireworks/models/llama-v3p1-70b-instruct'}, NamespacedName(name='llama3_70binstruct', namespace=''): {'fireworks': 'fireworks::accounts/fireworks/models/llama-v3-70b-instruct'}, NamespacedName(name='llama3p2_90bvisioninstruct', namespace=''): {'fireworks': 'fireworks::accounts/fireworks/models/llama-v3p2-90b-vision-instruct'}, NamespacedName(name='llama3p3_70binstruct', namespace=''): {'fireworks': 'fireworks::accounts/fireworks/models/llama-v3p3-70b-instruct'}, NamespacedName(name='qwen2p5coder32binstruct', namespace=''): {'fireworks': 'fireworks::accounts/fireworks/models/qwen2p5-coder-32b-instruct'}, NamespacedName(name='qwen2vl72binstruct', namespace=''): {'fireworks': 'fireworks::accounts/fireworks/models/qwen2-vl-72b-instruct'}, NamespacedName(name='llama3p2_11bvisioninstruct', namespace=''): {'fireworks': 'fireworks::accounts/fireworks/models/llama-v3p2-11b-vision-instruct'}, NamespacedName(name='llamaguard3_8b', namespace=''): {'fireworks': 'fireworks::accounts/fireworks/models/llama-guard-3-8b'}, NamespacedName(name='phi3vision128kinstruct', namespace=''): {'fireworks': 'fireworks::accounts/fireworks/models/phi-3-vision-128k-instruct'}, NamespacedName(name='dobbyminileashedllama3_18b', namespace='sentientfoundation'): {'fireworks': 'fireworks::accounts/sentientfoundation/models/dobby-mini-leashed-llama-3-1-8b'}, NamespacedName(name='dobbyminiunhingedllama3_18b', namespace='sentientfoundation'): {'fireworks': 'fireworks::accounts/sentientfoundation/models/dobby-mini-unhinged-llama-3-1-8b'}, NamespacedName(name='statetransitionllama3p1', namespace='marco0dece6'): {'fireworks': 'fireworks::accounts/marco-0dece6/models/state-transition-llama3p1'}, NamespacedName(name='mixtral8x7binstruct', namespace=''): {'fireworks': 'fireworks::accounts/fireworks/models/mixtral-8x7b-instruct'}, NamespacedName(name='mythomaxl2_13b', namespace=''): {'fireworks': 'fireworks::accounts/fireworks/models/mythomax-l2-13b'}, NamespacedName(name='tsr3', namespace='guzmanesneider75_94d6ba'): {'fireworks': 'fireworks::accounts/guzmanesneider75-94d6ba/models/tsr3'}, NamespacedName(name='serviceemergencyclassifierllama3_8binstruct', namespace='carlos0f74b3'): {'fireworks': 'fireworks::accounts/carlos-0f74b3/models/service-emergency-classifier-llama3-8b-instruct'}\n</code></pre> <p>Each model follows the format: <code>provider::account/model_name/model_version</code>. To list all providers:</p> <pre><code>providers = set([model.id.split(\"::\")[0] for model in models])\nprint(providers)\n</code></pre>"},{"location":"models/home/#working-with-ai-models","title":"Working with AI Models","text":"<p>When working with AI models, you will either want to directly use a model for inference, or fine-tune it for a specific use case.</p> <ul> <li>Fine-Tuning \u2013 Customize models for specific use cases.</li> <li>Benchmarks &amp; Evaluations \u2013 Assess model performance.</li> <li>Running Inference \u2013 Execute models for AI responses.</li> </ul> <pre><code>graph TD;\n    B[Available Models];\n    B --&gt;|Use Model| C[Inference API];\n    C --&gt;|Run Inference| D[Get Response];\n    B --&gt;|Fine-Tune| E[Customized Model];\n    E --&gt;|Benchmark| F[Evaluate Performance]; \n    F ----&gt;|Deploy| B;</code></pre>"},{"location":"tutorials/rag/agent/","title":"RAG Agent","text":"<p>Let's see how simple it is to create an agent that leverages the vector store we created in the previous section. This agent will demonstrate how to effectively use RAG (Retrieval-Augmented Generation) to provide accurate answers based on NEAR's documentation.</p> <p>Tip</p> <p>You can try out a live demo of this agent by signing in with your NEAR account below!</p>"},{"location":"tutorials/rag/agent/#documentation-agent","title":"Documentation Agent","text":"<p>In the previous section, we created a vector store and obtained its unique <code>id</code>. This <code>id</code> is a crucial piece that allows us to reference and interact with our stored embeddings. We'll now use this <code>id</code> within our agent to:</p> <ol> <li>Retrieve the vector store from storage</li> <li>Query it for relevant information based on user input</li> <li>Use the retrieved context to generate more informed responses</li> </ol> <p>The vector store <code>id</code> acts as a persistent identifier, enabling our agent to access the same knowledge base across multiple sessions or interactions.</p> <p>Let's update your <code>agent.py</code> file to include the vector store <code>id</code> and the RAG logic:</p> <pre><code>import json\nfrom nearai.agents.environment import Environment\n\nMODEL = \"llama-v3p3-70b-instruct\"\nVECTOR_STORE_ID = \"vs_cb8d5537f64d4f4aa6cbc95f\"\n\n\ndef run(env: Environment):\n    user_query = env.list_messages()[-1][\"content\"]\n\n    # Query the Vector Store\n    vector_results = env.query_vector_store(VECTOR_STORE_ID, user_query)\n    docs = [{\"file\": res[\"chunk_text\"]} for res in vector_results[:6]]\n\n    prompt = [\n        {\n            \"role\": \"user query\",\n            \"content\": user_query,\n        },\n        {\n            \"role\": \"documentation\",\n            \"content\": json.dumps(docs),\n        },\n        {\n            \"role\": \"system\",\n            \"content\": \"Give a brief but complete answer to the user's query, staying as true as possible to the documentation SPECIALLY when dealing with code.\"\n        }\n    ]\n\n    answer = env.completion(model=MODEL, messages=prompt)\n    env.add_reply(answer)\n\n\nrun(env)\n</code></pre>"},{"location":"tutorials/rag/agent/#how-it-works","title":"How it works?","text":"<p>In the code above, we extract the user's question from the environment's message history and use it as a semantic search query against our vector store. This allows us to find the most relevant document chunks based on their embedding similarity to the query.</p> <pre><code>vector_results = env.query_vector_store(VECTOR_STORE_ID, user_query)\n</code></pre> <p>The vector store returns a list of documents ranked by relevance to the user's query. To maintain efficiency and focus, we limit our context to the 6 most relevant document chunks:</p> <pre><code>docs = [{\"file\": res[\"chunk_text\"]} for res in vector_results[:6]]\n</code></pre> <p>We then construct a structured prompt with three key components:</p> <ol> <li>The user's original query</li> <li>The relevant documentation chunks</li> <li>A system message that guides the model's response behavior</li> </ol> <pre><code>prompt = [\n    {\n        \"role\": \"user query\",\n        \"content\": user_query,\n    },\n    {\n        \"role\": \"documentation\",\n        \"content\": json.dumps(docs),\n    },\n    {\n        \"role\": \"system\",\n        \"content\": \"Give a brief but complete answer to the user's query, staying as true as possible to the documentation SPECIALLY when dealing with code.\"\n    }\n]\n</code></pre> <p>This prompt structure ensures that the model has both the necessary context from our documentation and clear instructions on how to use it. We then pass this prompt to <code>env.completion()</code> to generate a response that accurately addresses the user's question based on our documentation.</p> <pre><code>answer = env.completion(model=MODEL, messages=prompt)\nenv.add_reply(answer)\n</code></pre>"},{"location":"tutorials/rag/agent/#did-it-work","title":"Did it work?","text":"<p>Let's try it out! Lets ask the agent the same questions we used when talking about hallucination:</p> What types of access keys are there in NEAR? <p>Assistant: There are two types of access keys in NEAR: </p> <ol> <li><code>Full Access</code> keys: Have full control over an account and should never be shared.</li> <li><code>Function-Call</code> keys: Can only sign calls for specific contracts, are meant to be shared, and do not allow transferring NEAR tokens. </li> </ol> <p><code>Function-Call</code> keys are defined by three attributes:  - <code>receiver_id</code>: The only contract that the key allows to call. - <code>method_names</code> (Optional): The contract's methods the key allows to call. - <code>allowance</code> (Optional): The amount of NEAR allowed to be spent on gas.</p> How can I create an account using the NEAR CLI? <p>Assistant: To create an account using the NEAR CLI, you can use the following command:</p> <pre><code>near account create-account sponsor-by-faucet-service &lt;your-account-id.testnet&gt; autogenerate-new-keypair save-to-keychain network-config testnet create\n</code></pre> <p>Replace <code>&lt;your-account-id.testnet&gt;</code> with the desired name for your account. This will create a new account with a generated key pair and save it to your keychain. </p> <p>Alternatively, you can also create an account using the <code>near create-account</code> command with the <code>--useFaucet</code> option:</p> <pre><code>near create-account &lt;accountId&gt; --useFaucet\n</code></pre> <p>Let's compare the results with our previous attempts. Without the vector store, the agent was prone to hallucination - generating plausible but incorrect responses. Now, the answers are both correct and complete grounded in the actual documentation - meaning that the agent was able to retrieve the right answers from the vector store.</p> <p>More impressively, the vector store was able to correctly pick up the relevant documents from the more than than 100 files that compose the official NEAR documentation.</p>"},{"location":"tutorials/rag/chunking/","title":"Chunking","text":"<p>Before converting documentation into vector embeddings, the NEAR AI platform segments the text into smaller, manageable chunks.</p> <p>This process - known as text chunking - makes it easier to process the documents, while allowing the model to better represent the information within each document.</p> <p>Tip</p> <p>Embedding models transform documents into fixed-size vectors, regardless of the original document size.</p> <p>By dividing documents into smaller chunks we can achieve a granular and accurate representation, since a single vector cannot capture a entire document's meaning.</p> <p>Warning</p> <p>Most users won't need to modify the default chunking settings. Only consider adjusting them if you notice the agent consistently struggles to answer questions due to how the documents are segmented.</p>"},{"location":"tutorials/rag/chunking/#chunking-strategy","title":"Chunking Strategy","text":"<p>When creating the vector store we left a parameter commented out in the code - <code>chunking_strategy</code>. This parameter allows you to specify how the documents are \"split\" into chunks before being processed by the model.</p> <pre><code>vs = client.vector_stores.create(\n    name=\"docs-vector-store-chunks\",\n    file_ids=file_ids,\n    # chunking_strategy={\n    #     \"chunk_overlap_tokens\": 400,\n    #     \"max_chunk_size_tokens\": 800\n    # }\n)\n</code></pre> <p>The <code>chunking_strategy</code> parameter is a dictionary that can have the following keys:</p> <ul> <li><code>chunk_overlap_tokens</code>: Number of tokens that chunks will overlap. (Default 400)</li> <li><code>max_chunk_size_tokens</code>: Max number of tokens that a chunk can have. (Default 800)</li> </ul> <p></p> <p>It is important to notice that chunks are not created based on the document's structure, but merely on counting tokens, which can lead to a loss of context.</p> <p>Make sure you change the <code>chunking_strategy</code> to better fit your documents' structure. For example, if your documents have a lot of code snippets, you can increase the <code>size</code> and <code>overlap</code> to make sure that the code can be correctly retrieved by the vector store.</p> <p>Additionally, you can use embedding models that are able to understand the document's structure and create chunks accordingly. We'll cover this in the next section.</p>"},{"location":"tutorials/rag/embeddings/","title":"Manual Embeddings","text":"<p>In the previous section we talked about chunking, and how it can help the model to better understand the document as a whole.</p> <p>However, current chunking strategies are very limited, dividing the document into chunks of fixed size with a fixed overlap, without considering the document's structure.</p> <p>If you know that your documents have a specific structure, you can create your own embeddings by manually dividing the document into chunks and processing them separately.</p> <p>Warning</p> <p>As with chunking, there are high chances that you do NOT need to create your own embeddings. We recommend to read this section only to gain understanding on how vector stores are implemented.</p>"},{"location":"tutorials/rag/embeddings/#manual-embeddings_1","title":"Manual Embeddings","text":"<p>Instead of using a vector store, you can directly call the Nomic v1.5 model so it creates the embedding vector for our documents:</p> <p>Example:</p> <pre><code>import json\nimport os\nimport re\n\nfrom glob import glob\nfrom urllib.parse import urlparse\n\nimport openai\nimport pandas as pd\nimport requests\nimport nearai\n\n# Load NEAR AI Hub configuration\nconfig = nearai.config.load_config_file()\nbase_url = config.get(\"api_url\", \"https://api.near.ai/\") + \"v1\"\nauth = config[\"auth\"]\n\nclient = openai.OpenAI(base_url=base_url, api_key=json.dumps(auth))\n\n# Create embeddings for all files\nembeddings_model = \"fireworks::nomic-ai/nomic-embed-text-v1.5\"\nprefix = \"classification: \"\n\ndocs = []\nmd_files = list(glob(\"./**/*.md\", recursive=True))\n\nfor file_path in md_files:\n    print(f\"Processing {file_path}\")\n\n    with open(file_path, 'r') as file:\n        docs.append(f\"{prefix}{file.read()}\")\n\nembeddings = client.embeddings.create(\n    input=docs,\n    model=embeddings_model\n)\n\ndf = pd.DataFrame.from_dict({\n    \"docs\": docs,\n    \"embeddings\": [e.embedding for e in embeddings.data]\n})\n\ndf.to_csv(\"embeddings.csv\", index=False)\n</code></pre> <p>Notice that we are manually storing the embeddings into a <code>CSV</code> file. This is because the platform does not support uploading embeddings directly into a vector store.</p> <p>Tip</p> <p>The Nomic v1.5 model uses <code>prefixes</code> (e.g. <code>classification:</code>, <code>search-document:</code>) to better guide the model on how to transform the documents. Make sure to read your model's documentation before using it to make the most of the embeddings</p>"},{"location":"tutorials/rag/embeddings/#using-manual-embeddings","title":"Using Manual Embeddings","text":"<p>After creating the embeddings, you will need to emulate the vector store's behavior by querying the embeddings and selecting the most relevant documents.</p> <p>Example:</p> <pre><code>import json\n\nimport openai\nimport nearai\nimport numpy as np\nimport pandas as pd\nfrom nearai.agents.environment import Environment\n\n# Load NEAR AI Hub configuration\nconfig = nearai.config.load_config_file()\nbase_url = config.get(\"api_url\", \"https://api.near.ai/\") + \"v1\"\nauth = config[\"auth\"]\n\nclient = openai.OpenAI(base_url=base_url, api_key=json.dumps(auth))\n\nMODEL = \"llama-v3p3-70b-instruct\"\n\ndf = pd.read_csv('./embeddings.csv')\nEMBEDDING_MODEL = \"fireworks::nomic-ai/nomic-embed-text-v1.5\"\nPREFIX = \"classification: \"\n\n\ndef cosine_similarity(a, b):\n    a = np.matrix(a)\n    b = np.array(b)\n    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n\n\ndef run(env: Environment):\n    user_query = env.list_messages()[-1][\"content\"]\n\n    embedding = client.embeddings.create(\n                input=[f\"{PREFIX}{user_query}\"],\n                model=EMBEDDING_MODEL,\n            ).data[0].embedding\n\n    df['similarities'] = df.embeddings.apply(\n        lambda x: cosine_similarity(x, embedding)\n    )\n\n    res = df.sort_values('similarities', ascending=False).head(6)\n\n    prompt = [\n        {\n            \"role\": \"user query\",\n            \"content\": user_query,\n        },\n        {\n            \"role\": \"documentation\",\n            \"content\": json.dumps(res.docs.tolist()),\n        },\n        {\n            \"role\": \"system\",\n            \"content\": \"Give a brief but complete answer to the user's query, staying as true as possible to the documentation SPECIALLY when dealing with code.\"\n        }\n    ]\n\n    answer = env.completion(model=MODEL, messages=prompt)\n    env.add_reply(answer)\n\n\nrun(env)\n</code></pre> <p>In the code above, we transform the user's query into a vector embedding using the same model that created our documentation embeddings. </p> <p>The system then calculates the cosine similarity between the query embedding and all stored document embeddings to find the most relevant matches.</p> <p>Finally, we rank the documents by their similarity scores and select the 6 most relevant chunks to provide context for the model's response.</p>"},{"location":"tutorials/rag/introduction/","title":"Building a Documentation Agent with RAG","text":"<p>Current AI models are trained on vast datasets, making them powerful at generating general-purpose text. However, when asked about specific topics outside their training data (like your company's internal documentation), these models often hallucinate - generating plausible-sounding but incorrect information.</p> <p>Thankfully, there is a solution to this problem: Retrieval-Augmented Generation (RAG). This technique consists on combining two key components:</p> <ol> <li>A retrieval system that finds relevant information from your custom dataset</li> <li>A language model that generates accurate responses using the retrieved information</li> </ol> <p>In this tutorial, you'll learn how to build a RAG-powered agent that accurately answers questions about NEAR Protocol.</p>"},{"location":"tutorials/rag/introduction/#what-you-will-need","title":"What You Will Need","text":"<p>To follow this tutorial you will need:</p> <ol> <li>NEAR AI CLI installed on your local machine \u2192 Installation Guide</li> <li>Basic understanding of creating a NEAR AI agent \u2192 Agents Quickstart Tutorial</li> </ol>"},{"location":"tutorials/rag/introduction/#overview","title":"Overview","text":"<p>This tutorial is divided in the following sections:</p> <ul> <li>The problem \u2192 Understanding AI hallucination from incorrect data</li> <li>Vector Stores \u2192 Getting started with vector stores</li> <li>RAG Agent \u2192 Building a NEAR Documentation Q&amp;A agent</li> <li>Chunking \u2192 Dive deeper into how vector stores store documents</li> <li>Embeddings \u2192 Creating document embeddings manually</li> </ul>"},{"location":"tutorials/rag/problem/","title":"The Problem","text":"<p>Let's start by understanding the problem that RAG solves. For this, we will create an agent that uses the open-source <code>llama</code> model, and ask it questions about NEAR Protocol.</p> <p>Then, we will enhance the agent by giving it access to the actual documentation, demonstrating how this dramatically improves the accuracy of its responses.</p>"},{"location":"tutorials/rag/problem/#an-agent-that-hallucinates","title":"An Agent that Hallucinates","text":"<p>Let's create an agent that uses the open-source <code>llama</code> model, and ask it questions about NEAR Protocol. For this, we will run the command <code>nearai agent create</code>, using the following parameters:</p> <pre><code>Agent Name: docs-ai-tutorial\nDescription: Answer questions about NEAR Protocol\nInstructions: You are a helpful assistant that answers questions about NEAR Protocol\n</code></pre> NEAR CLI not installed? <p>If you haven't installed the NEAR CLI yet, follow our Agent Quickstart Tutorial for installation instructions.</p> <p>Once the agent creation is complete, the CLI will show you how to run the agent locally:</p> <pre><code>nearai agent interactive ~/.nearai/registry/&lt;your-account.near&gt;/docs-ai-tutorial/0.0.1 --local\n</code></pre> <p>Now ask it a few questions about NEAR Protocol:</p> What types of access keys are there in NEAR? <pre><code>Assistant: In the NEAR Protocol, there are two main types of access keys:\n\n1. **Full Access Key**: This type of key has full control over the account and can perform any action, including managing other access keys. It's like the master key to the account.\n2. **Function Call Access Key**: This type of key has limited permissions and can only call specific functions on the account's smart contracts. It's like a key that only unlocks a specific door.\n\nThere's also a special case:\n\n3. **View Function Access Key**: This type of key can only call view functions on the account's smart contracts, which means it can only retrieve information but not modify the state of the contract. It's like a key that only allows peeking through a window.\n\nThese access keys are used to manage account permissions and control access to the account's resources. By setting up different access keys with varying levels of permissions, users can ensure that their account is secure and that others can only perform specific actions.\n\nDo you have any follow-up questions about access keys on NEAR Protocol?\n</code></pre> How can I create an account using the NEAR CLI? <pre><code>Assistant: To create an account using the NEAR CLI, you can use the `near create-account` command.\n\nHere's an example:\n\n`near create-account mynewaccount.near --masterAccount mymainaccount.near`\n\nThis will create a new account `mynewaccount.near` with a full access key, derived from the `mymainaccount.near` master account.\n\nIf you want to create an account with a specific public key or a specific access key permission, you can use additional options. For example:\n\n`near create-account mynewaccount.near --publicKey &lt;public_key&gt; --accessKey &lt;access_key_permission&gt;`\n\nReplace `&lt;public_key&gt;` with the actual public key and `&lt;access_key_permission&gt;` with one of the following: `FullAccess`, `FunctionCall`, or `ViewFunction`.\n\nMake sure to replace `mymainaccount.near` and `mynewaccount.near` with the actual account IDs you want to use.\n\nNote: You need to have the NEAR CLI installed and configured, and have a master account with sufficient balance to cover the account creation fee.\n</code></pre> <p>This agent uses <code>llama-v3p1-70b-instruct</code> model, which is a powerful open-domain model, and gets the answers mostly right... but then it starts to invent things.</p> <p>Without going too deep on the specifics of NEAR Protocol - which are not relevant for this tutorial - the agent invented a third type of access key (<code>View Function Access Key</code>), which simply does not exist, and proposed outdated commands to create accounts.</p>"},{"location":"tutorials/rag/problem/#giving-context-to-the-model","title":"Giving Context to the Model","text":"<p>Now, lets try to give some context to the model so it can improve its answers. </p> <p>For that, lets directly embed into the agent's prompt documentation on:</p> <ul> <li><code>Access Keys</code> </li> <li><code>NEAR CLI</code></li> </ul> <p>In your agent directory, let's download these files from the official NEAR docs repo:</p> <pre><code>curl -L https://raw.githubusercontent.com/near/docs/master/docs/1.concepts/protocol/access-keys.md -o access-keys.md &amp;&amp; \\\ncurl -L https://raw.githubusercontent.com/near/docs/master/docs/4.tools/cli.md -o cli.md\n</code></pre> <p>Your agent directory should now look like this:</p> <pre><code>\ud83d\udcc1 ~/.nearai/registry/&lt;your-account.near&gt;/ai-docs-tutorial/0.0.1\n\u251c\u2500\u2500 \ud83d\udcc4 agent.py\n\u251c\u2500\u2500 \ud83d\udcc4 access-keys.md\n\u251c\u2500\u2500 \ud83d\udcc4 cli.md\n\u2514\u2500\u2500 \ud83d\udcc4 metadata.json\n</code></pre> <p>Next, we'll update <code>agent.py</code> and instruct it to use these two markdown files:</p> <pre><code>import json\nfrom nearai.agents.environment import Environment\n\n\ndef run(env: Environment):\n    # Load files\n    files = [\"cli.md\", \"access-keys.md\"]\n    content = [{f: open(f, encoding=\"utf-8\").read()} for f in files]\n\n    # Get user query from last messages\n    user_query = env.list_messages()[-1][\"content\"]\n\n    prompt = [\n        {\n            \"role\": \"user query\",\n            \"content\": user_query,\n        },\n        {\n            \"role\": \"documentation\",\n            \"content\": json.dumps(content)\n        },\n        {\n            \"role\": \"system\",\n            \"content\": \"Give a brief but complete answer to the user's query, staying as true as possible to the documentation ESPECIALLY when dealing with code.\"\n        }\n    ]\n\n    result = env.completion(prompt)\n    env.add_reply(result)\n\nrun(env)\n</code></pre> <p>After updating the agent code, lets invoke it again and ask it the same questions:</p> What types of access keys are there in NEAR? <pre><code>Assistant: NEAR Protocol distinguishes two types of Access Keys:\n\n1. **Full-Access Keys**: Have full control over the account and should never be shared.\n2. **Function-Call Keys**: Can only sign calls for a specific contract and are meant to be shared.\n\nEach account in NEAR can hold multiple keys, which can be added or removed to fine-grain control over the account's permissions.\n</code></pre> How can I create an account using the NEAR CLI? <pre><code>Assistant: To create an account using the NEAR CLI, you can use the `create-account` command. Here is an example of how to do it:\n\n`\nexport ACCOUNT_ID=bob.testnet\nnear account create-account sponsor-by-faucet-service $ACCOUNT_ID autogenerate-new-keypair save-to-keychain network-config testnet create\n`\n\nAlternatively, you can use the short version of the command:\n\n`\nexport ACCOUNT_ID=bob.testnet\nnear create-account $ACCOUNT_ID --useFaucet --networkId testnet\n`\n</code></pre> <p>As you can see the answers now are much more accurate, which is expected, as we are providing the model with the actual documentation that it needs to generate the answers.</p>"},{"location":"tutorials/rag/problem/#limitations-of-this-approach","title":"Limitations of this Approach","text":"<p>While useful to understand the concept, this agent has a very important limitation: It does not scale</p> <p>Since this approach loads all of the content of a document into the prompt, it would quickly hit a limit on the number of tokens that the model can process if we were to load all of the documentation.</p> <p>To solve this problem, we need to use rely on a special kind of database called a Vector Store, which can help us to store documents and easily retrieve the most relevant ones to answer a given query.</p>"},{"location":"tutorials/rag/vector_store/","title":"Vector Stores","text":"<p>Vector Stores are a special kind of database that stores documents and allows retrieving them through natural language. </p> <p>In our case, we will store all NEAR documentation in a vector store, this way if a user asks \"What types of access keys are there in NEAR?\", the vector store will help us find our access keys docs, so we can use it to answer the question.</p> <p>Tip</p> <p>The Vector Store is built separately from the agent, and can be used in multiple places.</p> <p>Info</p> <p>This tutorial assumes OpenAI version 1.66.2 or greater. For previous versions the vector store methods can be found at <code>client.beta.vector_stores</code> instead of <code>client.vector_stores</code>.</p>"},{"location":"tutorials/rag/vector_store/#how-vector-stores-work","title":"How Vector Stores Work","text":"<p>Vector Stores use AI models to convert text documents into low-dimensional numerical representations that computers can easily process and compare. These vectors are like \"fingerprints\" of the documents - each one unique, but much smaller than the original text.</p> <p>For example, the Nomic v1.5 model can take our entire <code>cli.md</code> documentation file and represent it as a compact vector:</p> <pre><code>[0.01025390625, 0.048309326171875, -0.166015625, ... , -0.004741668701171875, -0.048553466796875]\n</code></pre> <p>Each number in this vector captures some aspect of the document's meaning. Documents with similar meanings will have similar vectors, appearing \"closer\" to each other in the vector space - like clusters of related topics.</p> <p>This is powerful because it means we can convert user questions into vectors too - allowing us to find relevant documentation by looking for documents that appear \"closest\" to the user's question.</p> <p></p> <p>When a user asks a question, the vector still will help us find relevant documentation by:</p> <ol> <li>Converting their question into a vector</li> <li>Comparing it with all our stored document vectors</li> <li>Retrieving the documents whose vectors are closest to the question's vector</li> </ol> <p>Tip</p> <p>How these representations are generated is a complex topic that exceeds the scope of this tutorial. For us, it suffices to know that such models already exist and are available for us to use.</p>"},{"location":"tutorials/rag/vector_store/#creating-a-vector-store","title":"Creating a Vector Store","text":"<p>Let's create a vector store using using all of our documentation files in the dataset.</p> <p>Didn't download the dataset?</p> <p>If you have not completed the problem section yet, you can download the dataset by running the following command: <pre><code>curl -L https://api.github.com/repos/near-examples/docs-ai/tarball/main | tar xz --strip=2 '*/docs-gpt/dataset'\n</code></pre></p> <p>Creating a vector store in NEAR AI consist of 2 main steps:</p> <p>1.) uploading the files to NEAR AI</p> <p>2.) assigning them to a vector store. </p> <p>Let's start by uploading the files to NEAR AI. Create a new file called <code>vector.py</code> and paste the following code:</p> <pre><code>import json\nfrom glob import glob\n\nimport openai\nimport nearai\n\n# Load NEAR AI Hub configuration\nconfig = nearai.config.load_config_file()\nbase_url = config.get(\"api_url\", \"https://api.near.ai/\") + \"v1\"\nauth = config[\"auth\"]\n\nclient = openai.OpenAI(base_url=base_url, api_key=json.dumps(auth))\n\n# Create a vector store for vector store docs\nmd_files = list(glob(\"./**/*.md\", recursive=True))\nfile_ids = []\n\nfor file_path in md_files:\n    print(f\"Processing {file_path}\")\n\n    with open(file_path, 'r', encoding='utf-8') as file:\n        uploaded_file = client.files.create(\n            file=(file_path, file.read(), \"text/markdown\"),\n            purpose=\"assistants\"\n        )\n        file_ids.append(uploaded_file.id)\n\nvs = client.vector_stores.create(\n    name=\"docs-vector-store-chunks\",\n    file_ids=file_ids,\n    # chunking_strategy={\n    #     \"chunk_overlap_tokens\": 400,\n    #     \"max_chunk_size_tokens\": 800\n    # }\n)\n\nprint(vs.id)\n</code></pre>"},{"location":"tutorials/rag/vector_store/#how-it-works","title":"How it works?","text":"<p>In the code above we are first instantiating an OpenAI compatible client</p> <pre><code># Load NEAR AI Hub configuration\nconfig = nearai.config.load_config_file()\nbase_url = config.get(\"api_url\", \"https://api.near.ai/\") + \"v1\"\nauth = config[\"auth\"]\n\nclient = openai.OpenAI(base_url=base_url, api_key=json.dumps(auth))\n</code></pre> <p>Note</p> <p>While we are using a OpenAI client, the files are being uploaded to the NEAR AI platform, not OpenAI.</p> <p>Then, we list all markdown files in the current directory and its subdirectories</p> <pre><code>md_files = list(glob(\"./**/*.md\", recursive=True))\n</code></pre> <p>For each file, we read its content and upload it to the NEAR AI platform</p> <pre><code>for file_path in md_files:\n    print(f\"Processing {file_path}\")\n\n    with open(file_path, 'r', encoding='utf-8') as file:\n        uploaded_file = client.files.create(\n            file=(file_path, file.read(), \"text/markdown\"),\n            purpose=\"assistants\"\n        )\n        file_ids.append(uploaded_file.id)\n</code></pre> <p>Finally, we create a vector store with the uploaded files and print its <code>id</code></p> <pre><code>vs = client.vector_stores.create(\n    name=\"docs-vector-store-chunks\",\n    file_ids=file_ids,\n    # chunking_strategy={\n    #     \"chunk_overlap_tokens\": 400,\n    #     \"max_chunk_size_tokens\": 800\n    # }\n)\n\nprint(vs.id)\n</code></pre> <p>Now run the script:</p> <p><pre><code>python3 vector.py\n</code></pre> You should see each file being uploaded to NEAR AI in the console. </p> <p>At the end of the script, you will get the <code>id</code> of the vector store that was created. This <code>id</code> will be used in the next section to build an agent that leverages the knowledge contained on it.</p> Where are the Embeddings? <p>No need to worry about this, the NEAR AI platform abstracts away all the complexity of processing the files to create embeddings, and then storing them in a database to be queried.</p> <p>You only need to upload the files and the platform will take care of the rest</p> <p>Chunking Strategy</p> <p>In the code above we have commented the <code>chunking_strategy</code> parameter, which allows you to specify how the documents are \"split\" into chunks before being processed by the model.</p> <p>Do not worry about this parameter for now, we will come back to it later in this tutorial.</p>"},{"location":"tutorials/rag/vector_store/#a-note-on-preprocessing","title":"A Note on Preprocessing","text":"<p>While the platform abstracts away the complexity of creating embeddings, it is still a good idea to preprocess the files before uploading them.</p> <p>In our case, our <code>markdown</code> files contain a lot of unnecessary information, such as <code>imports</code>, <code>iframes</code> and other tags that are simply not relevant for the document's semantics.</p> <p>Furthermore - and this might be common to many documentation files - our files do not necessarily contain the plain-text code snippets, but instead embed them from external sources (e.g. GitHub). So we need to preprocess them to translate the Github tags into actual code snippets.</p> <p>In case you are curious, you can file all the preprocessing steps we use for our documentation in the vector.py script</p>"}]}